{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 12:14:04.947077: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-29 12:14:04.947122: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# import needed libraries\n",
    "\n",
    "import pandas as pd\n",
    "import helper.Utils as Utils\n",
    "import matplotlib.pyplot as plt\n",
    "import configure as cf\n",
    "import re\n",
    "import pandas as pd\n",
    "from snowballstemmer import stemmer\n",
    "import arabicstopwords.arabicstopwords as stp\n",
    "import pyterrier as pt\n",
    "from pyterrier.measures import RR, R, Rprec, P, MAP\n",
    "from train_set_creator import MonoBertTrainSetCreator\n",
    "from transformers import AutoTokenizer\n",
    "from mono_bert_train import MonoBertTrainer\n",
    "from mono_bert_test import MonoBertTester\n",
    "import statsmodels.stats.multitest\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling PRF in pyterier\n",
      "PyTerrier 0.6.0 has loaded Terrier 5.6 (built by craigmacdonald on 2021-09-17 13:27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not pt.started():\n",
    "    print(\"Enabling PRF in pyterier\")\n",
    "    # In this lab, we need to specify that we start PyTerrier with PRF enabled\n",
    "    pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before indexing our data we need to do the following processing steps:\n",
    "\n",
    "\n",
    "1.   **Remove stopwords.**\n",
    "2.   **Normalization.**\n",
    "3.   **Stemming.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<snowballstemmer.arabic_stemmer.ArabicStemmer at 0x7fb857824fd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def clean(text):\n",
    "\n",
    "  '''\n",
    "  Clean input text form urls, handles, tabs, line jumps, and extra white spaces\n",
    "  '''\n",
    "  text = re.sub(r\"http\\S+\", \" \", text)  # remove urls\n",
    "  text = re.sub(r\"RT \", \" \", text)  # remove rt\n",
    "  text = re.sub(r\"@[\\w]*\", \" \", text)  # remove handles\n",
    "  # text = re.sub(r\"[\\.\\,\\#_\\|\\:\\?\\?\\/\\=]\", \" \", text)# remove special characters\n",
    "  text = re.sub(r\"\\t\", \" \", text)  # remove tabs\n",
    "  text = re.sub(r\"\\n\", \" \", text)  # remove line jump\n",
    "  text = re.sub(r\"\\s+\", \" \", text)  # remove extra white space\n",
    "  text = text.strip()\n",
    "  return text\n",
    "  \n",
    "#removing stop sords function\n",
    "def remove_stop_words(sentence):\n",
    "  terms=[]\n",
    "  stopWords= set(stp.stopwords_list())\n",
    "  for term in sentence.split() : \n",
    "      if term not in stopWords :\n",
    "          terms.append(term)\n",
    "  return \" \".join(terms)\n",
    "\n",
    "#a function to normalize the tweets\n",
    "def normalize(text):\n",
    "  text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
    "  text = re.sub(\"ى\", \"ي\", text)\n",
    "  text = re.sub(\"ؤ\", \"ء\", text)\n",
    "  text = re.sub(\"ئ\", \"ء\", text)\n",
    "  text = re.sub(\"ة\", \"ه\", text)\n",
    "  return(text)\n",
    "\n",
    "\n",
    "def stem(sentence):\n",
    "    return \" \".join([ar_stemmer.stemWord(i) for i in sentence.split()])\n",
    "\n",
    "\n",
    "def preprocess(sentence):\n",
    "  # apply preprocessing steps on the given sentence\n",
    "  sentence = Utils.clean(sentence)\n",
    "  sentence = Utils.remove_emoji_smileys(sentence)\n",
    "  sentence =normalize(sentence)\n",
    "  sentence =remove_stop_words(sentence)\n",
    "  sentence =stem(sentence)\n",
    "  return sentence\n",
    "\n",
    "#define the stemming function\n",
    "ar_stemmer = stemmer(\"arabic\")\n",
    "ar_stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define some constants.\n",
    "is_index_built= False\n",
    "DECIMAL_ROUND = 5\n",
    "pos_label = 1.0\n",
    "neg_label = 0.0\n",
    "RANK = cf.RANK\n",
    "SCORE = cf.SCORE\n",
    "TWEET_ID_COLUMN = cf.TWEET_ID_COLUMN\n",
    "TWEET_TEXT_COLUMN = cf.TWEET_TEXT_COLUMN\n",
    "VCLAIM_ID = cf.VCLAIM_ID\n",
    "VCLAIM = cf.VCLAIM\n",
    "TITLE = cf.TITLE \n",
    "LABEL = cf.LABEL\n",
    "\n",
    "\n",
    "QUERY = cf.QUERY\n",
    "QID = cf.QID\n",
    "DOC_NO = cf.DOC_NO\n",
    "DOCID = cf.DOCID\n",
    "\n",
    "index_path = \"./indexes/ar-clef-2021-index-multi-field/data.properties\"\n",
    "qrels_file=cf.AR_2021_QRElS_FILE\n",
    "claims_file = cf.AR_2021_VCLAIMS_FILE\n",
    "evaluation_save_path = \"./data/ar_clef2021_evaluation.xlsx\"\n",
    "\n",
    "raw_dev_query = cf.AR_2021_DEV_QUERIES_PATH\n",
    "expanded_dev_query = cf.AR_2021_URL_CLEANED_DEV_QUERIES\n",
    "\n",
    "dev_query_path = expanded_dev_query\n",
    "train_query_path = cf.AR_2021_URL_CLEANED_TRAIN_QUERIES\n",
    "test_query_path = cf.AR_2021_URL_CLEANED_TEST_QUERIES \n",
    "eval_metrics=[\"map\",MAP@5, P@1, RR, Rprec, R@5, R@10, R@20, R@50, RR@5]\n",
    "\n",
    "dev_bm25_run = \"./data/runs/bm25_dev_query_ar_clef_2021.tsv\"\n",
    "train_bm25_run = \"./data/runs/bm25_train_query_ar_clef_2021.tsv\"\n",
    "test_bm25_run = \"./data/runs/bm25_test_query_ar_clef_2021.tsv\"\n",
    "mono_bert_train_vclaim_set = \"./data/CLEF_2021/Arabic/train_sets/2021_mono_bert_train_set-vclaim-only--ar.xlsx\"\n",
    "mono_bert_train_title_set = \"./data/CLEF_2021/Arabic/train_sets/2021_mono_bert_train_set-title-only-ar.xlsx\"\n",
    "mono_bert_train_both_vclaim_and_title_set = \"./data/CLEF_2021/Arabic/train_sets/2021_mono_bert_train_set-both_vclaim_and_title.xlsx\"\n",
    "                                \n",
    "random_seeds = [61168821, 129995678, 22612812, 146764631, 21228945, 94412880, 204110176, 6155814, 187372311, 117623077,]\n",
    "\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "ONE_LAYER = 1\n",
    "TWO_LAYERS = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vclaim</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vclaim-0000</th>\n",
       "      <td>العاهل المغربي يتلقى الجرعة الاولى من لقاح كورونا</td>\n",
       "      <td>العاهل المغربي يتلقى الجرعة الأولى من اللقاح ا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-0001</th>\n",
       "      <td>نقل اسواق الموسكي والعتبة والفجالة إلى شرق الق...</td>\n",
       "      <td>نقل أسواق الموسكي والعتبة والفجالة والأزهر إلى...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-0002</th>\n",
       "      <td>جائحة كورونا ليست كارثة صحية ولا نرى ذلك في ال...</td>\n",
       "      <td>لم تنشر جامعة جون هوبكنز هذا التقرير بخصوص جائ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-0003</th>\n",
       "      <td>لقاح كورونا غير امن وبشهاده اطباء من دول اروبيه</td>\n",
       "      <td>لقاح كورونا غير آمن وبشهادة أطباء من دول أوروب...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-0004</th>\n",
       "      <td>احتمال اجراء امتحانات الصفين الاول والثاني الث...</td>\n",
       "      <td>هذه الصورة المتداولة حول امتحانات مصر مفبركة -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-9995</th>\n",
       "      <td>رفض الرئيس أيزنهاور فكرة العروض العسكرية ، قائ...</td>\n",
       "      <td>هل رفض دوايت أيزنهاور فكرة العروض العسكرية \"ال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-9996</th>\n",
       "      <td>شكوى FTC / DOJ / DOT.</td>\n",
       "      <td>تحذير من الفيروسات: تنزيل شكوى FTC / DOJ / DOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-9997</th>\n",
       "      <td>تم العثور على فتاة مهاجرة مع \"20 نوعا من السائ...</td>\n",
       "      <td>هل تم العثور على فتاة مهاجرة وبها 20 نوعًا من ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-9998</th>\n",
       "      <td>تشيفي فولت - تكلف أكثر من 7 أضعاف تكلفة تشغيله...</td>\n",
       "      <td>تكلفة تشغيل Chevy Volt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-9999</th>\n",
       "      <td>تظهر الصور الثلوج في الصحراء الكبرى في يناير 2...</td>\n",
       "      <td>هل تظهر هذه الصور تساقط الثلوج في الصحراء الكبرى؟</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30329 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        vclaim  \\\n",
       "vclaim_id                                                        \n",
       "vclaim-0000  العاهل المغربي يتلقى الجرعة الاولى من لقاح كورونا   \n",
       "vclaim-0001  نقل اسواق الموسكي والعتبة والفجالة إلى شرق الق...   \n",
       "vclaim-0002  جائحة كورونا ليست كارثة صحية ولا نرى ذلك في ال...   \n",
       "vclaim-0003    لقاح كورونا غير امن وبشهاده اطباء من دول اروبيه   \n",
       "vclaim-0004  احتمال اجراء امتحانات الصفين الاول والثاني الث...   \n",
       "...                                                        ...   \n",
       "vclaim-9995  رفض الرئيس أيزنهاور فكرة العروض العسكرية ، قائ...   \n",
       "vclaim-9996                              شكوى FTC / DOJ / DOT.   \n",
       "vclaim-9997  تم العثور على فتاة مهاجرة مع \"20 نوعا من السائ...   \n",
       "vclaim-9998  تشيفي فولت - تكلف أكثر من 7 أضعاف تكلفة تشغيله...   \n",
       "vclaim-9999  تظهر الصور الثلوج في الصحراء الكبرى في يناير 2...   \n",
       "\n",
       "                                                         title  \n",
       "vclaim_id                                                       \n",
       "vclaim-0000  العاهل المغربي يتلقى الجرعة الأولى من اللقاح ا...  \n",
       "vclaim-0001  نقل أسواق الموسكي والعتبة والفجالة والأزهر إلى...  \n",
       "vclaim-0002  لم تنشر جامعة جون هوبكنز هذا التقرير بخصوص جائ...  \n",
       "vclaim-0003  لقاح كورونا غير آمن وبشهادة أطباء من دول أوروب...  \n",
       "vclaim-0004  هذه الصورة المتداولة حول امتحانات مصر مفبركة -...  \n",
       "...                                                        ...  \n",
       "vclaim-9995  هل رفض دوايت أيزنهاور فكرة العروض العسكرية \"ال...  \n",
       "vclaim-9996     تحذير من الفيروسات: تنزيل شكوى FTC / DOJ / DOT  \n",
       "vclaim-9997  هل تم العثور على فتاة مهاجرة وبها 20 نوعًا من ...  \n",
       "vclaim-9998                             تكلفة تشغيل Chevy Volt  \n",
       "vclaim-9999  هل تظهر هذه الصور تساقط الثلوج في الصحراء الكبرى؟  \n",
       "\n",
       "[30329 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read verified claims file\n",
    "df_claim = Utils.read_file(claims_file)\n",
    "df_claim[VCLAIM_ID]= df_claim[VCLAIM_ID].astype(str)\n",
    "df_claim.set_index(VCLAIM_ID, inplace=True)\n",
    "df_claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load QRELs file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>Q0</th>\n",
       "      <th>docno</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tweet-ar-000</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-2264</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tweet-ar-001</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-2266</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tweet-ar-002</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-2267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tweet-ar-003</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-2271</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tweet-ar-004</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-2270</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1034</th>\n",
       "      <td>tweet-529</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-0237</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035</th>\n",
       "      <td>tweet-068</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-0171</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1036</th>\n",
       "      <td>tweet-742</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-1109</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1037</th>\n",
       "      <td>tweet-757</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-0434</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>tweet-192</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-1223</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1039 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               qid  Q0        docno  label\n",
       "0     tweet-ar-000   0  vclaim-2264      1\n",
       "1     tweet-ar-001   0  vclaim-2266      1\n",
       "2     tweet-ar-002   0  vclaim-2267      1\n",
       "3     tweet-ar-003   0  vclaim-2271      1\n",
       "4     tweet-ar-004   0  vclaim-2270      1\n",
       "...            ...  ..          ...    ...\n",
       "1034     tweet-529   0  vclaim-0237      1\n",
       "1035     tweet-068   0  vclaim-0171      1\n",
       "1036     tweet-742   0  vclaim-1109      1\n",
       "1037     tweet-757   0  vclaim-0434      1\n",
       "1038     tweet-192   0  vclaim-1223      1\n",
       "\n",
       "[1039 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_qrels(qrels_file):\n",
    "    df_qrels = pd.read_csv(qrels_file, sep=\"\\t\", names=[\"qid\", \"Q0\", \"docno\", LABEL])\n",
    "    df_qrels[\"qid\"]=df_qrels[\"qid\"].astype(str)\n",
    "    df_qrels[\"docno\"]=df_qrels[\"docno\"].astype(str)\n",
    "    return df_qrels\n",
    "\n",
    "df_qrels = load_qrels(qrels_file)\n",
    "df_qrels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create multi-fields index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 30329\n",
      "Number of terms: 36535\n",
      "Number of postings: 497017\n",
      "Number of fields: 2\n",
      "Number of tokens: 652148\n",
      "Field names: [text, title]\n",
      "Positions:   false\n",
      "\n",
      "Index has been loaded successfully\n"
     ]
    }
   ],
   "source": [
    "def load_index(index_path):\n",
    "    try:\n",
    "            # first load the index\n",
    "        multi_field_index = pt.IndexFactory.of(index_path)\n",
    "        # call getCollectionStatistics() to check the stats\n",
    "        print(multi_field_index.getCollectionStatistics().toString())\n",
    "        print(\"Index has been loaded successfully\")\n",
    "        return multi_field_index\n",
    "    except Exception as e:\n",
    "        print('Cannot load the index, check exception details {}'.format(e))\n",
    "        return []\n",
    "\n",
    "\n",
    "multi_field_index = load_index(index_path=index_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_query_for_search(query_path, data_column=\"cleaned\"):\n",
    "\n",
    "    print(\"Cleaning queries and applying preprocessing steps\")\n",
    "    df_query = Utils.read_file(query_path)\n",
    "    df_query[QUERY] =df_query[data_column].apply(Utils.remove_punctuation)\n",
    "    df_query[QUERY] = df_query[QUERY].apply(preprocess)\n",
    "    df_query[QID] = df_query[TWEET_ID_COLUMN].astype(str)\n",
    "    df_query = df_query[[QID, QUERY]]\n",
    "    print(\"Done with cleaning!\")\n",
    "    return df_query\n",
    "    \n",
    "def search_and_evaluate(query_path, run_save_path, retrieval_model=\"BM25\", evaluation_path=\"\",\n",
    "                        data_column='cleaned', depth=100, method_name=\"BM25\"):\n",
    "    \n",
    "    df_query = clean_query_for_search(query_path, data_column=data_column)\n",
    "    # intialize retrieval model to get the top k (depth) potentially relevant documents\n",
    "    retr = pt.BatchRetrieve(multi_field_index, controls = {\"wmodel\": retrieval_model},num_results=depth)\n",
    "    print(\"Searching for the queries .....\")\n",
    "    # retrieve potentially relevant documents for each query in queries file\n",
    "    retr_res = retr.transform(df_query)\n",
    "    # save the run in trec format\n",
    "    retr_res.to_csv(run_save_path, header=False, index=False, sep='\\t')\n",
    "\n",
    "\n",
    "    # Evaluate the performance\n",
    "    print(\"Perfomring evaluation ......\")\n",
    "    retr_eval = pt.Utils.evaluate(retr_res, df_qrels[[\"qid\", \"docno\", LABEL]],metrics=eval_metrics)\n",
    "    retr_eval.update({\"name\": method_name})\n",
    "    retr_eval.update({\"depth\": depth})\n",
    "    retr_eval = pd.DataFrame([retr_eval])\n",
    "    retr_eval = retr_eval.round(DECIMAL_ROUND)\n",
    "\n",
    "    # save evaluation results\n",
    "    if evaluation_path != \"\":\n",
    "        if not os.path.isfile(evaluation_path): # if file is not exist, create a new one\n",
    "            retr_eval.to_excel(evaluation_path, index=False)\n",
    "        else: # it is already exist, append current evaluation to it\n",
    "            df_eval = Utils.read_file(evaluation_path)\n",
    "            df_eval = df_eval.append(retr_eval, ignore_index=True)\n",
    "            df_eval.to_excel(evaluation_path, index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Done searching and evaluation \")\n",
    "    return retr_res, retr_eval\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test lexical retrieval models before and after applying the preprocessing steps (expanding the tweet) on the dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning queries and applying preprocessing steps\n",
      "Done with cleaning!\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Done with cleaning!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>map</th>\n",
       "      <th>Rprec</th>\n",
       "      <th>RR</th>\n",
       "      <th>P@1</th>\n",
       "      <th>R@5</th>\n",
       "      <th>R@10</th>\n",
       "      <th>R@20</th>\n",
       "      <th>R@50</th>\n",
       "      <th>AP@5</th>\n",
       "      <th>...</th>\n",
       "      <th>R@20 p-value</th>\n",
       "      <th>R@50 +</th>\n",
       "      <th>R@50 -</th>\n",
       "      <th>R@50 p-value</th>\n",
       "      <th>AP@5 +</th>\n",
       "      <th>AP@5 -</th>\n",
       "      <th>AP@5 p-value</th>\n",
       "      <th>RR@5 +</th>\n",
       "      <th>RR@5 -</th>\n",
       "      <th>RR@5 p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JM</td>\n",
       "      <td>0.817434</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.839100</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.878431</td>\n",
       "      <td>0.890196</td>\n",
       "      <td>0.943137</td>\n",
       "      <td>0.812157</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JM+PreP</td>\n",
       "      <td>0.830642</td>\n",
       "      <td>0.782353</td>\n",
       "      <td>0.842668</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.886275</td>\n",
       "      <td>0.890196</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.949020</td>\n",
       "      <td>0.826863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320183</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.144243</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.829863</td>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.844753</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.890196</td>\n",
       "      <td>0.890196</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.954902</td>\n",
       "      <td>0.826732</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BM25+PreP</td>\n",
       "      <td>0.850639</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.860182</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.960784</td>\n",
       "      <td>0.848301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320183</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.101347</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.158524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DPH</td>\n",
       "      <td>0.804322</td>\n",
       "      <td>0.758824</td>\n",
       "      <td>0.820088</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.884314</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.937255</td>\n",
       "      <td>0.797712</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DPH+PreP</td>\n",
       "      <td>0.825783</td>\n",
       "      <td>0.782353</td>\n",
       "      <td>0.841273</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.880392</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.937255</td>\n",
       "      <td>0.820261</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.098047</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.158524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DLH</td>\n",
       "      <td>0.818932</td>\n",
       "      <td>0.782353</td>\n",
       "      <td>0.834336</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.884314</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.943137</td>\n",
       "      <td>0.813007</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DLH+PreP</td>\n",
       "      <td>0.839428</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.848971</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.890196</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.949020</td>\n",
       "      <td>0.836536</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320183</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089780</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DFR_BM25</td>\n",
       "      <td>0.806798</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.820071</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.907843</td>\n",
       "      <td>0.943137</td>\n",
       "      <td>0.799085</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DFR_BM25+PreP</td>\n",
       "      <td>0.825922</td>\n",
       "      <td>0.770588</td>\n",
       "      <td>0.840628</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.886275</td>\n",
       "      <td>0.890196</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.949020</td>\n",
       "      <td>0.821634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657393</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.320183</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.132692</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.102793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rm3_pipe</td>\n",
       "      <td>0.727291</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>0.734718</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.716013</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rm3_pipe+PreP</td>\n",
       "      <td>0.721634</td>\n",
       "      <td>0.592157</td>\n",
       "      <td>0.727778</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.917647</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.712484</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.711804</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.515900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             name       map     Rprec        RR       P@1       R@5      R@10  \\\n",
       "0              JM  0.817434  0.776471  0.839100  0.800000  0.866667  0.878431   \n",
       "1         JM+PreP  0.830642  0.782353  0.842668  0.800000  0.886275  0.890196   \n",
       "2            BM25  0.829863  0.788235  0.844753  0.800000  0.890196  0.890196   \n",
       "3       BM25+PreP  0.850639  0.811765  0.860182  0.823529  0.901961  0.901961   \n",
       "4             DPH  0.804322  0.758824  0.820088  0.776471  0.856863  0.884314   \n",
       "5        DPH+PreP  0.825783  0.782353  0.841273  0.800000  0.880392  0.901961   \n",
       "6             DLH  0.818932  0.782353  0.834336  0.800000  0.860784  0.884314   \n",
       "7        DLH+PreP  0.839428  0.800000  0.848971  0.811765  0.890196  0.901961   \n",
       "8        DFR_BM25  0.806798  0.764706  0.820071  0.776471  0.856863  0.872549   \n",
       "9   DFR_BM25+PreP  0.825922  0.770588  0.840628  0.800000  0.886275  0.890196   \n",
       "10       rm3_pipe  0.727291  0.598039  0.734718  0.600000  0.852941  0.911765   \n",
       "11  rm3_pipe+PreP  0.721634  0.592157  0.727778  0.600000  0.858824  0.917647   \n",
       "\n",
       "        R@20      R@50      AP@5  ...  R@20 p-value  R@50 +  R@50 -  \\\n",
       "0   0.890196  0.943137  0.812157  ...           NaN     NaN     NaN   \n",
       "1   0.901961  0.949020  0.826863  ...      0.158524     1.0     0.0   \n",
       "2   0.901961  0.954902  0.826732  ...           NaN     NaN     NaN   \n",
       "3   0.913725  0.960784  0.848301  ...      0.158524     1.0     0.0   \n",
       "4   0.913725  0.937255  0.797712  ...           NaN     NaN     NaN   \n",
       "5   0.913725  0.937255  0.820261  ...      1.000000     2.0     1.0   \n",
       "6   0.901961  0.943137  0.813007  ...           NaN     NaN     NaN   \n",
       "7   0.901961  0.949020  0.836536  ...      1.000000     1.0     0.0   \n",
       "8   0.907843  0.943137  0.799085  ...           NaN     NaN     NaN   \n",
       "9   0.901961  0.949020  0.821634  ...      0.657393     1.0     0.0   \n",
       "10  0.941176  0.941176  0.716013  ...           NaN     NaN     NaN   \n",
       "11  0.941176  0.941176  0.712484  ...           NaN     0.0     0.0   \n",
       "\n",
       "    R@50 p-value  AP@5 +  AP@5 -  AP@5 p-value  RR@5 +  RR@5 -  RR@5 p-value  \n",
       "0            NaN     NaN     NaN           NaN     NaN     NaN           NaN  \n",
       "1       0.320183     4.0     2.0      0.144243     1.0     0.0      0.320183  \n",
       "2            NaN     NaN     NaN           NaN     NaN     NaN           NaN  \n",
       "3       0.320183     3.0     0.0      0.101347     2.0     0.0      0.158524  \n",
       "4            NaN     NaN     NaN           NaN     NaN     NaN           NaN  \n",
       "5       1.000000     4.0     0.0      0.098047     2.0     0.0      0.158524  \n",
       "6            NaN     NaN     NaN           NaN     NaN     NaN           NaN  \n",
       "7       0.320183     4.0     0.0      0.089780     2.0     0.0      0.181258  \n",
       "8            NaN     NaN     NaN           NaN     NaN     NaN           NaN  \n",
       "9       0.320183     3.0     1.0      0.132692     3.0     0.0      0.102793  \n",
       "10           NaN     NaN     NaN           NaN     NaN     NaN           NaN  \n",
       "11           NaN     2.0     5.0      0.711804     1.0     4.0      0.515900  \n",
       "\n",
       "[12 rows x 41 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Read test queries\n",
    "\n",
    "df_dev_query  = clean_query_for_search(raw_dev_query, data_column=TWEET_TEXT_COLUMN)\n",
    "df_dev_query_after_preprocessing = clean_query_for_search(expanded_dev_query, data_column=\"cleaned\")\n",
    "\n",
    "# retrieva models\n",
    "depth = 100 \n",
    "JM = pt.BatchRetrieve(multi_field_index,wmodel=\"Hiemstra_LM\",controls ={\"c\":0.05},num_results=depth)\n",
    "bm25_retr = pt.BatchRetrieve(multi_field_index, controls = {\"wmodel\": \"BM25\", \"c\" : 0.75, \"bm25.k_1\": 0.75, \"bm25.k_3\": 0.75},num_results=depth)\n",
    "DPH = pt.BatchRetrieve(multi_field_index,wmodel=\"DPH\", num_results=depth)\n",
    "DLH = pt.BatchRetrieve(multi_field_index,wmodel=\"DLH\", num_results=depth)\n",
    "DFR_BM25 = pt.BatchRetrieve(multi_field_index,wmodel=\"DFR_BM25\", num_results=depth)\n",
    "rm3_pipe = bm25_retr >> pt.rewrite.RM3(multi_field_index,fb_terms=10, fb_docs=3) >> bm25_retr\n",
    "\n",
    "retrieval_models = [JM, bm25_retr, DPH, DLH, DFR_BM25, rm3_pipe]\n",
    "retreival_model_names =  [\"JM\", \"BM25\", \"DPH\", \"DLH\", \"DFR_BM25\", \"rm3_pipe\"]\n",
    "\n",
    "df_eval_all = pd.DataFrame()\n",
    "for i in range(len(retrieval_models)):\n",
    "    retrieval_model = retrieval_models[i]\n",
    "    retrieval_model_name = retreival_model_names[i]\n",
    "\n",
    "    retrieval_list = retrieval_model.transform(df_dev_query)\n",
    "    retrieval_list_after_preprocessing= retrieval_model.transform(df_dev_query_after_preprocessing)\n",
    "\n",
    "\n",
    "    df_eval = pt.Experiment([retrieval_list, retrieval_list_after_preprocessing],\n",
    "                df_dev_query,\n",
    "                df_qrels[[\"qid\", \"docno\", LABEL]],\n",
    "                eval_metrics=eval_metrics,\n",
    "                names=[retrieval_model_name, retrieval_model_name+\"+PreP\"],\n",
    "                baseline=0)\n",
    "    df_eval_all = df_eval_all.append(df_eval, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "df_eval_all.to_excel(\"./data/ar_classic_retrieval_with_preprocessing_on_dev.xlsx\", index=False)\n",
    "df_eval_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training data for mono BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_set(mono_train_set_creator, train_query_path, train_bm25_run_path, bm25_evaluation_save_path,train_set_save_path, \n",
    "                    search_depth=100, query_column=\"cleaned\", retrieval_model=\"BM25\", depth_of_random=20,\n",
    "                    what_to_add=cf.VCLAIM_AND_TITLE,  add_similarity=False, lang=\"ar\"):\n",
    "\n",
    "\n",
    "   # 1- create run for both dev and train queries\n",
    "    print(\"Make runs and evaluation\")\n",
    "    mono_train_set_creator.search_and_evaluate(query_path=train_query_path, evaluation_path=bm25_evaluation_save_path, \n",
    "                        retrieval_model=retrieval_model, run_save_path=train_bm25_run_path, \n",
    "                        depth=search_depth, method_name=\"bm25_train+dev\", data_column=query_column, lang=lang)\n",
    "    print(\"Done with runs and evaluation\")\n",
    "\n",
    "    # 2- create the training set\n",
    "    print(\"Creating train set\")\n",
    "    mono_train_set_creator.create_train_pairs(train_query_path, train_bm25_run_path, train_set_save_path, \n",
    "                            query_column=\"cleaned\", what_to_add=what_to_add, \n",
    "                            depth_of_random=depth_of_random, add_similarity=add_similarity,)\n",
    "    print(\"Done creating train set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 30329\n",
      "Number of terms: 36535\n",
      "Number of postings: 497017\n",
      "Number of fields: 2\n",
      "Number of tokens: 652148\n",
      "Field names: [text, title]\n",
      "Positions:   false\n",
      "\n",
      "Index has been loaded successfully\n"
     ]
    }
   ],
   "source": [
    "mono_bert_dev_set_depth_20=  \"./data/CLEF_2021/Arabic/dev_sets/ar-clef2021-mono_bert_dev_set_top_20.tsv\"\n",
    "mono_bert_dev_set_depth_30= \"./data/CLEF_2021/Arabic/dev_sets/ar-clef2021-mono_bert_dev_set_top_30.tsv\"\n",
    "\n",
    "mono_bert_train_set = mono_bert_train_both_vclaim_and_title_set\n",
    "\n",
    "# depth from which negative pairs will be randomly selected from top k documents retrieved from a retrieval model\n",
    "RANDOM_DEPTH = 20 # RANDOM_DEPTH_FOR_SELECTING_NEGATIVE_DOCUMENTS\n",
    "search_depth = 100\n",
    "\n",
    "mono_train_set_creator = MonoBertTrainSetCreator(qrels_file, claims_file, index_path, eval_metrics)\n",
    "mono_bert_trainer = MonoBertTrainer(train_query_path, dev_query_path, qrels_path=qrels_file,)\n",
    "mono_bert_tester = MonoBertTester(qrels_path=qrels_file,\n",
    "                                evaluation_save_path=evaluation_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 30329\n",
      "Number of terms: 36535\n",
      "Number of postings: 497017\n",
      "Number of fields: 2\n",
      "Number of tokens: 652148\n",
      "Field names: [text, title]\n",
      "Positions:   false\n",
      "\n",
      "Index has been loaded successfully\n",
      "Make runs and evaluation\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying normalization, stemming and stop word removal for Arabic\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating train set\n",
      "Processing tweet number 0 with tweet id tweet-ar-003 \n",
      "Processing tweet number 100 with tweet id tweet-ar-167 \n",
      "Processing tweet number 200 with tweet id tweet-ar-347 \n",
      "Processing tweet number 300 with tweet id tweet-ar-508 \n",
      "Processing tweet number 400 with tweet id tweet-ar-670 \n",
      "Processing tweet number 500 with tweet id tweet-ar-848 \n",
      "Done creating train set\n",
      "Make runs and evaluation\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying normalization, stemming and stop word removal for Arabic\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating train set\n",
      "Processing tweet number 0 with tweet id tweet-ar-003 \n",
      "Processing tweet number 100 with tweet id tweet-ar-167 \n",
      "Processing tweet number 200 with tweet id tweet-ar-347 \n",
      "Processing tweet number 300 with tweet id tweet-ar-508 \n",
      "Processing tweet number 400 with tweet id tweet-ar-670 \n",
      "Processing tweet number 500 with tweet id tweet-ar-848 \n",
      "Done creating train set\n",
      "Make runs and evaluation\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying normalization, stemming and stop word removal for Arabic\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating train set\n",
      "Processing tweet number 0 with tweet id tweet-ar-003 \n",
      "Processing tweet number 100 with tweet id tweet-ar-167 \n",
      "Processing tweet number 200 with tweet id tweet-ar-347 \n",
      "Processing tweet number 300 with tweet id tweet-ar-508 \n",
      "Processing tweet number 400 with tweet id tweet-ar-670 \n",
      "Processing tweet number 500 with tweet id tweet-ar-848 \n",
      "Done creating train set\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train_set_both = create_train_set(mono_train_set_creator, train_query_path, train_bm25_run, \"\",\n",
    "                    train_set_save_path=mono_bert_train_both_vclaim_and_title_set, \n",
    "                    search_depth=search_depth, query_column=\"cleaned\", retrieval_model=\"BM25\", depth_of_random=RANDOM_DEPTH,\n",
    "                    what_to_add=cf.VCLAIM_AND_TITLE, add_similarity=False,)\n",
    "\n",
    "df_train_set_title_only = create_train_set(mono_train_set_creator, train_query_path, train_bm25_run, \"\",\n",
    "                    train_set_save_path=mono_bert_train_title_set, \n",
    "                    search_depth=search_depth, query_column=\"cleaned\", retrieval_model=\"BM25\", depth_of_random=RANDOM_DEPTH,\n",
    "                    what_to_add=cf.TITLE_ONLY, add_similarity=False,)\n",
    "\n",
    "df_train_set_vclaim = create_train_set(mono_train_set_creator, train_query_path, train_bm25_run, \"\",\n",
    "                    train_set_save_path=mono_bert_train_vclaim_set, \n",
    "                    search_depth=search_depth, query_column=\"cleaned\", retrieval_model=\"BM25\", depth_of_random=RANDOM_DEPTH,\n",
    "                    what_to_add=cf.VCLAIM_ONLY, add_similarity=False,)\n",
    "\n",
    "\n",
    "df_train_set_both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dev and test sets for mono BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_set(mono_train_set_creator, query_path, bm25_run_path, bm25_evaluation_save_path, pairs_save_path, \n",
    "                    search_depth=100, query_column=\"cleaned\", retrieval_model=\"BM25\", lang=\"ar\"):\n",
    "\n",
    "\n",
    "   # 1- create run for both dev and train queries\n",
    "    print(\"Make runs and evaluation for depth \", search_depth)\n",
    "    mono_train_set_creator.search_and_evaluate(query_path=query_path, evaluation_path=bm25_evaluation_save_path, \n",
    "                        retrieval_model=retrieval_model, run_save_path=bm25_run_path, \n",
    "                        depth=search_depth, method_name=\"bm25_dev-or-test\", data_column=query_column, lang=lang)\n",
    "    print(\"Done with runs and evaluation\")\n",
    "\n",
    "    # 2- create the training set\n",
    "    print(\"Creating test/dev set for mono BERT from queries path: \", query_path)\n",
    "    mono_train_set_creator.create_test_pairs(query_path, bm25_run_path, query_column=\"cleaned\", \n",
    "                    pairs_save_path=pairs_save_path,)\n",
    "    print(\"Done creating test set for mono BERT\")\n",
    "    \n",
    "    return  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dev sets from top k retrieved documents by bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 30329\n",
      "Number of terms: 36535\n",
      "Number of postings: 497017\n",
      "Number of fields: 2\n",
      "Number of tokens: 652148\n",
      "Field names: [text, title]\n",
      "Positions:   false\n",
      "\n",
      "Index has been loaded successfully\n",
      "Make runs and evaluation for depth  10\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying normalization, stemming and stop word removal for Arabic\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2021/Arabic/url_cleaned_dev_queries.xlsx\n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  20\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying normalization, stemming and stop word removal for Arabic\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2021/Arabic/url_cleaned_dev_queries.xlsx\n",
      "1000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  30\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying normalization, stemming and stop word removal for Arabic\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2021/Arabic/url_cleaned_dev_queries.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  50\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying normalization, stemming and stop word removal for Arabic\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2021/Arabic/url_cleaned_dev_queries.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "4000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  100\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying normalization, stemming and stop word removal for Arabic\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2021/Arabic/url_cleaned_dev_queries.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "4000  rows have been created so far \n",
      "5000  rows have been created so far \n",
      "6000  rows have been created so far \n",
      "7000  rows have been created so far \n",
      "8000  rows have been created so far \n",
      "Done creating test set for mono BERT\n"
     ]
    }
   ],
   "source": [
    "depths = [10, 20, 30, 50, 100,]\n",
    "mono_train_set_creator = MonoBertTrainSetCreator(qrels_file, claims_file, index_path, eval_metrics)\n",
    "\n",
    "for i in range(len(depths)):\n",
    "    depth = depths[i]\n",
    "    bm25_run_path =  \"./data/runs/bm25_dev_query_ar_clef_2021_depth_\" + str(depth) + \".tsv\"\n",
    "    mono_bert_dev_set = \"./data/CLEF_2021/Arabic/dev_sets/ar-clef2021-mono_bert_dev_set_top_\"  + str(depth) + \".tsv\"\n",
    "\n",
    "    create_test_set(mono_train_set_creator, dev_query_path, bm25_run_path, \n",
    "                    bm25_evaluation_save_path=\"\", pairs_save_path=mono_bert_dev_set, \n",
    "                    search_depth=depth, query_column=\"cleaned\", retrieval_model=\"BM25\", lang=\"ar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------\n",
    "## Create test sets from top k documents retrieved by bm25\n",
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 30329\n",
      "Number of terms: 36535\n",
      "Number of postings: 497017\n",
      "Number of fields: 2\n",
      "Number of tokens: 652148\n",
      "Field names: [text, title]\n",
      "Positions:   false\n",
      "\n",
      "Index has been loaded successfully\n",
      "Make runs and evaluation for depth  10\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying normalization, stemming and stop word removal for Arabic\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2021/Arabic/url_cleaned_test_queries.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  20\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying normalization, stemming and stop word removal for Arabic\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2021/Arabic/url_cleaned_test_queries.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "4000  rows have been created so far \n",
      "5000  rows have been created so far \n",
      "Done creating test set for mono BERT\n"
     ]
    }
   ],
   "source": [
    "depths = [10, 20, 30, 50, 100,]\n",
    "mono_train_set_creator = MonoBertTrainSetCreator(qrels_file, claims_file, index_path, eval_metrics)\n",
    "\n",
    "for i in range(len(depths)):\n",
    "    depth = depths[i]\n",
    "    bm25_run_path =  \"./data/runs/bm25_test_query_ar_clef_2021_depth_\" + str(depth) + \".tsv\"\n",
    "    mono_bert_test_set_path = \"./data/CLEF_2021/Arabic/dev_sets/2021-mono_bert_test_set_top_\"  + str(depth) + \".tsv\"\n",
    "\n",
    "    create_test_set(mono_train_set_creator, test_query_path, bm25_run_path, \n",
    "                    bm25_evaluation_save_path=\"\", pairs_save_path=mono_bert_test_set_path, \n",
    "                    search_depth=depth, query_column=\"cleaned\", retrieval_model=\"BM25\", lang=\"ar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the maximum length needed to tokenize a pair of query and document in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of a tokenized example is :  474\n",
      "Sentence Lengths: \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents that are bigger than max legnth \n",
      "544\n"
     ]
    }
   ],
   "source": [
    "# measure the maximum length needed to tokenize a triplet\n",
    "def get_pairs_max_length(data_path, tokenizer, max_length):\n",
    "    df = Utils.read_file(data_path)\n",
    "    lengths = []\n",
    "    maxl = 0\n",
    "    for i, row in df.iterrows():\n",
    "        query = row[TWEET_TEXT_COLUMN]\n",
    "        document = row[VCLAIM]\n",
    "        row_len = len(tokenizer.tokenize(query)) +len(tokenizer.tokenize(document))\n",
    "        lengths.append(row_len)\n",
    "        maxl = max(maxl, row_len)\n",
    "    print(\"Maximum length of a tokenized example is : \", maxl)\n",
    "    print(\"Sentence Lengths: \")\n",
    "    plt.hist(lengths ,bins=range(0,256,2))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Number of documents that are bigger than max legnth \")\n",
    "    print(sum([length > max_length for length in lengths]))\n",
    "    \n",
    "    return \n",
    "\n",
    "BERT = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT)\n",
    "get_pairs_max_length(mono_bert_train_both_vclaim_and_title_set ,tokenizer, max_length=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the model on dev set to figure out the best depth of the initial retrieved list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Testing ...\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/AR-2021-mono_AraBERT-seed-61168821_rerankded_dev-data-depth_10.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/AR2021/AR-2021-mono_AraBERT-seed-61168821_rerankded_dev-data-depth_10.tsv\n",
      "Running Testing ...\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/AR-2021-mono_AraBERT-seed-61168821_rerankded_dev-data-depth_20.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/AR2021/AR-2021-mono_AraBERT-seed-61168821_rerankded_dev-data-depth_20.tsv\n",
      "Running Testing ...\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/AR-2021-mono_AraBERT-seed-61168821_rerankded_dev-data-depth_30.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/AR2021/AR-2021-mono_AraBERT-seed-61168821_rerankded_dev-data-depth_30.tsv\n",
      "Running Testing ...\n",
      "  Batch   100  of    133.    Elapsed: 0:00:51.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/AR-2021-mono_AraBERT-seed-61168821_rerankded_dev-data-depth_50.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/AR2021/AR-2021-mono_AraBERT-seed-61168821_rerankded_dev-data-depth_50.tsv\n",
      "Running Testing ...\n",
      "  Batch   100  of    265.    Elapsed: 0:00:51.\n",
      "  Batch   200  of    265.    Elapsed: 0:01:42.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/AR-2021-mono_AraBERT-seed-61168821_rerankded_dev-data-depth_100.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/AR2021/AR-2021-mono_AraBERT-seed-61168821_rerankded_dev-data-depth_100.tsv\n"
     ]
    }
   ],
   "source": [
    "depth_evaluation_file = \"./data/evaluation/AR-2021-mono-bert-depth-evaluation-on-dev-set.xlsx\"\n",
    "bert_models = [ {\"model_path\":\"aubmindlab/bert-base-arabertv02\",\n",
    "                \"model_name\": \"AraBERT\",\n",
    "                \"learning_rate\": 2e-5,\n",
    "                \"num_of_epoch\": 4,\n",
    "                \"dropout\": 0.3},\n",
    "                ]\n",
    " \n",
    "seed = random_seeds[0]\n",
    "\n",
    "for i in range(len(bert_models)):\n",
    "    bert_model = bert_models[i][\"model_path\"]\n",
    "    model_name =  bert_models[i][\"model_name\"]\n",
    "    learning_rate =  bert_models[i][\"learning_rate\"]\n",
    "    num_of_epoch =  bert_models[i][\"num_of_epoch\"]\n",
    "    dropout =  bert_models[i][\"dropout\"]\n",
    "    hp = { # hyper parameters\n",
    "    \"model_name\": bert_model,\n",
    "    \"name\": model_name,\n",
    "    \"model_save_path\": \"./data/saved_models/AR-clef2021-\"+model_name+\"_mono_trained_model-seed-\"+ str(seed)+\".bin\",\n",
    "    \"model_training_log\": \"./data/bert_evaluation/AR-clef2021-mono_\"+model_name+\"_training_log-seed-\"+ str(seed)+\".xlsx\",\n",
    "    \"batch_size\": 32,\n",
    "    \"num_of_epochs\": num_of_epoch,\n",
    "    \"learning_rate\" : learning_rate,\n",
    "    \"dropout\": dropout,\n",
    "    \"seeds\": seed,\n",
    "    \"max_len\": 256,\n",
    "    \"curricula_type\": 0,\n",
    "    \"end_of_curriculum\": [0],\n",
    "    \"num_of_layers\": ONE_LAYER,\n",
    "    \"is_output_probability\": False, # if false, put loss_function = mono_loss, otherwise put loss_function=CrossEntropy\n",
    "    \"loss_function\": \"mono_loss\"}\n",
    "    \n",
    "    _, _, best_learning_rate, best_num_of_epochs, best_end_of_curriculum, best_dropout = mono_bert_trainer.train_mono_bert(\n",
    "                    mono_bert_train_set, mono_bert_dev_set_depth_30, hp[\"model_name\"], apply_cleaning=False, seeds=hp[\"seeds\"],\n",
    "                    trained_model_save_path=hp[\"model_save_path\"], shuffle=True, freeze_bert=False, max_len=hp[\"max_len\"], \n",
    "                    batch_size=hp[\"batch_size\"], epochs=hp[\"num_of_epochs\"], learning_rates=hp[\"learning_rate\"],\n",
    "                    is_output_probability=hp[\"is_output_probability\"], end_of_curriculums=hp[\"end_of_curriculum\"],\n",
    "                    curricula_type=hp[\"curricula_type\"], dropout=hp[\"dropout\"], hp=hp, results_path=hp[\"model_training_log\"],\n",
    "                    classifier_layers=hp[\"num_of_layers\"], what_to_eval=cf.VCLAIM_AND_TITLE)\n",
    "\n",
    "    # store the best hyperparameters\n",
    "    hp[\"num_of_epochs\"]= best_num_of_epochs\n",
    "    hp[\"learning_rate\"]= best_learning_rate\n",
    "    hp[\"end_of_curriculum\"]= best_end_of_curriculum\n",
    "    hp[\"dropout\"]= best_dropout\n",
    "\n",
    "    # measure the performance on the test set\n",
    "    depths = [10, 20, 30, 50, 100,]\n",
    "    for k in range(len(depths)):\n",
    "        depth = depths[k]\n",
    "        hp[\"test_depth\"] = depth\n",
    "\n",
    "        mono_bert_dev_set = \"./data/CLEF_2021/Arabic/dev_sets/ar-clef2021-mono_bert_dev_set_top_\"  + str(depth) + \".tsv\"\n",
    "        run_name = \"AR-2021-mono_\"+model_name+\"-seed-\"+str(seed)+\"_rerankded_dev-data-depth_\" +str(depth)+\".tsv\"\n",
    "        hp[\"reranked_data_path\"] = \"./data/runs/\" + run_name\n",
    "        hp[\"trec_run_path\"] = \"./data/runs/trec_eval/AR2021/\"+ run_name\n",
    "\n",
    "        mono_bert_tester.test_mono_bert(hp[\"model_name\"], hp[\"model_save_path\"], mono_bert_dev_set, \n",
    "                        hp[\"reranked_data_path\"],  depth_evaluation_file, max_len=hp[\"max_len\"], batch_size=hp[\"batch_size\"],\n",
    "                        dropout=hp[\"dropout\"], apply_cleaning=False, is_output_probability=hp[\"is_output_probability\"], \n",
    "                        hyper_parameters=hp,  classifier_layers=hp[\"num_of_layers\"], trec_run_path=hp[\"trec_run_path\"],\n",
    "                        what_to_test=cf.VCLAIM_AND_TITLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training sentence BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_models = [ \n",
    "                {\"model_path\":\"asafaya/bert-base-arabic\",\n",
    "                \"model_name\": \"Arabic_BERT\"},\n",
    "                {\"model_path\":\"lanwuwei/GigaBERT-v3-Arabic-and-English\",\n",
    "                \"model_name\": \"GigaBERT-v3\"},\n",
    "                {\"model_path\":\"UBC-NLP/MARBERT\",\n",
    "                \"model_name\": \"MARBERT\"},\n",
    "                {\"model_path\":\"aubmindlab/bert-base-arabertv02\",\n",
    "                \"model_name\": \"AraBERT\"},\n",
    "                {\"model_path\":\"qarib/bert-base-qarib\",\n",
    "                \"model_name\": \"QARiB\"},\n",
    "                {\"model_path\":\"kuisailab/albert-base-arabic\",\n",
    "                \"model_name\": \"Arabic-ALBERT\"},\n",
    "                ]\n",
    " \n",
    "\n",
    "num_rand_seeds  = 5\n",
    "seeds = random_seeds[:num_rand_seeds]\n",
    "for i in range(len(bert_models)):\n",
    "\n",
    "    seed = seeds[l]\n",
    "    bert_model = bert_models[i][\"model_path\"]\n",
    "    model_name =bert_models[i][\"model_name\"]\n",
    "    print(\"---------- training for model : --------\", model_name)\n",
    "    hp = { # hyper parameters\n",
    "\n",
    "    \"model_name\": bert_model,\n",
    "    \"name\": model_name +\"-seed-\"+ str(seed),\n",
    "    \"model_save_path\": \"./data/saved_models/AR-clef2021-\"+model_name+\"_mono_trained_model-seed-\"+ str(seed)+\".bin\",\n",
    "    \"model_training_log\": \"./data/bert_evaluation/AR-clef2021-mono_\"+model_name+\"_training_log-seed-\"+ str(seed)+\".xlsx\",\n",
    "    \"batch_size\": 32,\n",
    "    \"test_depth\": 30,\n",
    "    \"num_of_epochs\": [3, 4, 5], \n",
    "    \"learning_rate\" : [2e-5, 3e-5],\n",
    "    \"dropout\": [0.3, 0.4,], \n",
    "    \"seeds\": [seed],\n",
    "    \"max_len\": 256,\n",
    "    \"curricula_type\": 0,\n",
    "    \"end_of_curriculum\": [0],\n",
    "    \"num_of_layers\": ONE_LAYER,\n",
    "    \"is_output_probability\": False, # if false, put loss_function = mono_loss, otherwise put loss_function=CrossEntropy\n",
    "    \"loss_function\": \"mono_loss\"}\n",
    "   \n",
    "\n",
    "\n",
    "    _, _, best_learning_rate, best_num_of_epochs, best_end_of_curriculum, best_dropout = mono_bert_trainer.train_mono_bert(\n",
    "                        mono_bert_train_set, mono_bert_dev_set_depth_30, hp[\"model_name\"], apply_cleaning=False,  seeds=hp[\"seeds\"],\n",
    "                        trained_model_save_path=hp[\"model_save_path\"], shuffle=True, freeze_bert=False, max_len=hp[\"max_len\"], \n",
    "                        batch_size=hp[\"batch_size\"],epochs=hp[\"num_of_epochs\"], learning_rates = hp[\"learning_rate\"],\n",
    "                        is_output_probability=hp[\"is_output_probability\"],  end_of_curriculums=hp[\"end_of_curriculum\"],\n",
    "                        curricula_type=hp[\"curricula_type\"], dropout=hp[\"dropout\"], hp=hp, results_path=hp[\"model_training_log\"],\n",
    "                        classifier_layers=hp[\"num_of_layers\"], what_to_eval=cf.VCLAIM_AND_TITLE)\n",
    "\n",
    "\n",
    "\n",
    "    # store the best hyperparameters\n",
    "    hp[\"num_of_epochs\"]= best_num_of_epochs\n",
    "    hp[\"learning_rate\"]= best_learning_rate\n",
    "    hp[\"end_of_curriculum\"]= best_end_of_curriculum\n",
    "    hp[\"dropout\"]= best_dropout\n",
    "\n",
    "\n",
    "    # measure the performance on the test set\n",
    "    depths = [30,]\n",
    "    for k in range(len(depths)):\n",
    "        depth = depths[k]\n",
    "        hp[\"test_depth\"] = depth\n",
    "\n",
    "        mono_bert_test_set_path = \"./data/CLEF_2021/Arabic/dev_sets/2021-mono_bert_test_set_top_\"  + str(depth) + \".tsv\"\n",
    "        run_name = \"AR-2021-mono_\"+model_name+\"-seed-\"+str(seed)+\"_rerankded_data-depth_\" +str(depth)+\".tsv\"\n",
    "        hp[\"reranked_data_path\"] = \"./data/runs/\" + run_name\n",
    "        hp[\"trec_run_path\"] = \"./data/runs/trec_eval/AR2021/\"+ run_name\n",
    "\n",
    "        mono_bert_tester.test_mono_bert(hp[\"model_name\"], hp[\"model_save_path\"], mono_bert_test_set_path, \n",
    "                        hp[\"reranked_data_path\"],  evaluation_save_path, max_len=hp[\"max_len\"], batch_size=hp[\"batch_size\"],\n",
    "                        dropout=hp[\"dropout\"], apply_cleaning=False, is_output_probability=hp[\"is_output_probability\"], \n",
    "                        hyper_parameters=hp,  classifier_layers=hp[\"num_of_layers\"], trec_run_path=hp[\"trec_run_path\"],\n",
    "                        what_to_test=cf.VCLAIM_AND_TITLE)\n",
    "\n",
    "\n",
    "    # then we get the best hyper parameters values\n",
    "    # and we can run the training with different random seeds\n",
    "    l = 1\n",
    "    while l < num_rand_seeds:\n",
    "        seed = seeds[l]\n",
    "        l = l + 1\n",
    "        hp[\"name\"] = model_name +\"-seed-\"+ str(seed)\n",
    "        hp[\"model_save_path\"]=  \"./data/saved_models/AR-clef2021-\"+model_name+\"_mono_trained_model-seed-\"+ str(seed)+\".bin\"\n",
    "        hp[\"model_training_log\"]=\"./data/bert_evaluation/AR-clef2021-mono_\"+model_name+\"_training_log-seed-\"+ str(seed)+\".xlsx\"\n",
    "        hp[\"learning_rate\"]= [best_learning_rate]\n",
    "        hp[\"num_of_epochs\"]= [best_num_of_epochs]\n",
    "        hp[\"end_of_curriculum\"]= [best_end_of_curriculum]\n",
    "        hp[\"dropout\"]= [best_dropout]\n",
    "        hp[\"seeds\"] = [seed]\n",
    "\n",
    "\n",
    "        _, _, best_learning_rate, best_num_of_epochs, best_end_of_curriculum, best_dropout = mono_bert_trainer.train_mono_bert(\n",
    "                        mono_bert_train_set, mono_bert_dev_set_depth_30, hp[\"model_name\"], apply_cleaning=False,  seeds=hp[\"seeds\"],\n",
    "                        trained_model_save_path=hp[\"model_save_path\"], shuffle=True, freeze_bert=False, max_len=hp[\"max_len\"], \n",
    "                        batch_size=hp[\"batch_size\"],epochs=hp[\"num_of_epochs\"], learning_rates = hp[\"learning_rate\"],\n",
    "                        is_output_probability=hp[\"is_output_probability\"],  end_of_curriculums=hp[\"end_of_curriculum\"],\n",
    "                        curricula_type=hp[\"curricula_type\"], dropout=hp[\"dropout\"], hp=hp, results_path=hp[\"model_training_log\"],\n",
    "                        classifier_layers=hp[\"num_of_layers\"], what_to_eval=cf.VCLAIM_AND_TITLE)\n",
    "\n",
    "\n",
    "        # store the best hyperparameters\n",
    "        hp[\"num_of_epochs\"]= best_num_of_epochs\n",
    "        hp[\"learning_rate\"]= best_learning_rate\n",
    "        hp[\"end_of_curriculum\"]= best_end_of_curriculum\n",
    "        hp[\"dropout\"]= best_dropout\n",
    "\n",
    "\n",
    "        # measure the performance on the test set\n",
    "        depths = [30,]\n",
    "        for k in range(len(depths)):\n",
    "            depth = depths[k]\n",
    "            hp[\"test_depth\"] = depth\n",
    "\n",
    "            mono_bert_test_set_path = \"./data/CLEF_2021/Arabic/dev_sets/2021-mono_bert_test_set_top_\"  + str(depth) + \".tsv\"\n",
    "            run_name = \"AR-2021-mono_\"+model_name+\"-seed-\"+str(seed)+\"_rerankded_data-depth_\" +str(depth)+\".tsv\"\n",
    "            hp[\"reranked_data_path\"] = \"./data/runs/\" + run_name\n",
    "            hp[\"trec_run_path\"] = \"./data/runs/trec_eval/AR2021/\"+ run_name\n",
    "\n",
    "            mono_bert_tester.test_mono_bert(hp[\"model_name\"], hp[\"model_save_path\"], mono_bert_test_set_path, \n",
    "                        hp[\"reranked_data_path\"],  evaluation_save_path, max_len=hp[\"max_len\"], batch_size=hp[\"batch_size\"],\n",
    "                        dropout=hp[\"dropout\"], apply_cleaning=False, is_output_probability=hp[\"is_output_probability\"], \n",
    "                        hyper_parameters=hp,  classifier_layers=hp[\"num_of_layers\"], trec_run_path=hp[\"trec_run_path\"],\n",
    "                        what_to_test=cf.VCLAIM_AND_TITLE)\n",
    "                    \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform significance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>P@1</th>\n",
       "      <th>AP@5</th>\n",
       "      <th>P@1 +</th>\n",
       "      <th>P@1 -</th>\n",
       "      <th>P@1 p-value</th>\n",
       "      <th>AP@5 +</th>\n",
       "      <th>AP@5 -</th>\n",
       "      <th>AP@5 p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.908046</td>\n",
       "      <td>0.908046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AraBERT-</td>\n",
       "      <td>0.954023</td>\n",
       "      <td>0.947946</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.053632e-03</td>\n",
       "      <td>24.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.089077e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arabic_BERT</td>\n",
       "      <td>0.934866</td>\n",
       "      <td>0.932163</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.084347e-01</td>\n",
       "      <td>21.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>7.359127e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GigaBERT-v3</td>\n",
       "      <td>0.938697</td>\n",
       "      <td>0.938825</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.920786e-02</td>\n",
       "      <td>22.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.112389e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MARBERT</td>\n",
       "      <td>0.743295</td>\n",
       "      <td>0.767412</td>\n",
       "      <td>10.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.639120e-08</td>\n",
       "      <td>13.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.549125e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>QARiB</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>0.902735</td>\n",
       "      <td>10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2.400456e-01</td>\n",
       "      <td>21.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.110187e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Arabic-ALBERT</td>\n",
       "      <td>0.923372</td>\n",
       "      <td>0.921456</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.948010e-01</td>\n",
       "      <td>20.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.473279e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name       P@1      AP@5  P@1 +  P@1 -   P@1 p-value  AP@5 +  \\\n",
       "0       baseline  0.908046  0.908046    NaN    NaN           NaN     NaN   \n",
       "1       AraBERT-  0.954023  0.947946   16.0    4.0  7.053632e-03    24.0   \n",
       "2    Arabic_BERT  0.934866  0.932163   13.0    6.0  1.084347e-01    21.0   \n",
       "3    GigaBERT-v3  0.938697  0.938825   13.0    5.0  5.920786e-02    22.0   \n",
       "4        MARBERT  0.743295  0.767412   10.0   53.0  2.639120e-08    13.0   \n",
       "5          QARiB  0.885057  0.902735   10.0   16.0  2.400456e-01    21.0   \n",
       "6  Arabic-ALBERT  0.923372  0.921456   13.0    9.0  3.948010e-01    20.0   \n",
       "\n",
       "   AP@5 -  AP@5 p-value  \n",
       "0     NaN           NaN  \n",
       "1     9.0  4.089077e-03  \n",
       "2    19.0  7.359127e-02  \n",
       "3    12.0  2.112389e-02  \n",
       "4    73.0  1.549125e-09  \n",
       "5    26.0  7.110187e-01  \n",
       "6    21.0  3.473279e-01  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_run= \"./data/runs/bigIR_test_run_.tsv\"\n",
    "\n",
    "# baseline_run= \"./data/runs/trec_eval/bigIR_AR_2021_TEST.tsv\"\n",
    "other_runs_paths = [\n",
    "\"./data/runs/trec_eval/AR2021/AR-2021-mono_AraBERT-seed-22612812_rerankded_data-depth_30.tsv\",\n",
    "\"./data/runs/trec_eval/AR2021/AR-2021-mono_Arabic_BERT-seed-22612812_rerankded_data-depth_30.tsv\",\n",
    "\"./data/runs/trec_eval/AR2021/AR-2021-mono_GigaBERT-v3-seed-22612812_rerankded_data-depth_30.tsv\",\n",
    "\"./data/runs/trec_eval/AR2021/AR-2021-mono_MARBERT-seed-21228945_rerankded_data-depth_30.tsv\",\n",
    "\"./data/runs/trec_eval/AR2021/AR-2021-mono_QARiB-seed-21228945_rerankded_data-depth_30.tsv\",\n",
    "\"./data/runs/trec_eval/AR2021/AR-2021-mono_Arabic-ALBERT-seed-21228945_rerankded_data-depth_30.tsv\",]\n",
    "other_runs_names = [\n",
    "\"AraBERT-\",\n",
    "\"Arabic_BERT\",\n",
    "\"GigaBERT-v3\",\n",
    "\"MARBERT\",\n",
    "\"QARiB\",\n",
    "\"Arabic-ALBERT\",\n",
    "]\n",
    "\n",
    "df_eval = mono_bert_tester.perform_t_test(baseline_run, other_runs_paths, other_runs_names, qrels_file, test_query_path, \n",
    "                eval_metrics=[MAP@5, P@1], save_path=\"./data/arabic-t-test.xlsx\",)\n",
    "\n",
    "df_eval\n",
    "# method1: fdr_tsbh - FDR 2-stage Benjamini-Hochberg\n",
    "# method2: fdr_tsbky -FDR 2-stage Benjamini-Krieger-Yekutieli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ True, False,  True,  True, False, False]),\n",
       " array([1.22672323e-02, 1.10386910e-01, 4.22477899e-02, 9.29474900e-09,\n",
       "        7.11018709e-01, 4.16793526e-01]),\n",
       " 0.008512444610847103,\n",
       " 0.008333333333333333)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply the correction to solve the multiple comparison problem\n",
    "map_pval= df_eval[\"AP@5 p-value\"]\n",
    "statsmodels.stats.multitest.multipletests(map_pval[1:7], alpha=0.05, method=\"fdr_bh\", is_sorted=False)\n",
    "# method can have the following values:\n",
    "# `bonferroni` : one-step correction\n",
    "# `sidak` : one-step correction\n",
    "# `holm-sidak` : step down method using Sidak adjustments\n",
    "# `holm` : step-down method using Bonferroni adjustments\n",
    "# `simes-hochberg` : step-up method  (independent)\n",
    "# `hommel` : closed method based on Simes tests (non-negative)\n",
    "# `fdr_bh` : Benjamini/Hochberg  (non-negative)\n",
    "# `fdr_by` : Benjamini/Yekutieli (negative)\n",
    "# `fdr_tsbh` : two stage fdr correction (non-negative)\n",
    "# `fdr_tsbky` : two stage fdr correction (non-negative)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('tweetEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32ea63ce396b580d232aa3acb554bdf20f8441e7d8ed4d44b8e0f0e042cd90e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
