{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 12:14:51.211540: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-29 12:14:51.211583: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# import needed libraries\n",
    "import pandas as pd\n",
    "import helper.Utils as Utils\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "from mono_bert_train import MonoBertTrainer\n",
    "from mono_bert_test import MonoBertTester\n",
    "import configure  as cf\n",
    "from train_set_creator import MonoBertTrainSetCreator\n",
    "from pyterrier.measures import RR, R, Rprec, P, MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling PRF in pyterier\n",
      "PyTerrier 0.6.0 has loaded Terrier 5.6 (built by craigmacdonald on 2021-09-17 13:27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not pt.started():\n",
    "    print(\"Enabling PRF in pyterier\")\n",
    "    # In this lab, we need to specify that we start PyTerrier with PRF enabled\n",
    "    pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define some constants.\n",
    "DECIMAL_ROUND = 5\n",
    "RANK = cf.RANK\n",
    "SCORE = cf.SCORE\n",
    "TWEET_ID_COLUMN = cf.TWEET_ID_COLUMN\n",
    "TWEET_TEXT_COLUMN = cf.TWEET_TEXT_COLUMN\n",
    "VCLAIM_ID = cf.VCLAIM_ID\n",
    "VCLAIM = cf.VCLAIM\n",
    "TITLE = cf.TITLE \n",
    "LABEL = cf.LABEL\n",
    "QUERY = cf.QUERY\n",
    "QID = cf.QID\n",
    "\n",
    "claims_file = cf.ENG_CLEF_2020_VCLAIMS\n",
    "index_path = \"./indexes/en-clef-2020-index-multi-field/data.properties\"\n",
    "evaluation_save_path = \"./data/clef-2020-mono-bert-evaluations.xlsx\"\n",
    "\n",
    "train_query_path=cf.ENG_CLEF_2020_URL_CLEANED_TRAIN_QUERIES\n",
    "raw_dev_query = cf.ENG_CLEF_2020_DEV_QUERIES\n",
    "expanded_dev_query = cf.ENG_CLEF_2020_URL_CLEANED_DEV_QUERIES\n",
    "dev_query_path = expanded_dev_query\n",
    "test_query_path =cf.ENG_CLEF_2020_URL_CLEANED_TEST_QUERIES\n",
    "qrels_file=cf.ENG_CLEF_2020_QRELS\n",
    "\n",
    "eval_metrics=[\"map\",MAP@5, P@1, RR, Rprec, R@5, R@10, R@20, R@50, RR@5]\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "ONE_LAYER = 1\n",
    "TWO_LAYERS = 2\n",
    "NUM_OF_EXTRA_NEGATIVE_EXAMPLES = 1\n",
    "\n",
    "\n",
    "dev_bm25_run = \"./data/runs/bm25_dev_query_en_clef_2020.tsv\"\n",
    "train_bm25_run = \"./data/runs/bm25_train_query_en_clef_2020.tsv\"\n",
    "mono_bert_train_set = \"./data/CLEF_2020/train_sets/mono_bert_train_set-depth-20.xlsx\"\n",
    "mono_bert_dev_set_depth_20 =\"./data/CLEF_2020/dev_sets/en-clef2020-mono_bert_dev_set_top_20.tsv\"\n",
    "SEARCH_DEPTH = 100\n",
    "RANDOM_DEPTH = 20 # depth from which negative pairs will be randomly selected from top k documents retrieved from a retrieval model\n",
    "\n",
    "\n",
    "random_seeds = [61168821, 129995678, 22612812, 146764631, 21228945, 94412880, 204110176, 6155814, 187372311, 117623077,]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vclaim</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>122 detainees released from confinement at Gua...</td>\n",
       "      <td>Did 122 Prisoners Released from Guantanamo by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70 per cent of the persons arrested during pro...</td>\n",
       "      <td>70% of Arrested Charlotte Protesters Are Out-o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A \"Trump and Obama by the Numbers\" meme recoun...</td>\n",
       "      <td>Does This Meme Accurately Show ‘Trump and Obam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A \"large-scale killing\" of white farmers is ta...</td>\n",
       "      <td>Is a ‘Large-Scale Killing’ of White Farmers Un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A \"law to separate families\" was enacted prior...</td>\n",
       "      <td>Was the ‘Law to Separate Families’ Passed in 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10370</th>\n",
       "      <td>“Slime,” a do-it-yourself gooey craft project ...</td>\n",
       "      <td>Does the “Slime” Craze Bring Serious Health Ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10371</th>\n",
       "      <td>“Sun tea” (tea brewed by being left to steep i...</td>\n",
       "      <td>Bacteria in Sun Tea Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10372</th>\n",
       "      <td>“The Real Deal,” words of wisdom about gas, ge...</td>\n",
       "      <td>Red Thomas ‘Real Deal’ Letter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10373</th>\n",
       "      <td>“Valentine’s Day” worm.</td>\n",
       "      <td>Valentine’s Day Worm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10374</th>\n",
       "      <td>“WTC Survivor.”</td>\n",
       "      <td>WTC Survivor Virus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10375 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      vclaim  \\\n",
       "vclaim_id                                                      \n",
       "0          122 detainees released from confinement at Gua...   \n",
       "1          70 per cent of the persons arrested during pro...   \n",
       "2          A \"Trump and Obama by the Numbers\" meme recoun...   \n",
       "3          A \"large-scale killing\" of white farmers is ta...   \n",
       "4          A \"law to separate families\" was enacted prior...   \n",
       "...                                                      ...   \n",
       "10370      “Slime,” a do-it-yourself gooey craft project ...   \n",
       "10371      “Sun tea” (tea brewed by being left to steep i...   \n",
       "10372      “The Real Deal,” words of wisdom about gas, ge...   \n",
       "10373                                “Valentine’s Day” worm.   \n",
       "10374                                        “WTC Survivor.”   \n",
       "\n",
       "                                                       title  \n",
       "vclaim_id                                                     \n",
       "0          Did 122 Prisoners Released from Guantanamo by ...  \n",
       "1          70% of Arrested Charlotte Protesters Are Out-o...  \n",
       "2          Does This Meme Accurately Show ‘Trump and Obam...  \n",
       "3          Is a ‘Large-Scale Killing’ of White Farmers Un...  \n",
       "4          Was the ‘Law to Separate Families’ Passed in 1...  \n",
       "...                                                      ...  \n",
       "10370      Does the “Slime” Craze Bring Serious Health Ri...  \n",
       "10371                               Bacteria in Sun Tea Risk  \n",
       "10372                          Red Thomas ‘Real Deal’ Letter  \n",
       "10373                                   Valentine’s Day Worm  \n",
       "10374                                     WTC Survivor Virus  \n",
       "\n",
       "[10375 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read verified claims file\n",
    "df_claim = Utils.read_file(claims_file)\n",
    "df_claim[VCLAIM_ID]= df_claim[VCLAIM_ID].astype(str)\n",
    "df_claim.set_index(VCLAIM_ID, inplace=True)\n",
    "df_claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create multi-fields index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 10375\n",
      "Number of terms: 13271\n",
      "Number of postings: 118516\n",
      "Number of fields: 2\n",
      "Number of tokens: 162893\n",
      "Field names: [text, title]\n",
      "Positions:   false\n",
      "\n",
      "Index has been loaded successfully\n"
     ]
    }
   ],
   "source": [
    "def load_index(index_path):\n",
    "    try:\n",
    "            # first load the index\n",
    "        multi_field_index = pt.IndexFactory.of(index_path)\n",
    "        # call getCollectionStatistics() to check the stats\n",
    "        print(multi_field_index.getCollectionStatistics().toString())\n",
    "        print(\"Index has been loaded successfully\")\n",
    "        return multi_field_index\n",
    "    except Exception as e:\n",
    "        print('Cannot load the index, check exception details {}'.format(e))\n",
    "        return []\n",
    "\n",
    "\n",
    "multi_field_index = load_index(index_path=index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load QRELs file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>Q0</th>\n",
       "      <th>docno</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>394</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>670</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>670</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>1193</td>\n",
       "      <td>0</td>\n",
       "      <td>579</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>1194</td>\n",
       "      <td>0</td>\n",
       "      <td>9588</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>1195</td>\n",
       "      <td>0</td>\n",
       "      <td>5455</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>1196</td>\n",
       "      <td>0</td>\n",
       "      <td>5338</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>1197</td>\n",
       "      <td>0</td>\n",
       "      <td>5553</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1198 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       qid  Q0 docno  label\n",
       "0        1   0   394      1\n",
       "1        2   0   670      1\n",
       "2        3   0   670      1\n",
       "3        4   0   141      1\n",
       "4        5   0    83      1\n",
       "...    ...  ..   ...    ...\n",
       "1193  1193   0   579      1\n",
       "1194  1194   0  9588      1\n",
       "1195  1195   0  5455      1\n",
       "1196  1196   0  5338      1\n",
       "1197  1197   0  5553      1\n",
       "\n",
       "[1198 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_qrels(qrels_file):\n",
    "    df_qrels = pd.read_csv(qrels_file, sep=\"\\t\", names=[\"qid\", \"Q0\", \"docno\", LABEL])\n",
    "    df_qrels[\"qid\"]=df_qrels[\"qid\"].astype(str)\n",
    "    df_qrels[\"docno\"]=df_qrels[\"docno\"].astype(str)\n",
    "    return df_qrels\n",
    "\n",
    "df_qrels = get_qrels(qrels_file)\n",
    "df_qrels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful functions for creating the train, dev and test sets in monoBERT fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_set(mono_train_set_creator, train_query_path, train_bm25_run_path, bm25_evaluation_save_path,train_set_save_path, \n",
    "                    search_depth=100, query_column=\"cleaned\", retrieval_model=\"BM25\", depth_of_random=20,\n",
    "                    what_to_add=cf.VCLAIM_AND_TITLE,  add_similarity=False, lang=\"en\"):\n",
    "\n",
    "\n",
    "   # 1- create run for both dev and train queries\n",
    "    print(\"Make runs and evaluation\")\n",
    "    mono_train_set_creator.search_and_evaluate(query_path=train_query_path, evaluation_path=bm25_evaluation_save_path, \n",
    "                        retrieval_model=retrieval_model, run_save_path=train_bm25_run_path, \n",
    "                        depth=search_depth, method_name=\"bm25_train+dev\", data_column=query_column, lang=lang)\n",
    "    print(\"Done with runs and evaluation\")\n",
    "\n",
    "    # 2- create the training set\n",
    "    print(\"Creating train set\")\n",
    "    mono_train_set_creator.create_train_pairs(train_query_path, train_bm25_run_path, train_set_save_path, \n",
    "                            query_column=\"cleaned\", what_to_add=what_to_add, \n",
    "                            depth_of_random=depth_of_random, add_similarity=add_similarity,)\n",
    "    print(\"Done creating train set\")\n",
    "\n",
    "\n",
    "\n",
    "# create test set suitable for mono BERT re-ranker\n",
    "def create_test_set(mono_train_set_creator, query_path, run_path, bm25_evaluation_save_path, pairs_save_path, \n",
    "                    search_depth=100, query_column=\"cleaned\", retrieval_model=\"BM25\", lang=\"en\"):\n",
    "\n",
    "\n",
    "   # 1- create run for both dev and train queries\n",
    "    print(\"Make runs and evaluation for depth \", search_depth)\n",
    "    mono_train_set_creator.search_and_evaluate(query_path=query_path, evaluation_path=bm25_evaluation_save_path, \n",
    "                        retrieval_model=retrieval_model, run_save_path=run_path, \n",
    "                        depth=search_depth, method_name=\"bm25_dev-or-test\", data_column=query_column, lang=lang)\n",
    "    print(\"Done with runs and evaluation\")\n",
    "\n",
    "    # 2- create the training set\n",
    "    print(\"Creating test/dev set for mono BERT from queries path: \", query_path)\n",
    "    mono_train_set_creator.create_test_pairs(query_path, run_path, query_column=\"cleaned\", \n",
    "                    pairs_save_path=pairs_save_path,)\n",
    "    print(\"Done creating test set for mono BERT\")\n",
    "    \n",
    "    return  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training data for mono BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 10375\n",
      "Number of terms: 13271\n",
      "Number of postings: 118516\n",
      "Number of fields: 2\n",
      "Number of tokens: 162893\n",
      "Field names: [text, title]\n",
      "Positions:   false\n",
      "\n",
      "Index has been loaded successfully\n"
     ]
    }
   ],
   "source": [
    "mono_train_set_creator = MonoBertTrainSetCreator(qrels_file, claims_file, index_path, eval_metrics)\n",
    "mono_bert_trainer = MonoBertTrainer(train_query_path, dev_query_path, qrels_path=qrels_file,)\n",
    "mono_bert_tester = MonoBertTester(qrels_path=qrels_file, evaluation_save_path=evaluation_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make runs and evaluation\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating train set\n",
      "Processing tweet number 0 with tweet id 1 \n",
      "Processing tweet number 100 with tweet id 125 \n",
      "Processing tweet number 200 with tweet id 245 \n",
      "Processing tweet number 300 with tweet id 367 \n",
      "Processing tweet number 400 with tweet id 497 \n",
      "Processing tweet number 500 with tweet id 615 \n",
      "Processing tweet number 600 with tweet id 748 \n",
      "Processing tweet number 700 with tweet id 881 \n",
      "Done creating train set\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_train_set_both = create_train_set(mono_train_set_creator, train_query_path, train_bm25_run, \"\",\n",
    "                    train_set_save_path=mono_bert_train_set, \n",
    "                    search_depth=SEARCH_DEPTH, query_column=\"cleaned\", retrieval_model=\"BM25\", depth_of_random=RANDOM_DEPTH,\n",
    "                    what_to_add=cf.VCLAIM_AND_TITLE, add_similarity=False,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dev sets from top k retrieved documents by bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make runs and evaluation for depth  10\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2020/url_cleaned_dev_queries.xlsx\n",
      "1000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  20\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2020/url_cleaned_dev_queries.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  30\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2020/url_cleaned_dev_queries.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "4000  rows have been created so far \n",
      "5000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  50\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2020/url_cleaned_dev_queries.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "4000  rows have been created so far \n",
      "5000  rows have been created so far \n",
      "6000  rows have been created so far \n",
      "7000  rows have been created so far \n",
      "8000  rows have been created so far \n",
      "9000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  100\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2020/url_cleaned_dev_queries.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "4000  rows have been created so far \n",
      "5000  rows have been created so far \n",
      "6000  rows have been created so far \n",
      "7000  rows have been created so far \n",
      "8000  rows have been created so far \n",
      "9000  rows have been created so far \n",
      "10000  rows have been created so far \n",
      "11000  rows have been created so far \n",
      "12000  rows have been created so far \n",
      "13000  rows have been created so far \n",
      "14000  rows have been created so far \n",
      "15000  rows have been created so far \n",
      "16000  rows have been created so far \n",
      "17000  rows have been created so far \n",
      "18000  rows have been created so far \n",
      "19000  rows have been created so far \n",
      "Done creating test set for mono BERT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "depths = [10, 20, 30, 50, 100,]\n",
    "for i in range(len(depths)):\n",
    "    depth = depths[i]\n",
    "    bm25_run_path =  \"./data/runs/bm25_dev_query_en_clef_2020_depth_\" + str(depth) + \".tsv\"\n",
    "    mono_bert_dev_set = \"./data/CLEF_2020/dev_sets/en-clef2020-mono_bert_dev_set_top_\" + str(depth) + \".tsv\"\n",
    "\n",
    "    create_test_set(mono_train_set_creator, dev_query_path, bm25_run_path, \n",
    "                    bm25_evaluation_save_path=\"\", pairs_save_path=mono_bert_dev_set, \n",
    "                    search_depth=depth, query_column=\"cleaned\", retrieval_model=\"BM25\", lang=\"en\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------\n",
    "## Create test sets from top k documents retrieved by bm25\n",
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make runs and evaluation for depth  10\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2020/url_cleaned_test_queries.xlsx\n",
      "1000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  20\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2020/url_cleaned_test_queries.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  30\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2020/url_cleaned_test_queries.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "4000  rows have been created so far \n",
      "5000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  50\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2020/url_cleaned_test_queries.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "4000  rows have been created so far \n",
      "5000  rows have been created so far \n",
      "6000  rows have been created so far \n",
      "7000  rows have been created so far \n",
      "8000  rows have been created so far \n",
      "9000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  100\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2020/url_cleaned_test_queries.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "4000  rows have been created so far \n",
      "5000  rows have been created so far \n",
      "6000  rows have been created so far \n",
      "7000  rows have been created so far \n",
      "8000  rows have been created so far \n",
      "9000  rows have been created so far \n",
      "10000  rows have been created so far \n",
      "11000  rows have been created so far \n",
      "12000  rows have been created so far \n",
      "13000  rows have been created so far \n",
      "14000  rows have been created so far \n",
      "15000  rows have been created so far \n",
      "16000  rows have been created so far \n",
      "17000  rows have been created so far \n",
      "18000  rows have been created so far \n",
      "19000  rows have been created so far \n",
      "Done creating test set for mono BERT\n"
     ]
    }
   ],
   "source": [
    "depths = [10, 20, 30, 50, 100,]\n",
    "\n",
    "for i in range(len(depths)):\n",
    "    depth = depths[i]\n",
    "    bm25_run_path =  \"./data/runs/bm25_test_query_en_clef_2020_depth_\" + str(depth) + \".tsv\"\n",
    "    mono_bert_test_set_path = \"./data/CLEF_2020/test_sets/mono_bert_test_set_top_\" + str(depth) + \".tsv\"\n",
    "\n",
    "    \n",
    "    create_test_set(mono_train_set_creator, test_query_path, bm25_run_path, \n",
    "                    bm25_evaluation_save_path=\"\", pairs_save_path=mono_bert_test_set_path, \n",
    "                    search_depth=depth, query_column=\"cleaned\", retrieval_model=\"BM25\", lang=\"en\")\n",
    "\n",
    "\n",
    "\n",
    "                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the maximum length needed to tokenize a pair of query and document in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of a tokenized example is :  222\n",
      "Sentence Lengths: \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents that are bigger than max legnth \n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# measure the maximum length needed to tokenize a triplet\n",
    "def get_pairs_max_length(data_path, tokenizer, max_length):\n",
    "    df = Utils.read_file(data_path)\n",
    "    lengths = []\n",
    "    maxl = 0\n",
    "    for i, row in df.iterrows():\n",
    "        query = row[TWEET_TEXT_COLUMN]\n",
    "        document = row[VCLAIM]\n",
    "        row_len = len(tokenizer.tokenize(query)) +len(tokenizer.tokenize(document))\n",
    "        lengths.append(row_len)\n",
    "        maxl = max(maxl, row_len)\n",
    "    print(\"Maximum length of a tokenized example is : \", maxl)\n",
    "    print(\"Sentence Lengths: \")\n",
    "    plt.hist(lengths ,bins=range(0,256,2))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Number of documents that are bigger than max legnth \")\n",
    "    print(sum([length > max_length for length in lengths]))\n",
    "    \n",
    "    return \n",
    "\n",
    "BERT = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT)\n",
    "get_pairs_max_length(mono_bert_train_set ,tokenizer, max_length=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the model on dev set to figure out the best depth of the initial retrieved list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- training for model : -------- paraphrase-mpnet-base-v2\n",
      "device:  cuda:0\n",
      "train size  3204\n",
      "dev size  3940\n",
      "Epoch 1/4\n",
      "----------\n",
      "  Batch    40  of    101.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:03.\n",
      "\n",
      "  correct_predictions: 2535.00\n",
      "  Accuracy : 0.79\n",
      "  Average training loss: 0.45\n",
      "relevance results F1 score  0.7767767767767768  precision 0.8344086021505376 recall 0.7265917602996255\n",
      " Macro F1 0.7903233048351781 Weighted F1 0.7903233048351781 Accuracy 0.7911985018726592\n",
      "Train loss 0.4467813774279439 accuracy 0.7911985018726592\n",
      "Running Evaluation...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.81\n",
      "  Accuracy: 0.93\n",
      "  Average Validation loss: 0.23 \n",
      "\n",
      "Dev loss 0.23071459707834066 accuracy 0.9286802030456853 eval_measures {'map': 0.8079751276227837, 'AP@5': 0.8058375634517766, 'P@1': 0.7360406091370558, 'RR': 0.8079751276227837, 'Rprec': 0.7360406091370558, 'R@5': 0.9035532994923858, 'R@10': 0.9086294416243654, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8100676818950933}\n",
      "Epoch 2/4\n",
      "----------\n",
      "  Batch    40  of    101.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:03.\n",
      "\n",
      "  correct_predictions: 2888.00\n",
      "  Accuracy : 0.90\n",
      "  Average training loss: 0.25\n",
      "relevance results F1 score  0.8957783641160949  precision 0.9496503496503497 recall 0.8476903870162297\n",
      " Macro F1 0.9010882341907489 Weighted F1 0.9010882341907488 Accuracy 0.9013732833957553\n",
      "Train loss 0.2534358412174895 accuracy 0.9013732833957553\n",
      "Running Evaluation...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.88\n",
      "  Accuracy: 0.92\n",
      "  Average Validation loss: 0.26 \n",
      "\n",
      "Dev loss 0.2567694663640953 accuracy 0.9238578680203046 eval_measures {'map': 0.8780517283055355, 'AP@5': 0.8773265651438239, 'P@1': 0.8324873096446701, 'RR': 0.8780517283055355, 'Rprec': 0.8324873096446701, 'R@5': 0.9238578680203046, 'R@10': 0.9289340101522843, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8849407783417936}\n",
      "Epoch 3/4\n",
      "----------\n",
      "  Batch    40  of    101.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:03.\n",
      "\n",
      "  correct_predictions: 2965.00\n",
      "  Accuracy : 0.93\n",
      "  Average training loss: 0.18\n",
      "relevance results F1 score  0.9219719229513549  precision 0.9664613278576317 recall 0.8813982521847691\n",
      " Macro F1 0.9252609988448852 Weighted F1 0.9252609988448852 Accuracy 0.9254057428214731\n",
      "Train loss 0.18192373028043474 accuracy 0.9254057428214731\n",
      "Running Evaluation...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:27.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.87\n",
      "  Accuracy: 0.94\n",
      "  Average Validation loss: 0.22 \n",
      "\n",
      "Dev loss 0.2159332523212558 accuracy 0.9368020304568528 eval_measures {'map': 0.8652405124486342, 'AP@5': 0.8637901861252114, 'P@1': 0.817258883248731, 'RR': 0.8652405124486343, 'Rprec': 0.817258883248731, 'R@5': 0.9187817258883249, 'R@10': 0.9289340101522843, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8680203045685281}\n",
      "Epoch 4/4\n",
      "----------\n",
      "  Batch    40  of    101.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:03.\n",
      "\n",
      "  correct_predictions: 3021.00\n",
      "  Accuracy : 0.94\n",
      "  Average training loss: 0.13\n",
      "relevance results F1 score  0.9403714565004887  precision 0.983640081799591 recall 0.900749063670412\n",
      " Macro F1 0.9427823140543773 Weighted F1 0.9427823140543773 Accuracy 0.9428838951310862\n",
      "Train loss 0.12531340072429417 accuracy 0.9428838951310862\n",
      "Running Evaluation...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.86\n",
      "  Accuracy: 0.95\n",
      "  Average Validation loss: 0.20 \n",
      "\n",
      "Dev loss 0.20210043183197418 accuracy 0.9477157360406092 eval_measures {'map': 0.8632463137539279, 'AP@5': 0.8625211505922166, 'P@1': 0.8121827411167513, 'RR': 0.8632463137539279, 'Rprec': 0.8121827411167513, 'R@5': 0.9238578680203046, 'R@10': 0.9289340101522843, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.866751269035533}\n",
      " -------------------Training complete for learning rate 2e-05 and number of epochs 4 ------ \n",
      "Train loss 0.12531340072429417 accuracy 0.9428838951310862 end_of_curriculum 0\n",
      "Dev loss 0.20210043183197418 dev 0.9477157360406092 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      " ------------------- Overall Training complete! ---------------------------------------\n",
      "Best learning rate 2e-05 and number of epochs 4 end_of_curriculum 0 ------ \n",
      "Best Dev loss 0.20210043183197418 and dev accuracy  0.9477157360406092 best map {'map': 0.8632463137539279, 'AP@5': 0.8625211505922166, 'P@1': 0.8121827411167513, 'RR': 0.8632463137539279, 'Rprec': 0.8121827411167513, 'R@5': 0.9238578680203046, 'R@10': 0.9289340101522843, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.866751269035533}\n",
      "\n",
      " ------------------------------------------------------------ \n",
      "Running Testing ...\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_paraphrase-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_10.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_paraphrase-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_10.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8554991539763112, 'AP@5': 0.8554991539763112, 'P@1': 0.8071065989847716, 'RR': 0.8554991539763112, 'Rprec': 0.8071065989847716, 'R@5': 0.9137055837563451, 'R@10': 0.9137055837563451, 'R@20': 0.9137055837563451, 'R@50': 0.9137055837563451, 'RR@5': 0.8614213197969544}\n",
      "Running Testing ...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:54.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_paraphrase-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_20.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_paraphrase-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_20.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8660744500846023, 'AP@5': 0.8660744500846023, 'P@1': 0.817258883248731, 'RR': 0.8660744500846024, 'Rprec': 0.817258883248731, 'R@5': 0.9289340101522843, 'R@10': 0.9289340101522843, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8703045685279188}\n",
      "Running Testing ...\n",
      "  Batch   100  of    185.    Elapsed: 0:00:54.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_paraphrase-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_30.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_paraphrase-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_30.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8622673434856175, 'AP@5': 0.8622673434856175, 'P@1': 0.8071065989847716, 'RR': 0.8631133671742809, 'Rprec': 0.8045685279187818, 'R@5': 0.934010152284264, 'R@10': 0.934010152284264, 'R@20': 0.934010152284264, 'R@50': 0.934010152284264, 'RR@5': 0.8673434856175973}\n",
      "Running Testing ...\n",
      "  Batch   100  of    308.    Elapsed: 0:00:54.\n",
      "  Batch   200  of    308.    Elapsed: 0:01:47.\n",
      "  Batch   300  of    308.    Elapsed: 0:02:41.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_paraphrase-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_50.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_paraphrase-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_50.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8622673434856175, 'AP@5': 0.8622673434856175, 'P@1': 0.8020304568527918, 'RR': 0.8631133671742809, 'Rprec': 0.799492385786802, 'R@5': 0.9390862944162437, 'R@10': 0.9390862944162437, 'R@20': 0.9390862944162437, 'R@50': 0.9390862944162437, 'RR@5': 0.8673434856175973}\n",
      "Running Testing ...\n",
      "  Batch   100  of    616.    Elapsed: 0:00:54.\n",
      "  Batch   200  of    616.    Elapsed: 0:01:47.\n",
      "  Batch   300  of    616.    Elapsed: 0:02:40.\n",
      "  Batch   400  of    616.    Elapsed: 0:03:33.\n",
      "  Batch   500  of    616.    Elapsed: 0:04:27.\n",
      "  Batch   600  of    616.    Elapsed: 0:05:21.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_paraphrase-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_100.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_paraphrase-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_100.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8548787366046249, 'AP@5': 0.8529610829103214, 'P@1': 0.7918781725888325, 'RR': 0.8557247602932883, 'Rprec': 0.7893401015228426, 'R@5': 0.9390862944162437, 'R@10': 0.9543147208121827, 'R@20': 0.9543147208121827, 'R@50': 0.9543147208121827, 'RR@5': 0.8580372250423012}\n",
      "---------- training for model : -------- stsb-mpnet-base-v2\n",
      "device:  cuda:0\n",
      "train size  3204\n",
      "dev size  3940\n",
      "Epoch 1/4\n",
      "----------\n",
      "  Batch    40  of    101.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:03.\n",
      "\n",
      "  correct_predictions: 2557.00\n",
      "  Accuracy : 0.80\n",
      "  Average training loss: 0.43\n",
      "relevance results F1 score  0.7905471026222078  precision 0.8211163416274377 recall 0.7621722846441947\n",
      " Macro F1 0.7978044341071269 Weighted F1 0.7978044341071268 Accuracy 0.7980649188514357\n",
      "Train loss 0.42818742957298117 accuracy 0.7980649188514357\n",
      "Running Evaluation...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.79\n",
      "  Accuracy: 0.94\n",
      "  Average Validation loss: 0.19 \n",
      "\n",
      "Dev loss 0.19026248215607577 accuracy 0.9375634517766498 eval_measures {'map': 0.7866989891101567, 'AP@5': 0.7842639593908629, 'P@1': 0.6802030456852792, 'RR': 0.7875450127988199, 'Rprec': 0.6776649746192893, 'R@5': 0.9086294416243654, 'R@10': 0.9238578680203046, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.7910321489001694}\n",
      "Epoch 2/4\n",
      "----------\n",
      "  Batch    40  of    101.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:03.\n",
      "\n",
      "  correct_predictions: 2922.00\n",
      "  Accuracy : 0.91\n",
      "  Average training loss: 0.22\n",
      "relevance results F1 score  0.9076620825147347  precision 0.9545454545454546 recall 0.8651685393258427\n",
      " Macro F1 0.9117916852645229 Weighted F1 0.9117916852645228 Accuracy 0.9119850187265918\n",
      "Train loss 0.220303895781831 accuracy 0.9119850187265918\n",
      "Running Evaluation...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.82\n",
      "  Accuracy: 0.83\n",
      "  Average Validation loss: 0.51 \n",
      "\n",
      "Dev loss 0.5103137057665135 accuracy 0.8337563451776651 eval_measures {'map': 0.8151438240270726, 'AP@5': 0.8151438240270726, 'P@1': 0.7106598984771574, 'RR': 0.815989847715736, 'Rprec': 0.7081218274111675, 'R@5': 0.9289340101522843, 'R@10': 0.9289340101522843, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8236040609137057}\n",
      "Epoch 3/4\n",
      "----------\n",
      "  Batch    40  of    101.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:03.\n",
      "\n",
      "  correct_predictions: 2973.00\n",
      "  Accuracy : 0.93\n",
      "  Average training loss: 0.18\n",
      "relevance results F1 score  0.9247311827956989  precision 0.967280163599182 recall 0.8857677902621723\n",
      " Macro F1 0.9277743964292959 Weighted F1 0.9277743964292959 Accuracy 0.9279026217228464\n",
      "Train loss 0.17803196640371685 accuracy 0.9279026217228464\n",
      "Running Evaluation...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.84\n",
      "  Accuracy: 0.94\n",
      "  Average Validation loss: 0.20 \n",
      "\n",
      "Dev loss 0.20314300408016048 accuracy 0.9403553299492386 eval_measures {'map': 0.8409475465313029, 'AP@5': 0.8409475465313029, 'P@1': 0.766497461928934, 'RR': 0.841793570219966, 'Rprec': 0.7639593908629442, 'R@5': 0.9289340101522843, 'R@10': 0.9289340101522843, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8460236886632825}\n",
      "Epoch 4/4\n",
      "----------\n",
      "  Batch    40  of    101.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:03.\n",
      "\n",
      "  correct_predictions: 3009.00\n",
      "  Accuracy : 0.94\n",
      "  Average training loss: 0.14\n",
      "relevance results F1 score  0.9366265843353916  precision 0.9769491525423729 recall 0.8995006242197253\n",
      " Macro F1 0.9390428028251561 Weighted F1 0.939042802825156 Accuracy 0.9391385767790262\n",
      "Train loss 0.13939766239116688 accuracy 0.9391385767790262\n",
      "Running Evaluation...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.85\n",
      "  Accuracy: 0.95\n",
      "  Average Validation loss: 0.19 \n",
      "\n",
      "Dev loss 0.19254779053341237 accuracy 0.9467005076142132 eval_measures {'map': 0.8468697123519457, 'AP@5': 0.8468697123519457, 'P@1': 0.7766497461928934, 'RR': 0.8477157360406091, 'Rprec': 0.7741116751269036, 'R@5': 0.9289340101522843, 'R@10': 0.9289340101522843, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8536379018612521}\n",
      " -------------------Training complete for learning rate 2e-05 and number of epochs 4 ------ \n",
      "Train loss 0.13939766239116688 accuracy 0.9391385767790262 end_of_curriculum 0\n",
      "Dev loss 0.19254779053341237 dev 0.9467005076142132 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      " ------------------- Overall Training complete! ---------------------------------------\n",
      "Best learning rate 2e-05 and number of epochs 4 end_of_curriculum 0 ------ \n",
      "Best Dev loss 0.19254779053341237 and dev accuracy  0.9467005076142132 best map {'map': 0.8468697123519457, 'AP@5': 0.8468697123519457, 'P@1': 0.7766497461928934, 'RR': 0.8477157360406091, 'Rprec': 0.7741116751269036, 'R@5': 0.9289340101522843, 'R@10': 0.9289340101522843, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8536379018612521}\n",
      "\n",
      " ------------------------------------------------------------ \n",
      "Running Testing ...\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_stsb-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_10.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_stsb-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_10.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8426395939086294, 'AP@5': 0.8426395939086294, 'P@1': 0.7766497461928934, 'RR': 0.8434856175972928, 'Rprec': 0.7741116751269036, 'R@5': 0.9137055837563451, 'R@10': 0.9137055837563451, 'R@20': 0.9137055837563451, 'R@50': 0.9137055837563451, 'RR@5': 0.8494077834179357}\n",
      "Running Testing ...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:54.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_stsb-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_20.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_stsb-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_20.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8515228426395939, 'AP@5': 0.8515228426395939, 'P@1': 0.7817258883248731, 'RR': 0.8523688663282573, 'Rprec': 0.7791878172588832, 'R@5': 0.9289340101522843, 'R@10': 0.9289340101522843, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8578680203045687}\n",
      "Running Testing ...\n",
      "  Batch   100  of    185.    Elapsed: 0:00:54.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_stsb-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_30.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_stsb-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_30.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8521150592216582, 'AP@5': 0.8521150592216582, 'P@1': 0.7817258883248731, 'RR': 0.8533840947546532, 'Rprec': 0.7791878172588832, 'R@5': 0.934010152284264, 'R@10': 0.934010152284264, 'R@20': 0.934010152284264, 'R@50': 0.934010152284264, 'RR@5': 0.8587140439932319}\n",
      "Running Testing ...\n",
      "  Batch   100  of    308.    Elapsed: 0:00:54.\n",
      "  Batch   200  of    308.    Elapsed: 0:01:47.\n",
      "  Batch   300  of    308.    Elapsed: 0:02:41.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_stsb-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_50.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_stsb-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_50.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8484771573604061, 'AP@5': 0.8484771573604061, 'P@1': 0.7715736040609137, 'RR': 0.8497461928934009, 'Rprec': 0.7690355329949239, 'R@5': 0.9390862944162437, 'R@10': 0.9390862944162437, 'R@20': 0.9390862944162437, 'R@50': 0.9390862944162437, 'RR@5': 0.8550761421319798}\n",
      "Running Testing ...\n",
      "  Batch   100  of    616.    Elapsed: 0:00:54.\n",
      "  Batch   200  of    616.    Elapsed: 0:01:48.\n",
      "  Batch   300  of    616.    Elapsed: 0:02:42.\n",
      "  Batch   400  of    616.    Elapsed: 0:03:35.\n",
      "  Batch   500  of    616.    Elapsed: 0:04:29.\n",
      "  Batch   600  of    616.    Elapsed: 0:05:22.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_stsb-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_100.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_stsb-mpnet-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_100.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8310551124002902, 'AP@5': 0.8296954314720811, 'P@1': 0.7309644670050761, 'RR': 0.832324147933285, 'Rprec': 0.7284263959390863, 'R@5': 0.9441624365482234, 'R@10': 0.9543147208121827, 'R@20': 0.9543147208121827, 'R@50': 0.9543147208121827, 'RR@5': 0.8367174280879867}\n",
      "---------- training for model : -------- msmarco-roberta-base-v2\n",
      "device:  cuda:0\n",
      "train size  3204\n",
      "dev size  3940\n",
      "Epoch 1/4\n",
      "----------\n",
      "  Batch    40  of    101.    Elapsed: 0:00:31.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:02.\n",
      "\n",
      "  correct_predictions: 2543.00\n",
      "  Accuracy : 0.79\n",
      "  Average training loss: 0.44\n",
      "relevance results F1 score  0.787800963081862  precision 0.8109715796430932 recall 0.7659176029962547\n",
      " Macro F1 0.7935360721877576 Weighted F1 0.7935360721877577 Accuracy 0.7936953807740325\n",
      "Train loss 0.4356029238551855 accuracy 0.7936953807740325\n",
      "Running Evaluation...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:26.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.82\n",
      "  Accuracy: 0.94\n",
      "  Average Validation loss: 0.13 \n",
      "\n",
      "Dev loss 0.13043800289667543 accuracy 0.9449238578680204 eval_measures {'map': 0.8213197969543148, 'AP@5': 0.8182741116751269, 'P@1': 0.7563451776649747, 'RR': 0.8213197969543148, 'Rprec': 0.7563451776649747, 'R@5': 0.9035532994923858, 'R@10': 0.9238578680203046, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8182741116751269}\n",
      "Epoch 2/4\n",
      "----------\n",
      "  Batch    40  of    101.    Elapsed: 0:00:31.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:02.\n",
      "\n",
      "  correct_predictions: 2879.00\n",
      "  Accuracy : 0.90\n",
      "  Average training loss: 0.26\n",
      "relevance results F1 score  0.8938255472067953  precision 0.9376285126799178 recall 0.8539325842696629\n",
      " Macro F1 0.8983618324620771 Weighted F1 0.8983618324620771 Accuracy 0.8985642946317104\n",
      "Train loss 0.26454509186125036 accuracy 0.8985642946317104\n",
      "Running Evaluation...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:26.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.87\n",
      "  Accuracy: 0.94\n",
      "  Average Validation loss: 0.19 \n",
      "\n",
      "Dev loss 0.18539876427366248 accuracy 0.9439086294416245 eval_measures {'map': 0.874252679075014, 'AP@5': 0.873688663282572, 'P@1': 0.8375634517766497, 'RR': 0.8750987027636774, 'Rprec': 0.8350253807106599, 'R@5': 0.9238578680203046, 'R@10': 0.9289340101522843, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8745346869712352}\n",
      "Epoch 3/4\n",
      "----------\n",
      "  Batch    40  of    101.    Elapsed: 0:00:31.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:02.\n",
      "\n",
      "  correct_predictions: 2979.00\n",
      "  Accuracy : 0.93\n",
      "  Average training loss: 0.18\n",
      "relevance results F1 score  0.9268768280792979  precision 0.9667796610169491 recall 0.8901373283395755\n",
      " Macro F1 0.9296647724905647 Weighted F1 0.9296647724905647 Accuracy 0.9297752808988764\n",
      "Train loss 0.18025832623243332 accuracy 0.9297752808988764\n",
      "Running Evaluation...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:26.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.88\n",
      "  Accuracy: 0.94\n",
      "  Average Validation loss: 0.24 \n",
      "\n",
      "Dev loss 0.24053890334635658 accuracy 0.9390862944162437 eval_measures {'map': 0.8804286520022562, 'AP@5': 0.8798646362098138, 'P@1': 0.8477157360406091, 'RR': 0.8812746756909192, 'Rprec': 0.8451776649746193, 'R@5': 0.9238578680203046, 'R@10': 0.9289340101522843, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8807106598984772}\n",
      "Epoch 4/4\n",
      "----------\n",
      "  Batch    40  of    101.    Elapsed: 0:00:31.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:02.\n",
      "\n",
      "  correct_predictions: 3022.00\n",
      "  Accuracy : 0.94\n",
      "  Average training loss: 0.12\n",
      "relevance results F1 score  0.9407937540663631  precision 0.9823369565217391 recall 0.9026217228464419\n",
      " Macro F1 0.9431023359413999 Weighted F1 0.9431023359413998 Accuracy 0.9431960049937578\n",
      "Train loss 0.11549427261631383 accuracy 0.9431960049937578\n",
      "Running Evaluation...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:26.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.89\n",
      "  Accuracy: 0.94\n",
      "  Average Validation loss: 0.26 \n",
      "\n",
      "Dev loss 0.25700888645669984 accuracy 0.9393401015228426 eval_measures {'map': 0.8868443316412858, 'AP@5': 0.8853637901861252, 'P@1': 0.8629441624365483, 'RR': 0.8876903553299492, 'Rprec': 0.8604060913705583, 'R@5': 0.9187817258883249, 'R@10': 0.9289340101522843, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8862098138747886}\n",
      " -------------------Training complete for learning rate 2e-05 and number of epochs 4 ------ \n",
      "Train loss 0.11549427261631383 accuracy 0.9431960049937578 end_of_curriculum 0\n",
      "Dev loss 0.25700888645669984 dev 0.9393401015228426 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      " ------------------- Overall Training complete! ---------------------------------------\n",
      "Best learning rate 2e-05 and number of epochs 4 end_of_curriculum 0 ------ \n",
      "Best Dev loss 0.25700888645669984 and dev accuracy  0.9393401015228426 best map {'map': 0.8868443316412858, 'AP@5': 0.8853637901861252, 'P@1': 0.8629441624365483, 'RR': 0.8876903553299492, 'Rprec': 0.8604060913705583, 'R@5': 0.9187817258883249, 'R@10': 0.9289340101522843, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8862098138747886}\n",
      "\n",
      " ------------------------------------------------------------ \n",
      "Running Testing ...\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_msmarco-roberta-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_10.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_msmarco-roberta-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_10.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.883248730964467, 'AP@5': 0.883248730964467, 'P@1': 0.8629441624365483, 'RR': 0.8840947546531303, 'Rprec': 0.8604060913705583, 'R@5': 0.9137055837563451, 'R@10': 0.9137055837563451, 'R@20': 0.9137055837563451, 'R@50': 0.9137055837563451, 'RR@5': 0.8840947546531304}\n",
      "Running Testing ...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:51.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_msmarco-roberta-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_20.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_msmarco-roberta-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_20.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8908629441624365, 'AP@5': 0.8900169204737731, 'P@1': 0.868020304568528, 'RR': 0.8917089678510998, 'Rprec': 0.8654822335025381, 'R@5': 0.9238578680203046, 'R@10': 0.9289340101522843, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8908629441624367}\n",
      "Running Testing ...\n",
      "  Batch   100  of    185.    Elapsed: 0:00:51.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_msmarco-roberta-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_30.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_msmarco-roberta-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_30.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8866811699299009, 'AP@5': 0.8859560067681895, 'P@1': 0.8578680203045685, 'RR': 0.8875271936185642, 'Rprec': 0.8553299492385786, 'R@5': 0.9289340101522843, 'R@10': 0.934010152284264, 'R@20': 0.934010152284264, 'R@50': 0.934010152284264, 'RR@5': 0.8868020304568527}\n",
      "Running Testing ...\n",
      "  Batch   100  of    308.    Elapsed: 0:00:51.\n",
      "  Batch   200  of    308.    Elapsed: 0:01:42.\n",
      "  Batch   300  of    308.    Elapsed: 0:02:33.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_msmarco-roberta-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_50.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_msmarco-roberta-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_50.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8885325334563913, 'AP@5': 0.8880710659898476, 'P@1': 0.8578680203045685, 'RR': 0.8893785571450545, 'Rprec': 0.8553299492385786, 'R@5': 0.934010152284264, 'R@10': 0.934010152284264, 'R@20': 0.9390862944162437, 'R@50': 0.9390862944162437, 'RR@5': 0.888917089678511}\n",
      "Running Testing ...\n",
      "  Batch   100  of    616.    Elapsed: 0:00:51.\n",
      "  Batch   200  of    616.    Elapsed: 0:01:42.\n",
      "  Batch   300  of    616.    Elapsed: 0:02:33.\n",
      "  Batch   400  of    616.    Elapsed: 0:03:23.\n",
      "  Batch   500  of    616.    Elapsed: 0:04:14.\n",
      "  Batch   600  of    616.    Elapsed: 0:05:05.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_msmarco-roberta-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_100.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_msmarco-roberta-base-v2-seed-61168821_rerankded_data-dev-set_of_depth_100.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8782155614642924, 'AP@5': 0.8768189509306261, 'P@1': 0.8324873096446701, 'RR': 0.8790615851529557, 'Rprec': 0.8299492385786802, 'R@5': 0.9390862944162437, 'R@10': 0.9441624365482234, 'R@20': 0.949238578680203, 'R@50': 0.9543147208121827, 'RR@5': 0.8776649746192893}\n",
      "---------- training for model : -------- paraphrase-MiniLM-L12-v2\n",
      "device:  cuda:0\n",
      "train size  3204\n",
      "dev size  3940\n",
      "Epoch 1/4\n",
      "----------\n",
      "  Batch    40  of    101.    Elapsed: 0:00:13.\n",
      "  Batch    80  of    101.    Elapsed: 0:00:25.\n",
      "\n",
      "  correct_predictions: 2476.00\n",
      "  Accuracy : 0.77\n",
      "  Average training loss: 0.45\n",
      "relevance results F1 score  0.7538877620013522  precision 0.8222713864306784 recall 0.6960049937578028\n",
      " Macro F1 0.7714366346238645 Weighted F1 0.7714366346238645 Accuracy 0.7727840199750312\n",
      "Train loss 0.4545348469942513 accuracy 0.7727840199750312\n",
      "Running Evaluation...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:11.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.84\n",
      "  Accuracy: 0.95\n",
      "  Average Validation loss: 0.17 \n",
      "\n",
      "Dev loss 0.17475196298572324 accuracy 0.9456852791878173 eval_measures {'map': 0.8369391888427421, 'AP@5': 0.8336717428087987, 'P@1': 0.7918781725888325, 'RR': 0.838208224375737, 'Rprec': 0.7893401015228426, 'R@5': 0.8984771573604061, 'R@10': 0.9137055837563451, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8400169204737732}\n",
      "Epoch 2/4\n",
      "----------\n",
      "  Batch    40  of    101.    Elapsed: 0:00:13.\n",
      "  Batch    80  of    101.    Elapsed: 0:00:25.\n",
      "\n",
      "  correct_predictions: 2932.00\n",
      "  Accuracy : 0.92\n",
      "  Average training loss: 0.22\n",
      "relevance results F1 score  0.9115159401431359  precision 0.951766304347826 recall 0.8745318352059925\n",
      " Macro F1 0.9149661284398942 Weighted F1 0.9149661284398942 Accuracy 0.9151061173533084\n",
      "Train loss 0.22054735969493885 accuracy 0.9151061173533084\n",
      "Running Evaluation...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:11.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.88\n",
      "  Accuracy: 0.94\n",
      "  Average Validation loss: 0.21 \n",
      "\n",
      "Dev loss 0.2108683250544052 accuracy 0.93502538071066 eval_measures {'map': 0.8808516638465876, 'AP@5': 0.8777495769881556, 'P@1': 0.8527918781725888, 'RR': 0.8821206993795826, 'Rprec': 0.850253807106599, 'R@5': 0.9086294416243654, 'R@10': 0.9289340101522843, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.88663282571912}\n",
      "Epoch 3/4\n",
      "----------\n",
      "  Batch    40  of    101.    Elapsed: 0:00:13.\n",
      "  Batch    80  of    101.    Elapsed: 0:00:25.\n",
      "\n",
      "  correct_predictions: 2990.00\n",
      "  Accuracy : 0.93\n",
      "  Average training loss: 0.15\n",
      "relevance results F1 score  0.9301110385369039  precision 0.9753424657534246 recall 0.8888888888888888\n",
      " Macro F1 0.9330770374991753 Weighted F1 0.933077037499175 Accuracy 0.9332084893882646\n",
      "Train loss 0.15310868051474785 accuracy 0.9332084893882646\n",
      "Running Evaluation...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:11.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.86\n",
      "  Accuracy: 0.94\n",
      "  Average Validation loss: 0.21 \n",
      "\n",
      "Dev loss 0.21245661415084596 accuracy 0.9423857868020306 eval_measures {'map': 0.8569615663524293, 'AP@5': 0.8557529610829102, 'P@1': 0.8071065989847716, 'RR': 0.8582306018854241, 'Rprec': 0.8045685279187818, 'R@5': 0.9187817258883249, 'R@10': 0.9238578680203046, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8646362098138747}\n",
      "Epoch 4/4\n",
      "----------\n",
      "  Batch    40  of    101.    Elapsed: 0:00:13.\n",
      "  Batch    80  of    101.    Elapsed: 0:00:25.\n",
      "\n",
      "  correct_predictions: 2993.00\n",
      "  Accuracy : 0.93\n",
      "  Average training loss: 0.13\n",
      "relevance results F1 score  0.9310232101994116  precision 0.9773507206588882 recall 0.8888888888888888\n",
      " Macro F1 0.9340096642218318 Weighted F1 0.9340096642218318 Accuracy 0.9341448189762797\n",
      "Train loss 0.1304338275187529 accuracy 0.9341448189762797\n",
      "Running Evaluation...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:11.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.87\n",
      "  Accuracy: 0.95\n",
      "  Average Validation loss: 0.19 \n",
      "\n",
      "Dev loss 0.18818984633582014 accuracy 0.9482233502538071 eval_measures {'map': 0.8688058979937152, 'AP@5': 0.8675972927241963, 'P@1': 0.8324873096446701, 'RR': 0.8700749335267102, 'Rprec': 0.8299492385786802, 'R@5': 0.9187817258883249, 'R@10': 0.9238578680203046, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8747884940778341}\n",
      " -------------------Training complete for learning rate 2e-05 and number of epochs 4 ------ \n",
      "Train loss 0.1304338275187529 accuracy 0.9341448189762797 end_of_curriculum 0\n",
      "Dev loss 0.18818984633582014 dev 0.9482233502538071 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      " ------------------- Overall Training complete! ---------------------------------------\n",
      "Best learning rate 2e-05 and number of epochs 4 end_of_curriculum 0 ------ \n",
      "Best Dev loss 0.18818984633582014 and dev accuracy  0.9482233502538071 best map {'map': 0.8688058979937152, 'AP@5': 0.8675972927241963, 'P@1': 0.8324873096446701, 'RR': 0.8700749335267102, 'Rprec': 0.8299492385786802, 'R@5': 0.9187817258883249, 'R@10': 0.9238578680203046, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8747884940778341}\n",
      "\n",
      " ------------------------------------------------------------ \n",
      "Running Testing ...\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_paraphrase-MiniLM-L12-v2-seed-61168821_rerankded_data-dev-set_of_depth_10.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_paraphrase-MiniLM-L12-v2-seed-61168821_rerankded_data-dev-set_of_depth_10.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8604060913705583, 'AP@5': 0.8604060913705583, 'P@1': 0.8223350253807107, 'RR': 0.8612521150592216, 'Rprec': 0.8197969543147208, 'R@5': 0.9137055837563451, 'R@10': 0.9137055837563451, 'R@20': 0.9137055837563451, 'R@50': 0.9137055837563451, 'RR@5': 0.8671742808798647}\n",
      "Running Testing ...\n",
      "  Batch   100  of    124.    Elapsed: 0:00:21.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_paraphrase-MiniLM-L12-v2-seed-61168821_rerankded_data-dev-set_of_depth_20.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_paraphrase-MiniLM-L12-v2-seed-61168821_rerankded_data-dev-set_of_depth_20.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8643542019176537, 'AP@5': 0.8637901861252116, 'P@1': 0.817258883248731, 'RR': 0.8656232374506486, 'Rprec': 0.8147208121827412, 'R@5': 0.9238578680203046, 'R@10': 0.9289340101522843, 'R@20': 0.9289340101522843, 'R@50': 0.9289340101522843, 'RR@5': 0.8709813874788495}\n",
      "Running Testing ...\n",
      "  Batch   100  of    185.    Elapsed: 0:00:21.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_paraphrase-MiniLM-L12-v2-seed-61168821_rerankded_data-dev-set_of_depth_30.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_paraphrase-MiniLM-L12-v2-seed-61168821_rerankded_data-dev-set_of_depth_30.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8544899685762631, 'AP@5': 0.8518612521150593, 'P@1': 0.8020304568527918, 'RR': 0.8560128112158569, 'Rprec': 0.799492385786802, 'R@5': 0.9137055837563451, 'R@10': 0.9289340101522843, 'R@20': 0.934010152284264, 'R@50': 0.934010152284264, 'RR@5': 0.8593062605752961}\n",
      "Running Testing ...\n",
      "  Batch   100  of    308.    Elapsed: 0:00:21.\n",
      "  Batch   200  of    308.    Elapsed: 0:00:42.\n",
      "  Batch   300  of    308.    Elapsed: 0:01:02.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_paraphrase-MiniLM-L12-v2-seed-61168821_rerankded_data-dev-set_of_depth_50.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_paraphrase-MiniLM-L12-v2-seed-61168821_rerankded_data-dev-set_of_depth_50.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8535130126500685, 'AP@5': 0.851015228426396, 'P@1': 0.7969543147208121, 'RR': 0.8550358552896625, 'Rprec': 0.7944162436548223, 'R@5': 0.9187817258883249, 'R@10': 0.934010152284264, 'R@20': 0.9390862944162437, 'R@50': 0.9390862944162437, 'RR@5': 0.8584602368866329}\n",
      "Running Testing ...\n",
      "  Batch   100  of    616.    Elapsed: 0:00:21.\n",
      "  Batch   200  of    616.    Elapsed: 0:00:42.\n",
      "  Batch   300  of    616.    Elapsed: 0:01:02.\n",
      "  Batch   400  of    616.    Elapsed: 0:01:23.\n",
      "  Batch   500  of    616.    Elapsed: 0:01:44.\n",
      "  Batch   600  of    616.    Elapsed: 0:02:05.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2020-mono_paraphrase-MiniLM-L12-v2-seed-61168821_rerankded_data-dev-set_of_depth_100.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2020/en-clef2020-mono_paraphrase-MiniLM-L12-v2-seed-61168821_rerankded_data-dev-set_of_depth_100.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8494907588181699, 'AP@5': 0.8449238578680203, 'P@1': 0.7918781725888325, 'RR': 0.8513943121176624, 'Rprec': 0.7893401015228426, 'R@5': 0.916243654822335, 'R@10': 0.9390862944162437, 'R@20': 0.9543147208121827, 'R@50': 0.9543147208121827, 'RR@5': 0.8533840947546532}\n"
     ]
    }
   ],
   "source": [
    "depth_evaluation_file = \"./data/evaluation/en-clef-2020-mono-bert-depth-evaluation-on-dev-set.xlsx\"\n",
    "bert_models = [\n",
    "            \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "            \"sentence-transformers/stsb-mpnet-base-v2\",\n",
    "            \"sentence-transformers/msmarco-roberta-base-v2\",\n",
    "            \"sentence-transformers/paraphrase-MiniLM-L12-v2\",\n",
    "            ]\n",
    "models_names = [\n",
    "            \"paraphrase-mpnet-base-v2\",\n",
    "            \"stsb-mpnet-base-v2\",\n",
    "            \"msmarco-roberta-base-v2\",\n",
    "            \"paraphrase-MiniLM-L12-v2\",\n",
    "            ]\n",
    "\n",
    "\n",
    "\n",
    "num_rand_seeds  = 5\n",
    "seeds = random_seeds[:num_rand_seeds]\n",
    "for i in range(len(bert_models)):\n",
    "    l = 0\n",
    "    seed = seeds[l]\n",
    "    bert_model = bert_models[i]\n",
    "    model_name = models_names[i]\n",
    "    print(\"---------- training for model : --------\", model_name)\n",
    "    hp = { # hyper parameters\n",
    "    \"model_name\": bert_model,\n",
    "    \"name\": model_name +\"-seed-\"+ str(seed),\n",
    "    \"model_save_path\": \"./data/saved_models/EN-clef2020-\"+model_name+\"_mono_trained_model-seed-\"+ str(seed)+\".bin\",\n",
    "    \"model_training_log\": \"./data/bert_evaluation/en-clef2020-mono_\"+model_name+\"_training_log-seed-\"+ str(seed)+\".xlsx\",\n",
    "    \"batch_size\": 32,\n",
    "    \"num_of_epochs\": [4], \n",
    "    \"learning_rate\" : [2e-5,],\n",
    "    \"dropout\": [0.3,], \n",
    "    \"seeds\": [seed],\n",
    "    \"max_len\": 256,\n",
    "    \"curricula_type\": 0,\n",
    "    \"end_of_curriculum\": [0],\n",
    "    \"num_of_layers\": ONE_LAYER,\n",
    "    \"is_output_probability\": False, # if false, put loss_function = mono_loss, otherwise put loss_function=CrossEntropy\n",
    "    \"loss_function\": \"mono_loss\"}\n",
    "   \n",
    "\n",
    "    _, _, best_learning_rate, best_num_of_epochs, best_end_of_curriculum, best_dropout = mono_bert_trainer.train_mono_bert(\n",
    "                        mono_bert_train_set, mono_bert_dev_set_depth_20, hp[\"model_name\"], apply_cleaning=False,  seeds=hp[\"seeds\"],\n",
    "                        trained_model_save_path=hp[\"model_save_path\"], shuffle=True, freeze_bert=False, max_len=hp[\"max_len\"], \n",
    "                        batch_size=hp[\"batch_size\"],epochs=hp[\"num_of_epochs\"], learning_rates = hp[\"learning_rate\"],\n",
    "                        is_output_probability=hp[\"is_output_probability\"],  end_of_curriculums=hp[\"end_of_curriculum\"],\n",
    "                        curricula_type=hp[\"curricula_type\"], dropout=hp[\"dropout\"], hp=hp, results_path=hp[\"model_training_log\"],\n",
    "                        classifier_layers=hp[\"num_of_layers\"], what_to_eval=cf.VCLAIM_AND_TITLE)\n",
    "\n",
    "\n",
    "    # store the best hyperparameters\n",
    "    hp[\"num_of_epochs\"]= best_num_of_epochs\n",
    "    hp[\"learning_rate\"]= best_learning_rate\n",
    "    hp[\"end_of_curriculum\"]= best_end_of_curriculum\n",
    "    hp[\"dropout\"]= best_dropout\n",
    "\n",
    "\n",
    "   \n",
    "    depths = [10, 20, 30, 50, 100]\n",
    "    for i in range(len(depths)):\n",
    "        depth = depths[i]\n",
    "        mono_bert_dev_set = \"./data/CLEF_2020/dev_sets/en-clef2020-mono_bert_dev_set_top_\"+ str(depth) + \".tsv\"\n",
    "        hp[\"test_depth\"] = depth\n",
    "        run_name = \"en-clef2020-mono_\"+model_name+\"-seed-\"+str(seed)+\"_rerankded_data-dev-set_of_depth_\" +str(depth)+\".tsv\"\n",
    "        hp[\"reranked_data_path\"] = \"./data/runs/\" + run_name\n",
    "        hp[\"trec_run_path\"] = \"./data/runs/trec_eval/EN2020/\"+ run_name\n",
    "\n",
    "        mono_bert_tester.test_mono_bert(hp[\"model_name\"], hp[\"model_save_path\"], mono_bert_dev_set, \n",
    "                                hp[\"reranked_data_path\"],  depth_evaluation_file, max_len=hp[\"max_len\"], batch_size=hp[\"batch_size\"],\n",
    "                                dropout=hp[\"dropout\"], apply_cleaning=False, is_output_probability=hp[\"is_output_probability\"], \n",
    "                                hyper_parameters=hp,  classifier_layers=hp[\"num_of_layers\"], trec_run_path=hp[\"trec_run_path\"],\n",
    "                                what_to_test=cf.VCLAIM_AND_TITLE)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing multiple bert  models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- training for model : -------- paraphrase-mpnet-base-v2\n",
      "device:  cuda:0\n",
      "train size  3204\n",
      "dev size  3940\n",
      "Epoch 1/3\n",
      "----------\n",
      "  Batch    40  of    101.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    101.    Elapsed: 0:01:04.\n",
      "\n",
      "  correct_predictions: 2570.00\n",
      "  Accuracy : 0.80\n",
      "  Average training loss: 0.41\n",
      "relevance results F1 score  0.788101604278075  precision 0.8482014388489209 recall 0.7359550561797753\n",
      " Macro F1 0.8012522072912622 Weighted F1 0.8012522072912623 Accuracy 0.8021223470661673\n",
      "Train loss 0.41303354702742384 accuracy 0.8021223470661673\n",
      "Running Evaluation...\n"
     ]
    }
   ],
   "source": [
    "bert_models = [\n",
    "            \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "            \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "            \"sentence-transformers/stsb-mpnet-base-v2\",\n",
    "            \"sentence-transformers/msmarco-roberta-base-v2\",\n",
    "            \"sentence-transformers/paraphrase-MiniLM-L12-v2\",\n",
    "            \"bert-base-cased\",\n",
    "            ]\n",
    "models_names = [\n",
    "            \"paraphrase-mpnet-base-v2\",\n",
    "            \"paraphrase-multilingual-mpnet-base-v2\",\n",
    "            \"stsb-mpnet-base-v2\",\n",
    "            \"msmarco-roberta-base-v2\",\n",
    "            \"paraphrase-MiniLM-L12-v2\",\n",
    "            \"BERT\",\n",
    "            ]\n",
    "            \n",
    "num_rand_seeds  = 5\n",
    "seeds = random_seeds[:num_rand_seeds]\n",
    "for i in range(len(bert_models)):\n",
    "    l = 0\n",
    "    seed = seeds[l]\n",
    "    bert_model = bert_models[i]\n",
    "    model_name = models_names[i]\n",
    "    print(\"---------- training for model : --------\", model_name)\n",
    "    hp = { # hyper parameters\n",
    "\n",
    "    \"model_name\": bert_model,\n",
    "    \"name\": model_name +\"-seed-\"+ str(seed),\n",
    "    \"model_save_path\": \"./data/saved_models/EN-clef2020-\"+model_name+\"_mono_trained_model-tuning-seed-\"+ str(seed)+\".bin\",\n",
    "    \"model_training_log\": \"./data/bert_evaluation/en-clef2020-mono_\"+model_name+\"_training_log-tuning-seed-\"+ str(seed)+\".xlsx\",\n",
    "    \"batch_size\": 32,\n",
    "    \"test_depth\": 30,\n",
    "    \"num_of_epochs\": [3, 4, 5], \n",
    "    \"learning_rate\" : [2e-5, 3e-5],\n",
    "    \"dropout\": [0.3, 0.4,], \n",
    "    \"seeds\": [seed],\n",
    "    \"max_len\": 256,\n",
    "    \"curricula_type\": 0,\n",
    "    \"end_of_curriculum\": [0],\n",
    "    \"num_of_layers\": ONE_LAYER,\n",
    "    \"is_output_probability\": False, # if false, put loss_function = mono_loss, otherwise put loss_function=CrossEntropy\n",
    "    \"loss_function\": \"mono_loss\"}\n",
    "   \n",
    "\n",
    "\n",
    "    _, _, best_learning_rate, best_num_of_epochs, best_end_of_curriculum, best_dropout = mono_bert_trainer.train_mono_bert(\n",
    "                        mono_bert_train_set, mono_bert_dev_set_depth_20, hp[\"model_name\"], apply_cleaning=False,  seeds=hp[\"seeds\"],\n",
    "                        trained_model_save_path=hp[\"model_save_path\"], shuffle=True, freeze_bert=False, max_len=hp[\"max_len\"], \n",
    "                        batch_size=hp[\"batch_size\"],epochs=hp[\"num_of_epochs\"], learning_rates = hp[\"learning_rate\"],\n",
    "                        is_output_probability=hp[\"is_output_probability\"],  end_of_curriculums=hp[\"end_of_curriculum\"],\n",
    "                        curricula_type=hp[\"curricula_type\"], dropout=hp[\"dropout\"], hp=hp, results_path=hp[\"model_training_log\"],\n",
    "                        classifier_layers=hp[\"num_of_layers\"], what_to_eval=cf.VCLAIM_AND_TITLE)\n",
    "\n",
    "\n",
    "    # store the best hyperparameters\n",
    "    hp[\"num_of_epochs\"]= best_num_of_epochs\n",
    "    hp[\"learning_rate\"]= best_learning_rate\n",
    "    hp[\"end_of_curriculum\"]= best_end_of_curriculum\n",
    "    hp[\"dropout\"]= best_dropout\n",
    "\n",
    "\n",
    "    # measure the performance on the test set\n",
    "    depths = [20,]\n",
    "    for k in range(len(depths)):\n",
    "        depth = depths[k]\n",
    "        hp[\"test_depth\"] = depth\n",
    "        mono_bert_test_set_path = \"./data/CLEF_2020/test_sets/mono_bert_test_set_top_\" + str(depth) + \".tsv\"\n",
    "        run_name = \"en-clef2020-mono_\"+model_name+\"-seed-\"+str(seed)+\"_rerankded_data-depth_\" +str(depth)+\".tsv\"\n",
    "        hp[\"reranked_data_path\"] = \"./data/runs/\" + run_name\n",
    "        hp[\"trec_run_path\"] = \"./data/runs/trec_eval/EN2020/\"+ run_name\n",
    "\n",
    "        mono_bert_tester.test_mono_bert(hp[\"model_name\"], hp[\"model_save_path\"], mono_bert_test_set_path, \n",
    "                        hp[\"reranked_data_path\"],  evaluation_save_path, max_len=hp[\"max_len\"], batch_size=hp[\"batch_size\"],\n",
    "                        dropout=hp[\"dropout\"], apply_cleaning=False, is_output_probability=hp[\"is_output_probability\"], \n",
    "                        hyper_parameters=hp, classifier_layers=hp[\"num_of_layers\"], trec_run_path=hp[\"trec_run_path\"],\n",
    "                        what_to_test=cf.VCLAIM_AND_TITLE)\n",
    "\n",
    "\n",
    "    # then we get the best hyper parameters values\n",
    "    # and we can run the training with different random seeds\n",
    "\n",
    "    l = 1\n",
    "    while l < num_rand_seeds:\n",
    "        seed = seeds[l]\n",
    "        l = l + 1\n",
    "        hp[\"name\"] = model_name +\"-seed-\"+ str(seed)\n",
    "        hp[\"model_save_path\"]= \"./data/saved_models/EN-clef2020-\"+model_name+\"_mono_trained_model-tuning-seed-\"+ str(seed)+\".bin\"\n",
    "        hp[\"model_training_log\"]= \"./data/bert_evaluation/en-clef2020-mono_\"+model_name+\"_training_log-tuning-seed-\"+ str(seed)+\".xlsx\"\n",
    "        hp[\"learning_rate\"]= [best_learning_rate]\n",
    "        hp[\"num_of_epochs\"]= [best_num_of_epochs]\n",
    "        hp[\"end_of_curriculum\"]= [best_end_of_curriculum]\n",
    "        hp[\"dropout\"]= [best_dropout]\n",
    "        hp[\"seeds\"] = [seed]\n",
    "\n",
    "\n",
    "        _, _, best_learning_rate, best_num_of_epochs, best_end_of_curriculum, best_dropout = mono_bert_trainer.train_mono_bert(\n",
    "                        mono_bert_train_set, mono_bert_dev_set_depth_20, hp[\"model_name\"], apply_cleaning=False,  seeds=hp[\"seeds\"],\n",
    "                        trained_model_save_path=hp[\"model_save_path\"], shuffle=True, freeze_bert=False, max_len=hp[\"max_len\"], \n",
    "                        batch_size=hp[\"batch_size\"],epochs=hp[\"num_of_epochs\"], learning_rates = hp[\"learning_rate\"],\n",
    "                        is_output_probability=hp[\"is_output_probability\"],  end_of_curriculums=hp[\"end_of_curriculum\"],\n",
    "                        curricula_type=hp[\"curricula_type\"], dropout=hp[\"dropout\"], hp=hp, results_path=hp[\"model_training_log\"],\n",
    "                        classifier_layers=hp[\"num_of_layers\"], what_to_eval=cf.VCLAIM_AND_TITLE)\n",
    "\n",
    "        # store the best hyperparameters\n",
    "        hp[\"num_of_epochs\"]= best_num_of_epochs\n",
    "        hp[\"learning_rate\"]= best_learning_rate\n",
    "        hp[\"end_of_curriculum\"]= best_end_of_curriculum\n",
    "        hp[\"dropout\"]= best_dropout\n",
    "\n",
    "\n",
    "         # measure the performance on the test set\n",
    "        depths = [20,]\n",
    "        for k in range(len(depths)):\n",
    "            depth = depths[k]\n",
    "            hp[\"test_depth\"] = depth\n",
    "            mono_bert_test_set_path = \"./data/CLEF_2020/test_sets/mono_bert_test_set_top_\" + str(depth) + \".tsv\"\n",
    "            run_name = \"en-clef2020-mono_\"+model_name+\"-seed-\"+str(seed)+\"_rerankded_data-depth_\" +str(depth)+\".tsv\"\n",
    "            hp[\"reranked_data_path\"] = \"./data/runs/\" + run_name\n",
    "            hp[\"trec_run_path\"] = \"./data/runs/trec_eval/EN2020/\"+ run_name\n",
    "\n",
    "            mono_bert_tester.test_mono_bert(hp[\"model_name\"], hp[\"model_save_path\"], mono_bert_test_set_path, \n",
    "                            hp[\"reranked_data_path\"],  evaluation_save_path, max_len=hp[\"max_len\"], batch_size=hp[\"batch_size\"],\n",
    "                            dropout=hp[\"dropout\"], apply_cleaning=False, is_output_probability=hp[\"is_output_probability\"], \n",
    "                            hyper_parameters=hp, classifier_layers=hp[\"num_of_layers\"], trec_run_path=hp[\"trec_run_path\"],\n",
    "                            what_to_test=cf.VCLAIM_AND_TITLE)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('tweetEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32ea63ce396b580d232aa3acb554bdf20f8441e7d8ed4d44b8e0f0e042cd90e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
