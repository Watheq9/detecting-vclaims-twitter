{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-29 12:12:29.285893: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-29 12:12:29.285939: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# import needed libraries\n",
    "import pandas as pd\n",
    "import helper.Utils as Utils\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from mono_bert_train import MonoBertTrainer\n",
    "from mono_bert_test import MonoBertTester\n",
    "import configure  as cf\n",
    "from train_set_creator import MonoBertTrainSetCreator\n",
    "from pyterrier.measures import RR, R, Rprec, P, MAP\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling PRF in pyterier\n",
      "PyTerrier 0.6.0 has loaded Terrier 5.6 (built by craigmacdonald on 2021-09-17 13:27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not pt.started():\n",
    "    print(\"Enabling PRF in pyterier\")\n",
    "    # In this lab, we need to specify that we start PyTerrier with PRF enabled\n",
    "    pt.init(boot_packages=[\"com.github.terrierteam:terrier-prf:-SNAPSHOT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before indexing our data we need to do the following processing steps:\n",
    "\n",
    "\n",
    "1.   **Remove stopwords.**\n",
    "2.   **Normalization.**\n",
    "3.   **Stemming.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean(text):\n",
    "  '''\n",
    "  Clean input text form urls, handles, tabs, line jumps, and extra white spaces\n",
    "  '''\n",
    "  text = re.sub(r\"http\\S+\", \" \", text)  # remove urls\n",
    "  text = re.sub(r\"RT \", \" \", text)  # remove rt\n",
    "  text = re.sub(r\"@[\\w]*\", \" \", text)  # remove handles\n",
    "  text = re.sub(r\"[\\.\\,\\#_\\|\\:\\?\\?\\/\\=]\", \" \", text)# remove special characters\n",
    "  text = re.sub(r\"\\t\", \" \", text)  # remove tabs\n",
    "  text = re.sub(r\"\\n\", \" \", text)  # remove line jump\n",
    "  text = re.sub(r\"\\s+\", \" \", text)  # remove extra white space\n",
    "  text = text.strip()\n",
    "  return text\n",
    "\n",
    "\n",
    "#removing stop sords function\n",
    "def remove_stop_words(sentence):\n",
    "  terms=[]\n",
    "  stop_words= set(stopwords.words('english'))\n",
    "  words = sentence.split()\n",
    "  for term in words: \n",
    "      if term not in stop_words :\n",
    "          terms.append(term)\n",
    "  return \" \".join(terms)\n",
    "\n",
    "\n",
    "def stem(sentence):\n",
    "    token_words=word_tokenize(sentence)\n",
    "    return \" \".join([porter.stem(word) for word in token_words])\n",
    "\n",
    "\n",
    "def preprocess(sentence):\n",
    "  # apply preprocessing steps on the given sentence\n",
    "  sentence = sentence.lower()\n",
    "  sentence =remove_stop_words(sentence)\n",
    "  sentence =stem(sentence)\n",
    "  return sentence\n",
    "\n",
    "#define the stemming function\n",
    "porter=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define some constants.\n",
    "is_index_built= False\n",
    "DECIMAL_ROUND = 5\n",
    "pos_label = 1.0\n",
    "neg_label = 0.0\n",
    "RANK = cf.RANK\n",
    "SCORE = cf.SCORE\n",
    "TWEET_ID_COLUMN = cf.TWEET_ID_COLUMN\n",
    "TWEET_TEXT_COLUMN = cf.TWEET_TEXT_COLUMN\n",
    "VCLAIM_ID = cf.VCLAIM_ID\n",
    "VCLAIM = cf.VCLAIM\n",
    "TITLE = cf.TITLE \n",
    "LABEL = cf.LABEL\n",
    "qrels_file=cf.ENG_2021_QRELS\n",
    "claims_file = cf.ENG_2021_VCLAIMS\n",
    "evaluation_save_path = cf.ENG_CLEF_2021_EVALUATION_FILE\n",
    "\n",
    "train_and_dev_query_path = cf.ENG_CLEF_2021_URL_CLEANED_TRAIN_AND_DEV_QUERIES\n",
    "train_query_path=cf.ENG_CLEF_2021_URL_CLEANED_TRAIN_QUERIES\n",
    "raw_dev_query = cf.ENG_2021_DEV_QUERIES\n",
    "expanded_dev_query = cf.ENG_CLEF_2021_URL_CLEANED_DEV_QUERIES\n",
    "dev_query_path = expanded_dev_query\n",
    "test_query_path =cf.ENG_CLEF_2021_URL_CLEANED_TEST_QUERIES\n",
    "eval_metrics=[\"map\",MAP@5, P@1, RR, Rprec, R@5, R@10, R@20, R@50, RR@5]\n",
    "\n",
    "QUERY = cf.QUERY\n",
    "QID = cf.QID\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "ONE_LAYER = 1\n",
    "TWO_LAYERS = 2\n",
    "\n",
    "\n",
    "index_save_path = \"./indexes/en-clef-2021-index-multi-field/data.properties\"\n",
    "dev_bm25_run = \"./data/runs/bm25_dev_query_en_clef_2021.tsv\"\n",
    "train_bm25_run = \"./data/runs/bm25_train_query_en_clef_2021.tsv\"\n",
    "test_bm25_run = \"./data/runs/bm25_test_query_en_clef_2021.tsv\"\n",
    "mono_bert_train_vclaim_set = \"./data/CLEF_2021/English/train_sets/2021_mono_bert_train_set-vclaim-only.xlsx\"\n",
    "mono_bert_train_title_set = \"./data/CLEF_2021/English/train_sets/2021_mono_bert_train_set-title-only.xlsx\"\n",
    "mono_bert_train_both_vclaim_and_title_set = \"./data/CLEF_2021/English/train_sets/2021_mono_bert_train_set-both_vclaim_and_title.xlsx\"\n",
    "\n",
    "mono_bert_train_set = mono_bert_train_both_vclaim_and_title_set\n",
    "mono_bert_dev_set_depth_20= \"./data/CLEF_2021/English/dev_sets/en-clef2021-mono_bert_dev_set_top_20.tsv\"\n",
    "mono_bert_dev_set_depth_30= \"./data/CLEF_2021/English/dev_sets/en-clef2021-mono_bert_dev_set_top_30.tsv\"\n",
    "\n",
    "\n",
    "random_seeds = [61168821, 129995678, 22612812, 146764631, 21228945, 94412880, 204110176, 6155814, 187372311, 117623077,]\n",
    "\n",
    "# depth from which negative pairs will be randomly selected from top k documents retrieved from a retrieval model\n",
    "RANDOM_DEPTH = 20 # RANDOM_DEPTH_FOR_SELECTING_NEGATIVE_DOCUMENTS\n",
    "search_depth = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mono_bert_trainer = MonoBertTrainer(qrels_path=qrels_file,)\n",
    "mono_bert_tester = MonoBertTester(qrels_path=qrels_file,\n",
    "                                evaluation_save_path=evaluation_save_path)\n",
    "                                \n",
    "random_seeds = [61168821, 129995678, 22612812, 146764631, 21228945, 94412880, 204110176, 6155814, 187372311, 117623077,]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vclaim</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vclaim-sno-1-mcdonald39s-any-size-fries</th>\n",
       "      <td>McDonald’s restaurants are offering $1 Any Siz...</td>\n",
       "      <td>$1 McDonald's Any Size Fries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-sno-10-meter-anaconda</th>\n",
       "      <td>A viral video shows a 10-meter-long anaconda s...</td>\n",
       "      <td>Does Video Show a 10-Meter-Long Anaconda?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-sno-10-month-old-baby-drawing</th>\n",
       "      <td>A video shows a 10-month-old baby drawing cart...</td>\n",
       "      <td>Is This Really a Video of a 10-Month-Old Baby ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-sno-100-facebook-shares-appeal</th>\n",
       "      <td>100 Facebook “shares” will secure a heart tran...</td>\n",
       "      <td>100 Facebook Shares Heart Transplant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-sno-100-target-coupon</th>\n",
       "      <td>A $100 Target coupon circulated via Facebook i...</td>\n",
       "      <td>$100 Target Coupon on Facebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-sno-zookeeper-arrested-molest-gorilla</th>\n",
       "      <td>A 29-year-old San Diego Zoo intern was arreste...</td>\n",
       "      <td>Was a Zookeeper Arrested for Molesting a Gorilla?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-sno-zoom-teachers-mute</th>\n",
       "      <td>Zoom allows hosts of meetings, such as educato...</td>\n",
       "      <td>Can Teachers Hear ‘Muted’ Students on Zoom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-sno-zuckerberg-dont-share-hoaxes</th>\n",
       "      <td>Mark Zuckerberg is giving $1,000 away to Faceb...</td>\n",
       "      <td>Mark Zuckerberg Promises $1,000 to Facebook Us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-sno-zuckerberg-facebook-ireland-ads</th>\n",
       "      <td>Mark Zuckerberg admitted or bragged that Faceb...</td>\n",
       "      <td>Did Mark Zuckerberg ‘Brag’ About Banning Pro-L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vclaim-sno-zuckerberg-removing-pages</th>\n",
       "      <td>Mark Zuckerberg will delete Donald Trump's or ...</td>\n",
       "      <td>Mark Zuckerberg Removing Clinton, Trump Pages ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13825 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         vclaim  \\\n",
       "vclaim_id                                                                                         \n",
       "vclaim-sno-1-mcdonald39s-any-size-fries       McDonald’s restaurants are offering $1 Any Siz...   \n",
       "vclaim-sno-10-meter-anaconda                  A viral video shows a 10-meter-long anaconda s...   \n",
       "vclaim-sno-10-month-old-baby-drawing          A video shows a 10-month-old baby drawing cart...   \n",
       "vclaim-sno-100-facebook-shares-appeal         100 Facebook “shares” will secure a heart tran...   \n",
       "vclaim-sno-100-target-coupon                  A $100 Target coupon circulated via Facebook i...   \n",
       "...                                                                                         ...   \n",
       "vclaim-sno-zookeeper-arrested-molest-gorilla  A 29-year-old San Diego Zoo intern was arreste...   \n",
       "vclaim-sno-zoom-teachers-mute                 Zoom allows hosts of meetings, such as educato...   \n",
       "vclaim-sno-zuckerberg-dont-share-hoaxes       Mark Zuckerberg is giving $1,000 away to Faceb...   \n",
       "vclaim-sno-zuckerberg-facebook-ireland-ads    Mark Zuckerberg admitted or bragged that Faceb...   \n",
       "vclaim-sno-zuckerberg-removing-pages          Mark Zuckerberg will delete Donald Trump's or ...   \n",
       "\n",
       "                                                                                          title  \n",
       "vclaim_id                                                                                        \n",
       "vclaim-sno-1-mcdonald39s-any-size-fries                            $1 McDonald's Any Size Fries  \n",
       "vclaim-sno-10-meter-anaconda                          Does Video Show a 10-Meter-Long Anaconda?  \n",
       "vclaim-sno-10-month-old-baby-drawing          Is This Really a Video of a 10-Month-Old Baby ...  \n",
       "vclaim-sno-100-facebook-shares-appeal                      100 Facebook Shares Heart Transplant  \n",
       "vclaim-sno-100-target-coupon                                     $100 Target Coupon on Facebook  \n",
       "...                                                                                         ...  \n",
       "vclaim-sno-zookeeper-arrested-molest-gorilla  Was a Zookeeper Arrested for Molesting a Gorilla?  \n",
       "vclaim-sno-zoom-teachers-mute                       Can Teachers Hear ‘Muted’ Students on Zoom?  \n",
       "vclaim-sno-zuckerberg-dont-share-hoaxes       Mark Zuckerberg Promises $1,000 to Facebook Us...  \n",
       "vclaim-sno-zuckerberg-facebook-ireland-ads    Did Mark Zuckerberg ‘Brag’ About Banning Pro-L...  \n",
       "vclaim-sno-zuckerberg-removing-pages          Mark Zuckerberg Removing Clinton, Trump Pages ...  \n",
       "\n",
       "[13825 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read verified claims file\n",
    "df_claim = Utils.read_file(claims_file)\n",
    "df_claim[VCLAIM_ID]= df_claim[VCLAIM_ID].astype(str)\n",
    "df_claim.set_index(VCLAIM_ID, inplace=True)\n",
    "df_claim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load QRELs file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>Q0</th>\n",
       "      <th>docno</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tweet-sno-0</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-sno-hero-wombats-australia-fires</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tweet-sno-1</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-sno-ilhan-omar-treason-advice-iran</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tweet-sno-2</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-sno-us-army-sending-texts-on-draft</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tweet-sno-3</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-sno-us-army-sending-texts-on-draft</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tweet-sno-4</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-sno-video-of-drone-strike-soleimani</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>tweet-sno-1397</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-sno-trump-covid-fundraising</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>tweet-sno-1398</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-sno-amy-coney-barrett-rhodes-scholar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>tweet-sno-1399</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-sno-biden-fail-condemn-antifa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>tweet-sno-1400</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-sno-trump-condemn-white-supremacists</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>tweet-sno-1401</td>\n",
       "      <td>0</td>\n",
       "      <td>vclaim-sno-bloomberg-pay-florida-felons-vote</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1400 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 qid  Q0                                         docno  label\n",
       "0        tweet-sno-0   0       vclaim-sno-hero-wombats-australia-fires      1\n",
       "1        tweet-sno-1   0     vclaim-sno-ilhan-omar-treason-advice-iran      1\n",
       "2        tweet-sno-2   0     vclaim-sno-us-army-sending-texts-on-draft      1\n",
       "3        tweet-sno-3   0     vclaim-sno-us-army-sending-texts-on-draft      1\n",
       "4        tweet-sno-4   0    vclaim-sno-video-of-drone-strike-soleimani      1\n",
       "...              ...  ..                                           ...    ...\n",
       "1395  tweet-sno-1397   0            vclaim-sno-trump-covid-fundraising      1\n",
       "1396  tweet-sno-1398   0   vclaim-sno-amy-coney-barrett-rhodes-scholar      1\n",
       "1397  tweet-sno-1399   0          vclaim-sno-biden-fail-condemn-antifa      1\n",
       "1398  tweet-sno-1400   0   vclaim-sno-trump-condemn-white-supremacists      1\n",
       "1399  tweet-sno-1401   0  vclaim-sno-bloomberg-pay-florida-felons-vote      1\n",
       "\n",
       "[1400 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_qrels(qrels_file):\n",
    "    df_qrels = pd.read_csv(qrels_file, sep=\"\\t\", names=[\"qid\", \"Q0\", \"docno\", LABEL])\n",
    "    df_qrels[\"qid\"]=df_qrels[\"qid\"].astype(str)\n",
    "    df_qrels[\"docno\"]=df_qrels[\"docno\"].astype(str)\n",
    "    return df_qrels\n",
    "\n",
    "df_qrels = get_qrels(qrels_file)\n",
    "df_qrels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create multi-fields index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 13825\n",
      "Number of terms: 14822\n",
      "Number of postings: 163240\n",
      "Number of fields: 2\n",
      "Number of tokens: 228726\n",
      "Field names: [text, title]\n",
      "Positions:   false\n",
      "\n",
      "Index has been loaded successfully\n"
     ]
    }
   ],
   "source": [
    "def build_index(claims_file=cf.ENG_2021_VCLAIMS, index_path=\"./indexes/en-clef-2021-index-multi-field\"):\n",
    "\n",
    "    def get_document():\n",
    "        for i, row in df_doc.iterrows():\n",
    "            yield {\"docno\": row[VCLAIM_ID], \"text\": row[VCLAIM], \"title\": row[TITLE]}\n",
    "\n",
    "    # load the documents (verified claims) and apply preprocessing steps over them\n",
    "    df_doc = Utils.read_file(claims_file)  \n",
    "    df_doc[VCLAIM] = df_doc[VCLAIM].apply(preprocess)\n",
    "    df_doc[TITLE] = df_doc[TITLE].apply(preprocess)\n",
    "    df_doc[\"text\"] = df_doc[VCLAIM].astype(str)\n",
    "    df_doc[\"docno\"] = df_doc[VCLAIM_ID].astype(str)\n",
    "\n",
    "\n",
    "    iter_indexer = pt.IterDictIndexer(index_path,  overwrite=True, verbose=True)\n",
    "    # the default is an English tokenizer: Tokenises text obtained from a text stream assuming English language.\n",
    "    iter_indexer.setProperty(\"tokeniser\", \"EnglishTokeniser\")\n",
    "    indexref3 = iter_indexer.index(get_document(), fields=[\"text\", \"title\"], meta=['docno'])\n",
    "    print(indexref3.toString())\n",
    "\n",
    "    # first load the index\n",
    "    multi_field_index = pt.IndexFactory.of(indexref3)\n",
    "    print(\"Index was build on path \", indexref3.toString())\n",
    "    print(multi_field_index.getCollectionStatistics().toString())\n",
    "\n",
    "    return multi_field_index\n",
    "\n",
    "def load_index(index_path):\n",
    "    try:\n",
    "            # first load the index\n",
    "        multi_field_index = pt.IndexFactory.of(index_path)\n",
    "        # call getCollectionStatistics() to check the stats\n",
    "        print(multi_field_index.getCollectionStatistics().toString())\n",
    "        print(\"Index has been loaded successfully\")\n",
    "        return multi_field_index\n",
    "    except Exception as e:\n",
    "        print('Cannot load the index, check exception details {}'.format(e))\n",
    "        return []\n",
    "\n",
    "\n",
    "multi_field_index = load_index(index_path=index_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_query_for_search(query_path, data_column=\"cleaned\"):\n",
    "\n",
    "    print(\"Cleaning queries and applying preprocessing steps\")\n",
    "    df_query = Utils.read_file(query_path)\n",
    "    # try test quries after extracting some information from urls\n",
    "    df_query[QUERY] =df_query[data_column].apply(clean)\n",
    "    df_query[QUERY] =df_query[QUERY].apply(Utils.remove_emoji_smileys)\n",
    "    df_query[QUERY] =df_query[QUERY].apply(Utils.remove_punctuation)\n",
    "    df_query[QUERY] = df_query[QUERY].apply(preprocess)\n",
    "    df_query[QID] = df_query[TWEET_ID_COLUMN].astype(str)\n",
    "    df_query = df_query[[QID, QUERY]]\n",
    "    print(\"Done with cleaning!\")\n",
    "    return df_query\n",
    "    \n",
    "def search_and_evaluate(query_path, run_save_path, retrieval_model=\"BM25\", evaluation_path=\"\",\n",
    "                        data_column='cleaned', depth=100, method_name=\"BM25\"):\n",
    "    \n",
    "    df_query = clean_query_for_search(query_path, data_column=data_column)\n",
    "    # intialize BM25 model to get the top 100 potentially relevant documents\n",
    "    bm25_retr = pt.BatchRetrieve(multi_field_index, controls = {\"wmodel\": retrieval_model},num_results=depth)\n",
    "    print(\"Searching for the queries .....\")\n",
    "    # retrieve potentially relevant documents for each query in queries file\n",
    "    bm25_res = bm25_retr.transform(df_query)\n",
    "    # save the run in trec format\n",
    "    bm25_res.to_csv(run_save_path, header=False, index=False, sep='\\t')\n",
    "\n",
    "\n",
    "    # Evaluate the performance\n",
    "    print(\"Perfomring evaluation ......\")\n",
    "    bm25_eval = pt.Utils.evaluate(bm25_res, df_qrels[[\"qid\", \"docno\", LABEL]],metrics=eval_metrics)\n",
    "    bm25_eval.update({\"name\": method_name})\n",
    "    bm25_eval.update({\"depth\": depth})\n",
    "    bm25_eval = pd.DataFrame([bm25_eval])\n",
    "    bm25_eval = bm25_eval.round(DECIMAL_ROUND)\n",
    "\n",
    "    # save evaluation results\n",
    "    if evaluation_path != \"\":\n",
    "        if not os.path.isfile(evaluation_path): # if file is not exist, create a new one\n",
    "            bm25_eval.to_excel(evaluation_path, index=False)\n",
    "        else: # it is already exist, append current evaluation to it\n",
    "            df_eval = Utils.read_file(evaluation_path)\n",
    "            df_eval = df_eval.append(bm25_eval, ignore_index=True)\n",
    "            df_eval.to_excel(evaluation_path, index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Done searching and evaluation \")\n",
    "    return bm25_res, bm25_eval\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning queries and applying preprocessing steps\n",
      "Done with cleaning!\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Done with cleaning!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>map</th>\n",
       "      <th>Rprec</th>\n",
       "      <th>RR</th>\n",
       "      <th>P@1</th>\n",
       "      <th>R@5</th>\n",
       "      <th>R@10</th>\n",
       "      <th>R@20</th>\n",
       "      <th>R@50</th>\n",
       "      <th>AP@5</th>\n",
       "      <th>...</th>\n",
       "      <th>R@20 p-value</th>\n",
       "      <th>R@50 +</th>\n",
       "      <th>R@50 -</th>\n",
       "      <th>R@50 p-value</th>\n",
       "      <th>AP@5 +</th>\n",
       "      <th>AP@5 -</th>\n",
       "      <th>AP@5 p-value</th>\n",
       "      <th>RR@5 +</th>\n",
       "      <th>RR@5 -</th>\n",
       "      <th>RR@5 p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JM</td>\n",
       "      <td>0.880876</td>\n",
       "      <td>0.844221</td>\n",
       "      <td>0.880876</td>\n",
       "      <td>0.844221</td>\n",
       "      <td>0.929648</td>\n",
       "      <td>0.934673</td>\n",
       "      <td>0.949749</td>\n",
       "      <td>0.954774</td>\n",
       "      <td>0.878894</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JM+PreP</td>\n",
       "      <td>0.914051</td>\n",
       "      <td>0.884422</td>\n",
       "      <td>0.914051</td>\n",
       "      <td>0.884422</td>\n",
       "      <td>0.954774</td>\n",
       "      <td>0.959799</td>\n",
       "      <td>0.964824</td>\n",
       "      <td>0.964824</td>\n",
       "      <td>0.913065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083261</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.318531</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004128</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BM25</td>\n",
       "      <td>0.892202</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.892202</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.924623</td>\n",
       "      <td>0.929648</td>\n",
       "      <td>0.949749</td>\n",
       "      <td>0.954774</td>\n",
       "      <td>0.889447</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BM25+PreP</td>\n",
       "      <td>0.923946</td>\n",
       "      <td>0.904523</td>\n",
       "      <td>0.923946</td>\n",
       "      <td>0.904523</td>\n",
       "      <td>0.944724</td>\n",
       "      <td>0.949749</td>\n",
       "      <td>0.964824</td>\n",
       "      <td>0.964824</td>\n",
       "      <td>0.921692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083261</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.318531</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DPH</td>\n",
       "      <td>0.870232</td>\n",
       "      <td>0.834171</td>\n",
       "      <td>0.870232</td>\n",
       "      <td>0.834171</td>\n",
       "      <td>0.924623</td>\n",
       "      <td>0.934673</td>\n",
       "      <td>0.944724</td>\n",
       "      <td>0.954774</td>\n",
       "      <td>0.867755</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DPH+PreP</td>\n",
       "      <td>0.892412</td>\n",
       "      <td>0.849246</td>\n",
       "      <td>0.892412</td>\n",
       "      <td>0.849246</td>\n",
       "      <td>0.949749</td>\n",
       "      <td>0.959799</td>\n",
       "      <td>0.964824</td>\n",
       "      <td>0.969849</td>\n",
       "      <td>0.890620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045223</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083261</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.043046</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.032985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>DLH</td>\n",
       "      <td>0.891557</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.891557</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.929648</td>\n",
       "      <td>0.944724</td>\n",
       "      <td>0.944724</td>\n",
       "      <td>0.954774</td>\n",
       "      <td>0.889363</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DLH+PreP</td>\n",
       "      <td>0.919877</td>\n",
       "      <td>0.894472</td>\n",
       "      <td>0.919877</td>\n",
       "      <td>0.894472</td>\n",
       "      <td>0.949749</td>\n",
       "      <td>0.959799</td>\n",
       "      <td>0.964824</td>\n",
       "      <td>0.964824</td>\n",
       "      <td>0.918090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045223</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.318531</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011384</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DFR_BM25</td>\n",
       "      <td>0.891849</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.891849</td>\n",
       "      <td>0.864322</td>\n",
       "      <td>0.924623</td>\n",
       "      <td>0.929648</td>\n",
       "      <td>0.949749</td>\n",
       "      <td>0.954774</td>\n",
       "      <td>0.889196</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DFR_BM25+PreP</td>\n",
       "      <td>0.923592</td>\n",
       "      <td>0.904523</td>\n",
       "      <td>0.923592</td>\n",
       "      <td>0.904523</td>\n",
       "      <td>0.944724</td>\n",
       "      <td>0.949749</td>\n",
       "      <td>0.964824</td>\n",
       "      <td>0.964824</td>\n",
       "      <td>0.921441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.083261</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.318531</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rm3_pipe</td>\n",
       "      <td>0.736715</td>\n",
       "      <td>0.603015</td>\n",
       "      <td>0.736715</td>\n",
       "      <td>0.603015</td>\n",
       "      <td>0.904523</td>\n",
       "      <td>0.914573</td>\n",
       "      <td>0.929648</td>\n",
       "      <td>0.934673</td>\n",
       "      <td>0.734087</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>rm3_pipe+PreP</td>\n",
       "      <td>0.759821</td>\n",
       "      <td>0.618090</td>\n",
       "      <td>0.759821</td>\n",
       "      <td>0.618090</td>\n",
       "      <td>0.934673</td>\n",
       "      <td>0.944724</td>\n",
       "      <td>0.949749</td>\n",
       "      <td>0.954774</td>\n",
       "      <td>0.757956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045223</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045223</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.041873</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.041873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             name       map     Rprec        RR       P@1       R@5      R@10  \\\n",
       "0              JM  0.880876  0.844221  0.880876  0.844221  0.929648  0.934673   \n",
       "1         JM+PreP  0.914051  0.884422  0.914051  0.884422  0.954774  0.959799   \n",
       "2            BM25  0.892202  0.864322  0.892202  0.864322  0.924623  0.929648   \n",
       "3       BM25+PreP  0.923946  0.904523  0.923946  0.904523  0.944724  0.949749   \n",
       "4             DPH  0.870232  0.834171  0.870232  0.834171  0.924623  0.934673   \n",
       "5        DPH+PreP  0.892412  0.849246  0.892412  0.849246  0.949749  0.959799   \n",
       "6             DLH  0.891557  0.864322  0.891557  0.864322  0.929648  0.944724   \n",
       "7        DLH+PreP  0.919877  0.894472  0.919877  0.894472  0.949749  0.959799   \n",
       "8        DFR_BM25  0.891849  0.864322  0.891849  0.864322  0.924623  0.929648   \n",
       "9   DFR_BM25+PreP  0.923592  0.904523  0.923592  0.904523  0.944724  0.949749   \n",
       "10       rm3_pipe  0.736715  0.603015  0.736715  0.603015  0.904523  0.914573   \n",
       "11  rm3_pipe+PreP  0.759821  0.618090  0.759821  0.618090  0.934673  0.944724   \n",
       "\n",
       "        R@20      R@50      AP@5  ...  R@20 p-value  R@50 +  R@50 -  \\\n",
       "0   0.949749  0.954774  0.878894  ...           NaN     NaN     NaN   \n",
       "1   0.964824  0.964824  0.913065  ...      0.083261     3.0     1.0   \n",
       "2   0.949749  0.954774  0.889447  ...           NaN     NaN     NaN   \n",
       "3   0.964824  0.964824  0.921692  ...      0.083261     3.0     1.0   \n",
       "4   0.944724  0.954774  0.867755  ...           NaN     NaN     NaN   \n",
       "5   0.964824  0.969849  0.890620  ...      0.045223     3.0     0.0   \n",
       "6   0.944724  0.954774  0.889363  ...           NaN     NaN     NaN   \n",
       "7   0.964824  0.964824  0.918090  ...      0.045223     3.0     1.0   \n",
       "8   0.949749  0.954774  0.889196  ...           NaN     NaN     NaN   \n",
       "9   0.964824  0.964824  0.921441  ...      0.083261     3.0     1.0   \n",
       "10  0.929648  0.934673  0.734087  ...           NaN     NaN     NaN   \n",
       "11  0.949749  0.954774  0.757956  ...      0.045223     4.0     0.0   \n",
       "\n",
       "    R@50 p-value  AP@5 +  AP@5 -  AP@5 p-value  RR@5 +  RR@5 -  RR@5 p-value  \n",
       "0            NaN     NaN     NaN           NaN     NaN     NaN           NaN  \n",
       "1       0.318531    11.0     1.0      0.004128    10.0     0.0      0.002646  \n",
       "2            NaN     NaN     NaN           NaN     NaN     NaN           NaN  \n",
       "3       0.318531     9.0     1.0      0.008511     8.0     0.0      0.005957  \n",
       "4            NaN     NaN     NaN           NaN     NaN     NaN           NaN  \n",
       "5       0.083261     9.0     3.0      0.043046     8.0     2.0      0.032985  \n",
       "6            NaN     NaN     NaN           NaN     NaN     NaN           NaN  \n",
       "7       0.318531     9.0     1.0      0.011384     8.0     0.0      0.007683  \n",
       "8            NaN     NaN     NaN           NaN     NaN     NaN           NaN  \n",
       "9       0.318531     9.0     1.0      0.008511     8.0     0.0      0.005957  \n",
       "10           NaN     NaN     NaN           NaN     NaN     NaN           NaN  \n",
       "11      0.045223    21.0     9.0      0.041873    21.0     9.0      0.041873  \n",
       "\n",
       "[12 rows x 41 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Read test queries\n",
    "\n",
    "df_dev_query  = clean_query_for_search(raw_dev_query, data_column=TWEET_TEXT_COLUMN)\n",
    "df_dev_query_after_preprocessing = clean_query_for_search(expanded_dev_query, data_column=\"cleaned\")\n",
    "\n",
    "# retrieval models\n",
    "depth = 100 \n",
    "JM = pt.BatchRetrieve(multi_field_index,wmodel=\"Hiemstra_LM\",controls ={\"c\":0.05},num_results=depth)\n",
    "bm25_retr = pt.BatchRetrieve(multi_field_index, controls = {\"wmodel\": \"BM25\",},num_results=depth)\n",
    "DPH = pt.BatchRetrieve(multi_field_index,wmodel=\"DPH\", num_results=depth)\n",
    "DLH = pt.BatchRetrieve(multi_field_index,wmodel=\"DLH\", num_results=depth)\n",
    "DFR_BM25 = pt.BatchRetrieve(multi_field_index,wmodel=\"DFR_BM25\", num_results=depth)\n",
    "rm3_pipe = bm25_retr >> pt.rewrite.RM3(multi_field_index,fb_terms=10, fb_docs=3) >> bm25_retr\n",
    "\n",
    "retrieval_models = [JM, bm25_retr, DPH, DLH, DFR_BM25, rm3_pipe]\n",
    "retreival_model_names =  [\"JM\", \"BM25\", \"DPH\", \"DLH\", \"DFR_BM25\", \"rm3_pipe\"]\n",
    "\n",
    "df_eval_all = pd.DataFrame()\n",
    "for i in range(len(retrieval_models)):\n",
    "    retrieval_model = retrieval_models[i]\n",
    "    retrieval_model_name = retreival_model_names[i]\n",
    "\n",
    "    retrieval_list = retrieval_model.transform(df_dev_query)\n",
    "    retrieval_list_after_preprocessing= retrieval_model.transform(df_dev_query_after_preprocessing)\n",
    "\n",
    "\n",
    "    df_eval = pt.Experiment([retrieval_list, retrieval_list_after_preprocessing],\n",
    "                df_dev_query,\n",
    "                df_qrels[[\"qid\", \"docno\", LABEL]],\n",
    "                eval_metrics=eval_metrics,\n",
    "                names=[retrieval_model_name, retrieval_model_name+\"+PreP\"],\n",
    "                baseline=0)\n",
    "    df_eval_all = df_eval_all.append(df_eval, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "df_eval_all.to_excel(\"./data/en-2021_classic_retrieval_with_preprocessing_on_dev.xlsx\", index=False)\n",
    "df_eval_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training data for mono BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_set(mono_train_set_creator, train_query_path, train_bm25_run_path, bm25_evaluation_save_path,train_set_save_path, \n",
    "                    search_depth=100, query_column=\"cleaned\", retrieval_model=\"BM25\", depth_of_random=20,\n",
    "                    what_to_add=cf.VCLAIM_AND_TITLE,  add_similarity=False, lang=\"en\"):\n",
    "\n",
    "\n",
    "   # 1- create run for both dev and train queries\n",
    "    print(\"Make runs and evaluation\")\n",
    "    mono_train_set_creator.search_and_evaluate(query_path=train_query_path, evaluation_path=bm25_evaluation_save_path, \n",
    "                        retrieval_model=retrieval_model, run_save_path=train_bm25_run_path, \n",
    "                        depth=search_depth, method_name=\"bm25_train+dev\", data_column=query_column, lang=lang)\n",
    "    print(\"Done with runs and evaluation\")\n",
    "\n",
    "    # 2- create the training set\n",
    "    print(\"Creating train set\")\n",
    "    mono_train_set_creator.create_train_pairs(train_query_path, train_bm25_run_path, train_set_save_path, \n",
    "                            query_column=\"cleaned\", what_to_add=what_to_add, \n",
    "                            depth_of_random=depth_of_random, add_similarity=add_similarity,)\n",
    "    print(\"Done creating train set\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 13825\n",
      "Number of terms: 14822\n",
      "Number of postings: 163240\n",
      "Number of fields: 2\n",
      "Number of tokens: 228726\n",
      "Field names: [text, title]\n",
      "Positions:   false\n",
      "\n",
      "Index has been loaded successfully\n"
     ]
    }
   ],
   "source": [
    "mono_train_set_creator = MonoBertTrainSetCreator(qrels_file, claims_file, index_save_path, eval_metrics)\n",
    "mono_bert_trainer = MonoBertTrainer(train_query_path, dev_query_path, qrels_path=qrels_file,)\n",
    "mono_bert_tester = MonoBertTester(qrels_path=qrels_file,\n",
    "                                evaluation_save_path=evaluation_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make runs and evaluation\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating train set\n",
      "Processing tweet number 0 with tweet id tweet-sno-0 \n",
      "Processing tweet number 100 with tweet id tweet-sno-100 \n",
      "Processing tweet number 200 with tweet id tweet-sno-200 \n",
      "Processing tweet number 300 with tweet id tweet-sno-300 \n",
      "Processing tweet number 400 with tweet id tweet-sno-400 \n",
      "Processing tweet number 500 with tweet id tweet-sno-500 \n",
      "Processing tweet number 600 with tweet id tweet-sno-600 \n",
      "Processing tweet number 700 with tweet id tweet-sno-700 \n",
      "Processing tweet number 800 with tweet id tweet-sno-800 \n",
      "Processing tweet number 900 with tweet id tweet-sno-901 \n",
      "Done creating train set\n",
      "Make runs and evaluation\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating train set\n",
      "Processing tweet number 0 with tweet id tweet-sno-0 \n",
      "Processing tweet number 100 with tweet id tweet-sno-100 \n",
      "Processing tweet number 200 with tweet id tweet-sno-200 \n",
      "Processing tweet number 300 with tweet id tweet-sno-300 \n",
      "Processing tweet number 400 with tweet id tweet-sno-400 \n",
      "Processing tweet number 500 with tweet id tweet-sno-500 \n",
      "Processing tweet number 600 with tweet id tweet-sno-600 \n",
      "Processing tweet number 700 with tweet id tweet-sno-700 \n",
      "Processing tweet number 800 with tweet id tweet-sno-800 \n",
      "Processing tweet number 900 with tweet id tweet-sno-901 \n",
      "Done creating train set\n",
      "Make runs and evaluation\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating train set\n",
      "Processing tweet number 0 with tweet id tweet-sno-0 \n",
      "Processing tweet number 100 with tweet id tweet-sno-100 \n",
      "Processing tweet number 200 with tweet id tweet-sno-200 \n",
      "Processing tweet number 300 with tweet id tweet-sno-300 \n",
      "Processing tweet number 400 with tweet id tweet-sno-400 \n",
      "Processing tweet number 500 with tweet id tweet-sno-500 \n",
      "Processing tweet number 600 with tweet id tweet-sno-600 \n",
      "Processing tweet number 700 with tweet id tweet-sno-700 \n",
      "Processing tweet number 800 with tweet id tweet-sno-800 \n",
      "Processing tweet number 900 with tweet id tweet-sno-901 \n",
      "Done creating train set\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_train_set_both = create_train_set(mono_train_set_creator, train_query_path, train_bm25_run, \"\",\n",
    "                    train_set_save_path=mono_bert_train_both_vclaim_and_title_set, \n",
    "                    search_depth=search_depth, query_column=\"cleaned\", retrieval_model=\"BM25\", depth_of_random=RANDOM_DEPTH,\n",
    "                    what_to_add=cf.VCLAIM_AND_TITLE, add_similarity=False,)\n",
    "\n",
    "df_train_set_title_only = create_train_set(mono_train_set_creator, train_query_path, train_bm25_run, \"\",\n",
    "                    train_set_save_path=mono_bert_train_title_set, \n",
    "                    search_depth=search_depth, query_column=\"cleaned\", retrieval_model=\"BM25\", depth_of_random=RANDOM_DEPTH,\n",
    "                    what_to_add=cf.TITLE_ONLY, add_similarity=False,)\n",
    "\n",
    "df_train_set_vclaim = create_train_set(mono_train_set_creator, train_query_path, train_bm25_run, \"\",\n",
    "                    train_set_save_path=mono_bert_train_vclaim_set, \n",
    "                    search_depth=search_depth, query_column=\"cleaned\", retrieval_model=\"BM25\", depth_of_random=RANDOM_DEPTH,\n",
    "                    what_to_add=cf.VCLAIM_ONLY, add_similarity=False,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpful function for creating the dev and test sets in monoBERT fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create test set suitable for mono BERT re-ranker\n",
    "def create_test_set(mono_train_set_creator, query_path, run_path, bm25_evaluation_save_path, pairs_save_path, \n",
    "                    search_depth=100, query_column=\"cleaned\", retrieval_model=\"BM25\", lang=\"en\"):\n",
    "\n",
    "\n",
    "   # 1- create run for both dev and train queries\n",
    "    print(\"Make runs and evaluation for depth \", search_depth)\n",
    "    mono_train_set_creator.search_and_evaluate(query_path=query_path, evaluation_path=bm25_evaluation_save_path, \n",
    "                        retrieval_model=retrieval_model, run_save_path=run_path, \n",
    "                        depth=search_depth, method_name=\"bm25_dev-or-test\", data_column=query_column, lang=lang)\n",
    "    print(\"Done with runs and evaluation\")\n",
    "\n",
    "    # 2- create the training set\n",
    "    print(\"Creating test/dev set for mono BERT from queries path: \", query_path)\n",
    "    mono_train_set_creator.create_test_pairs(query_path, run_path, query_column=\"cleaned\", \n",
    "                    pairs_save_path=pairs_save_path,)\n",
    "    print(\"Done creating test set for mono BERT\")\n",
    "    \n",
    "    return  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dev sets from top k retrieved documents by bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make runs and evaluation for depth  10\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2021/English/url_cleaned_dev_queries_2021.xlsx\n",
      "1000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  20\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2021/English/url_cleaned_dev_queries_2021.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  30\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2021/English/url_cleaned_dev_queries_2021.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "4000  rows have been created so far \n",
      "5000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  50\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2021/English/url_cleaned_dev_queries_2021.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "4000  rows have been created so far \n",
      "5000  rows have been created so far \n",
      "6000  rows have been created so far \n",
      "7000  rows have been created so far \n",
      "8000  rows have been created so far \n",
      "9000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  100\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2021/English/url_cleaned_dev_queries_2021.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "4000  rows have been created so far \n",
      "5000  rows have been created so far \n",
      "6000  rows have been created so far \n",
      "7000  rows have been created so far \n",
      "8000  rows have been created so far \n",
      "9000  rows have been created so far \n",
      "10000  rows have been created so far \n",
      "11000  rows have been created so far \n",
      "12000  rows have been created so far \n",
      "13000  rows have been created so far \n",
      "14000  rows have been created so far \n",
      "15000  rows have been created so far \n",
      "16000  rows have been created so far \n",
      "17000  rows have been created so far \n",
      "18000  rows have been created so far \n",
      "19000  rows have been created so far \n",
      "Done creating test set for mono BERT\n"
     ]
    }
   ],
   "source": [
    "\n",
    "depths = [10, 20, 30, 50, 100,]\n",
    "for i in range(len(depths)):\n",
    "    depth = depths[i]\n",
    "    bm25_run_path =  \"./data/runs/bm25_dev_query_en_clef_2021_depth_\" + str(depth) + \".tsv\"\n",
    "    mono_bert_dev_set = \"./data/CLEF_2021/English/dev_sets/en-clef2021-mono_bert_dev_set_top_\" + str(depth) + \".tsv\"\n",
    "\n",
    "    create_test_set(mono_train_set_creator, dev_query_path, bm25_run_path, \n",
    "                    bm25_evaluation_save_path=\"\", pairs_save_path=mono_bert_dev_set, \n",
    "                    search_depth=depth, query_column=\"cleaned\", retrieval_model=\"BM25\", lang=\"en\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------\n",
    "## Create test sets from top k documents retrieved by bm25\n",
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make runs and evaluation for depth  10\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2021/English/url_cleaned_test_queries_2021.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  20\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2021/English/url_cleaned_test_queries_2021.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "4000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  30\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2021/English/url_cleaned_test_queries_2021.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "4000  rows have been created so far \n",
      "5000  rows have been created so far \n",
      "6000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  50\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2021/English/url_cleaned_test_queries_2021.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "4000  rows have been created so far \n",
      "5000  rows have been created so far \n",
      "6000  rows have been created so far \n",
      "7000  rows have been created so far \n",
      "8000  rows have been created so far \n",
      "9000  rows have been created so far \n",
      "10000  rows have been created so far \n",
      "Done creating test set for mono BERT\n",
      "Make runs and evaluation for depth  100\n",
      "Cleaning queries and applying preprocessing steps\n",
      "Applying lowercasing, stemming and stop word removal for English\n",
      "Done with cleaning!\n",
      "Searching for the queries .....\n",
      "Done searching and evaluation \n",
      "Done with runs and evaluation\n",
      "Creating test/dev set for mono BERT from queries path:  ./data/CLEF_2021/English/url_cleaned_test_queries_2021.xlsx\n",
      "1000  rows have been created so far \n",
      "2000  rows have been created so far \n",
      "3000  rows have been created so far \n",
      "4000  rows have been created so far \n",
      "5000  rows have been created so far \n",
      "6000  rows have been created so far \n",
      "7000  rows have been created so far \n",
      "8000  rows have been created so far \n",
      "9000  rows have been created so far \n",
      "10000  rows have been created so far \n",
      "11000  rows have been created so far \n",
      "12000  rows have been created so far \n",
      "13000  rows have been created so far \n",
      "14000  rows have been created so far \n",
      "15000  rows have been created so far \n",
      "16000  rows have been created so far \n",
      "17000  rows have been created so far \n",
      "18000  rows have been created so far \n",
      "19000  rows have been created so far \n",
      "20000  rows have been created so far \n",
      "Done creating test set for mono BERT\n"
     ]
    }
   ],
   "source": [
    "depths = [10, 20, 30, 50, 100,]\n",
    "\n",
    "for i in range(len(depths)):\n",
    "    depth = depths[i]\n",
    "    bm25_run_path =  \"./data/runs/bm25_test_query_en_clef_2021_depth_\" + str(depth) + \".tsv\"\n",
    "    mono_bert_test_set_path = \"./data/CLEF_2021/English/test_sets/mono_bert_test_set_top_\" + str(depth) + \".tsv\"\n",
    "\n",
    "    \n",
    "    create_test_set(mono_train_set_creator, test_query_path, bm25_run_path, \n",
    "                    bm25_evaluation_save_path=\"\", pairs_save_path=mono_bert_test_set_path, \n",
    "                    search_depth=depth, query_column=\"cleaned\", retrieval_model=\"BM25\", lang=\"en\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the maximum length needed to tokenize a pair of query and document in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of a tokenized example is :  222\n",
      "Sentence Lengths: \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents that are bigger than max legnth \n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# measure the maximum length needed to tokenize a triplet\n",
    "def get_pairs_max_length(data_path, tokenizer, max_length):\n",
    "    df = Utils.read_file(data_path)\n",
    "    lengths = []\n",
    "    maxl = 0\n",
    "    for i, row in df.iterrows():\n",
    "        query = row[TWEET_TEXT_COLUMN]\n",
    "        document = row[VCLAIM]\n",
    "        row_len = len(tokenizer.tokenize(query)) +len(tokenizer.tokenize(document))\n",
    "        lengths.append(row_len)\n",
    "        maxl = max(maxl, row_len)\n",
    "    print(\"Maximum length of a tokenized example is : \", maxl)\n",
    "    print(\"Sentence Lengths: \")\n",
    "    plt.hist(lengths ,bins=range(0,256,2))\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Number of documents that are bigger than max legnth \")\n",
    "    print(sum([length > max_length for length in lengths]))\n",
    "    \n",
    "    return \n",
    "\n",
    "BERT = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT)\n",
    "get_pairs_max_length(mono_bert_train_set ,tokenizer, max_length=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the model on dev set to figure out the best depth of the initial retrieved list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- training for model : -------- stsb-mpnet-base-v2\n",
      "device:  cuda:0\n",
      "train size  3996\n",
      "dev size  3980\n",
      "Epoch 1/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3189.00\n",
      "  Accuracy : 0.80\n",
      "  Average training loss: 0.41\n",
      "relevance results F1 score  0.786451442180471  precision 0.8343627175743964 recall 0.7437437437437437\n",
      " Macro F1 0.7974507388922768 Weighted F1 0.7974507388922768 Accuracy 0.7980480480480481\n",
      "Train loss 0.4148215020895004 accuracy 0.7980480480480481\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.93\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.13 \n",
      "\n",
      "Dev loss 0.12558557152748107 accuracy 0.97035175879397 eval_measures {'map': 0.932719151312116, 'AP@5': 0.9321608040201005, 'P@1': 0.914572864321608, 'RR': 0.9327191513121161, 'Rprec': 0.914572864321608, 'R@5': 0.9597989949748744, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9321608040201006}\n",
      "Epoch 2/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3659.00\n",
      "  Accuracy : 0.92\n",
      "  Average training loss: 0.19\n",
      "relevance results F1 score  0.9112457203055043  precision 0.9616453585325181 recall 0.8658658658658659\n",
      " Macro F1 0.9154559948368999 Weighted F1 0.9154559948368999 Accuracy 0.9156656656656657\n",
      "Train loss 0.19409922689199446 accuracy 0.9156656656656657\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.99\n",
      "  Average Validation loss: 0.05 \n",
      "\n",
      "Dev loss 0.049898413822054864 accuracy 0.9864321608040202 eval_measures {'map': 0.9505862646566163, 'AP@5': 0.9505862646566163, 'P@1': 0.9396984924623115, 'RR': 0.9505862646566163, 'Rprec': 0.9396984924623115, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9505862646566166}\n",
      "Epoch 3/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3744.00\n",
      "  Accuracy : 0.94\n",
      "  Average training loss: 0.15\n",
      "relevance results F1 score  0.9343408025013027  precision 0.9744565217391304 recall 0.8973973973973974\n",
      " Macro F1 0.9368381913325001 Weighted F1 0.9368381913325001 Accuracy 0.9369369369369369\n",
      "Train loss 0.14778287100791931 accuracy 0.9369369369369369\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:27.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.99\n",
      "  Average Validation loss: 0.04 \n",
      "\n",
      "Dev loss 0.040547667723149064 accuracy 0.9891959798994976 eval_measures {'map': 0.9484924623115578, 'AP@5': 0.9476549413735342, 'P@1': 0.9396984924623115, 'RR': 0.9484924623115578, 'Rprec': 0.9396984924623115, 'R@5': 0.9597989949748744, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9476549413735343}\n",
      "Epoch 4/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3782.00\n",
      "  Accuracy : 0.95\n",
      "  Average training loss: 0.11\n",
      "relevance results F1 score  0.9438909281594128  precision 0.9911894273127754 recall 0.9009009009009009\n",
      " Macro F1 0.9463351242041678 Weighted F1 0.9463351242041679 Accuracy 0.9464464464464465\n",
      "Train loss 0.11170770938694477 accuracy 0.9464464464464465\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.96\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.08 \n",
      "\n",
      "Dev loss 0.07969508326984942 accuracy 0.9801507537688443 eval_measures {'map': 0.9589614740368508, 'AP@5': 0.9589614740368508, 'P@1': 0.9547738693467337, 'RR': 0.9589614740368508, 'Rprec': 0.9547738693467337, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.958961474036851}\n",
      "Epoch 5/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3791.00\n",
      "  Accuracy : 0.95\n",
      "  Average training loss: 0.09\n",
      "relevance results F1 score  0.9461518255844497  precision 0.9955776672194583 recall 0.9014014014014013\n",
      " Macro F1 0.9485836786225713 Weighted F1 0.9485836786225714 Accuracy 0.9486986986986987\n",
      "Train loss 0.08579727580025792 accuracy 0.9486986986986987\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:27.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.96\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.11 \n",
      "\n",
      "Dev loss 0.1061643541418016 accuracy 0.9738693467336684 eval_measures {'map': 0.9589614740368508, 'AP@5': 0.9589614740368508, 'P@1': 0.9547738693467337, 'RR': 0.9589614740368508, 'Rprec': 0.9547738693467337, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.958961474036851}\n",
      " -------------------Training complete for learning rate 2e-05 and number of epochs 5 ------ \n",
      "Train loss 0.08579727580025792 accuracy 0.9486986986986987 end_of_curriculum 0\n",
      "Dev loss 0.1061643541418016 dev 0.9738693467336684 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      " ------------------- Overall Training complete! ---------------------------------------\n",
      "Best learning rate 2e-05 and number of epochs 5 end_of_curriculum 0 ------ \n",
      "Best Dev loss 0.1061643541418016 and dev accuracy  0.9738693467336684 best map {'map': 0.9589614740368508, 'AP@5': 0.9589614740368508, 'P@1': 0.9547738693467337, 'RR': 0.9589614740368508, 'Rprec': 0.9547738693467337, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.958961474036851}\n",
      "\n",
      " ------------------------------------------------------------ \n",
      "Running Testing ...\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef-2021-mono_stsb-mpnet-base-v2_rerankded_data-dev-set_of_top_10.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2021/en-clef-2021-mono_stsb-mpnet-base-v2_rerankded_data-dev-set_of_top_10.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.9463986599664991, 'AP@5': 0.9463986599664991, 'P@1': 0.9447236180904522, 'RR': 0.9463986599664991, 'Rprec': 0.9447236180904522, 'R@5': 0.949748743718593, 'R@10': 0.949748743718593, 'R@20': 0.949748743718593, 'R@50': 0.949748743718593, 'RR@5': 0.9463986599664992}\n",
      "Running Testing ...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:53.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef-2021-mono_stsb-mpnet-base-v2_rerankded_data-dev-set_of_top_20.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2021/en-clef-2021-mono_stsb-mpnet-base-v2_rerankded_data-dev-set_of_top_20.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.9542504187604689, 'AP@5': 0.9539363484087101, 'P@1': 0.949748743718593, 'RR': 0.9542504187604689, 'Rprec': 0.949748743718593, 'R@5': 0.9597989949748744, 'R@10': 0.9597989949748744, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9539363484087102}\n",
      "Running Testing ...\n",
      "  Batch   100  of    187.    Elapsed: 0:00:54.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef-2021-mono_stsb-mpnet-base-v2_rerankded_data-dev-set_of_top_30.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2021/en-clef-2021-mono_stsb-mpnet-base-v2_rerankded_data-dev-set_of_top_30.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.949129706503532, 'AP@5': 0.9489112227805694, 'P@1': 0.9396984924623115, 'RR': 0.949129706503532, 'Rprec': 0.9396984924623115, 'R@5': 0.9597989949748744, 'R@10': 0.9597989949748744, 'R@20': 0.9597989949748744, 'R@50': 0.964824120603015, 'RR@5': 0.9489112227805696}\n",
      "Running Testing ...\n",
      "  Batch   100  of    311.    Elapsed: 0:00:54.\n",
      "  Batch   200  of    311.    Elapsed: 0:01:47.\n",
      "  Batch   300  of    311.    Elapsed: 0:02:41.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef-2021-mono_stsb-mpnet-base-v2_rerankded_data-dev-set_of_top_50.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2021/en-clef-2021-mono_stsb-mpnet-base-v2_rerankded_data-dev-set_of_top_50.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.9490470369867354, 'AP@5': 0.9489112227805694, 'P@1': 0.9396984924623115, 'RR': 0.9490470369867354, 'Rprec': 0.9396984924623115, 'R@5': 0.9597989949748744, 'R@10': 0.9597989949748744, 'R@20': 0.9597989949748744, 'R@50': 0.964824120603015, 'RR@5': 0.9489112227805696}\n",
      "Running Testing ...\n",
      "  Batch   100  of    622.    Elapsed: 0:00:54.\n",
      "  Batch   200  of    622.    Elapsed: 0:01:48.\n",
      "  Batch   300  of    622.    Elapsed: 0:02:41.\n",
      "  Batch   400  of    622.    Elapsed: 0:03:35.\n",
      "  Batch   500  of    622.    Elapsed: 0:04:28.\n",
      "  Batch   600  of    622.    Elapsed: 0:05:21.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef-2021-mono_stsb-mpnet-base-v2_rerankded_data-dev-set_of_top_100.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2021/en-clef-2021-mono_stsb-mpnet-base-v2_rerankded_data-dev-set_of_top_100.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.9450477022795134, 'AP@5': 0.9448911222780568, 'P@1': 0.9296482412060302, 'RR': 0.9450477022795135, 'Rprec': 0.9296482412060302, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9448911222780569}\n"
     ]
    }
   ],
   "source": [
    "depth_evaluation_file = \"./data/evaluation/en-clef-2021-mono-bert-depth-evaluation-on-dev-set.xlsx\"\n",
    "bert_models = [ \"sentence-transformers/stsb-mpnet-base-v2\",]# just the best performed model are chosen for this step\n",
    "models_names = [\"stsb-mpnet-base-v2\",]\n",
    "\n",
    "num_rand_seeds  = 5\n",
    "seeds = random_seeds[:num_rand_seeds]\n",
    "for i in range(len(bert_models)):\n",
    "    l = 0\n",
    "    seed = seeds[l]\n",
    "    bert_model = bert_models[i]\n",
    "    model_name = models_names[i]\n",
    "    print(\"---------- training for model : --------\", model_name)\n",
    "    hp = { # hyper parameters\n",
    "    \"model_name\": bert_model,\n",
    "    \"name\": model_name +\"-seed-\"+ str(seed),\n",
    "    \"model_save_path\": \"./data/saved_models/EN-clef2021-\"+model_name+\"_mono_trained_model-seed-\"+ str(seed)+\".bin\",\n",
    "    \"model_training_log\": \"./data/bert_evaluation/en-clef2021-mono_\"+model_name+\"_training_log-seed-\"+ str(seed)+\".xlsx\",\n",
    "    \"batch_size\": 32,\n",
    "    \"num_of_epochs\": [5], \n",
    "    \"learning_rate\" : [2e-5,],\n",
    "    \"dropout\": [0.3,], \n",
    "    \"seeds\": [seed],\n",
    "    \"max_len\": 256,\n",
    "    \"curricula_type\": 0,\n",
    "    \"end_of_curriculum\": [0],\n",
    "    \"num_of_layers\": ONE_LAYER,\n",
    "    \"is_output_probability\": False, # if false, put loss_function = mono_loss, otherwise put loss_function=CrossEntropy\n",
    "    \"loss_function\": \"mono_loss\"}\n",
    "   \n",
    "\n",
    "\n",
    "    _, _, best_learning_rate, best_num_of_epochs, best_end_of_curriculum, best_dropout = mono_bert_trainer.train_mono_bert(\n",
    "                        mono_bert_train_set, mono_bert_dev_set_depth_20, hp[\"model_name\"], apply_cleaning=False,  seeds=hp[\"seeds\"],\n",
    "                        trained_model_save_path=hp[\"model_save_path\"], shuffle=True, freeze_bert=False, max_len=hp[\"max_len\"], \n",
    "                        batch_size=hp[\"batch_size\"],epochs=hp[\"num_of_epochs\"], learning_rates = hp[\"learning_rate\"],\n",
    "                        is_output_probability=hp[\"is_output_probability\"],  end_of_curriculums=hp[\"end_of_curriculum\"],\n",
    "                        curricula_type=hp[\"curricula_type\"], dropout=hp[\"dropout\"], hp=hp, results_path=hp[\"model_training_log\"],\n",
    "                        classifier_layers=hp[\"num_of_layers\"], what_to_eval=cf.VCLAIM_AND_TITLE)\n",
    "\n",
    "\n",
    "    # store the best hyperparameters\n",
    "    hp[\"num_of_epochs\"]= best_num_of_epochs\n",
    "    hp[\"learning_rate\"]= best_learning_rate\n",
    "    hp[\"end_of_curriculum\"]= best_end_of_curriculum\n",
    "    hp[\"dropout\"]= best_dropout\n",
    "\n",
    "    depths = [10, 20, 30, 50, 100,]\n",
    "    for k in range(len(depths)):\n",
    "        depth = depths[k]\n",
    "        mono_bert_dev_set = \"./data/CLEF_2021/English/dev_sets/en-clef2021-mono_bert_dev_set_top_\" + str(depth) + \".tsv\"\n",
    "        hp[\"test_depth\"] = depth\n",
    "        run_name = \"en-clef-2021-mono_\"+model_name+\"_rerankded_data-dev-set_of_top_\"+str(depth)+\".tsv\"\n",
    "        hp[\"reranked_data_path\"] = \"./data/runs/\" + run_name \n",
    "        hp[\"trec_run_path\"] = \"./data/runs/trec_eval/EN2021/\"+ run_name\n",
    "        \n",
    "        mono_bert_tester.test_mono_bert(hp[\"model_name\"], hp[\"model_save_path\"], mono_bert_dev_set, \n",
    "                            hp[\"reranked_data_path\"],  depth_evaluation_file, max_len=hp[\"max_len\"], batch_size=hp[\"batch_size\"],\n",
    "                            dropout=hp[\"dropout\"], apply_cleaning=False, is_output_probability=hp[\"is_output_probability\"], \n",
    "                            hyper_parameters=hp, classifier_layers=hp[\"num_of_layers\"], trec_run_path=hp[\"trec_run_path\"],\n",
    "                            what_to_test=cf.VCLAIM_AND_TITLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training sentence BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- training for model : -------- paraphrase-mpnet-base-v2\n",
      "device:  cuda:0\n",
      "train size  3996\n",
      "dev size  3980\n",
      "Epoch 1/3\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:04.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3217.00\n",
      "  Accuracy : 0.81\n",
      "  Average training loss: 0.41\n",
      "relevance results F1 score  0.7949460384311662  precision 0.8384230982787341 recall 0.7557557557557557\n",
      " Macro F1 0.8045801024495445 Weighted F1 0.8045801024495445 Accuracy 0.805055055055055\n",
      "Train loss 0.4102092065811157 accuracy 0.805055055055055\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.94\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.12 \n",
      "\n",
      "Dev loss 0.11687562397122384 accuracy 0.971105527638191 eval_measures {'map': 0.9376884422110554, 'AP@5': 0.9376884422110554, 'P@1': 0.9195979899497487, 'RR': 0.9376884422110554, 'Rprec': 0.9195979899497487, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9376884422110554}\n",
      "Epoch 2/3\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:04.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3631.00\n",
      "  Accuracy : 0.91\n",
      "  Average training loss: 0.21\n",
      "relevance results F1 score  0.903820816864295  precision 0.9543683917640512 recall 0.8583583583583584\n",
      " Macro F1 0.9084269678793717 Weighted F1 0.9084269678793718 Accuracy 0.9086586586586587\n",
      "Train loss 0.20732271352410317 accuracy 0.9086586586586587\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.06 \n",
      "\n",
      "Dev loss 0.06487762849032878 accuracy 0.9806532663316584 eval_measures {'map': 0.9522613065326633, 'AP@5': 0.9522613065326633, 'P@1': 0.9396984924623115, 'RR': 0.9522613065326633, 'Rprec': 0.9396984924623115, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9522613065326633}\n",
      "Epoch 3/3\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:04.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3761.00\n",
      "  Accuracy : 0.94\n",
      "  Average training loss: 0.13\n",
      "relevance results F1 score  0.9381416162147934  precision 0.9894503053858967 recall 0.8918918918918919\n",
      " Macro F1 0.9410479128057034 Weighted F1 0.9410479128057033 Accuracy 0.9411911911911912\n",
      "Train loss 0.13497682458162308 accuracy 0.9411911911911912\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.10 \n",
      "\n",
      "Dev loss 0.09673763723671436 accuracy 0.9731155778894474 eval_measures {'map': 0.949748743718593, 'AP@5': 0.949748743718593, 'P@1': 0.9346733668341709, 'RR': 0.949748743718593, 'Rprec': 0.9346733668341709, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.949748743718593}\n",
      " -------------------Training complete for learning rate 2e-05 and number of epochs 3 ------ \n",
      "Train loss 0.13497682458162308 accuracy 0.9411911911911912 end_of_curriculum 0\n",
      "Dev loss 0.09673763723671436 dev 0.9731155778894474 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      "train size  3996\n",
      "dev size  3980\n",
      "Epoch 1/3\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:04.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3144.00\n",
      "  Accuracy : 0.79\n",
      "  Average training loss: 0.44\n",
      "relevance results F1 score  0.7617449664429531  precision 0.8631178707224335 recall 0.6816816816816816\n",
      " Macro F1 0.7844050919171288 Weighted F1 0.7844050919171287 Accuracy 0.7867867867867868\n",
      "Train loss 0.44179414904117587 accuracy 0.7867867867867868\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.94\n",
      "  Accuracy: 0.95\n",
      "  Average Validation loss: 0.19 \n",
      "\n",
      "Dev loss 0.18549120885133744 accuracy 0.9457286432160805 eval_measures {'map': 0.9354271356783919, 'AP@5': 0.9354271356783919, 'P@1': 0.9195979899497487, 'RR': 0.9354271356783919, 'Rprec': 0.9195979899497487, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.935427135678392}\n",
      "Epoch 2/3\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:04.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3495.00\n",
      "  Accuracy : 0.87\n",
      "  Average training loss: 0.25\n",
      "relevance results F1 score  0.8627021101671691  precision 0.9533615990308903 recall 0.7877877877877878\n",
      " Macro F1 0.8736720313672595 Weighted F1 0.8736720313672595 Accuracy 0.8746246246246246\n",
      "Train loss 0.24630862724781036 accuracy 0.8746246246246246\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.06 \n",
      "\n",
      "Dev loss 0.062284315019845965 accuracy 0.9836683417085428 eval_measures {'map': 0.9511725293132328, 'AP@5': 0.9511725293132328, 'P@1': 0.9447236180904522, 'RR': 0.9511725293132328, 'Rprec': 0.9447236180904522, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9511725293132328}\n",
      "Epoch 3/3\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3616.00\n",
      "  Accuracy : 0.90\n",
      "  Average training loss: 0.17\n",
      "relevance results F1 score  0.897018970189702  precision 0.9781323877068558 recall 0.8283283283283284\n",
      " Macro F1 0.9043439806783006 Weighted F1 0.9043439806783006 Accuracy 0.9049049049049049\n",
      "Train loss 0.17267410400509833 accuracy 0.9049049049049049\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.09 \n",
      "\n",
      "Dev loss 0.0922538987621665 accuracy 0.970854271356784 eval_measures {'map': 0.9514237855946398, 'AP@5': 0.9514237855946398, 'P@1': 0.9447236180904522, 'RR': 0.9514237855946398, 'Rprec': 0.9447236180904522, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9514237855946399}\n",
      " -------------------Training complete for learning rate 2e-05 and number of epochs 3 ------ \n",
      "Train loss 0.17267410400509833 accuracy 0.9049049049049049 end_of_curriculum 0\n",
      "Dev loss 0.0922538987621665 dev 0.970854271356784 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      "train size  3996\n",
      "dev size  3980\n",
      "Epoch 1/3\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3220.00\n",
      "  Accuracy : 0.81\n",
      "  Average training loss: 0.41\n",
      "relevance results F1 score  0.795142555438226  precision 0.841340782122905 recall 0.7537537537537538\n",
      " Macro F1 0.8052782234850502 Weighted F1 0.8052782234850503 Accuracy 0.8058058058058059\n",
      "Train loss 0.40909988814592363 accuracy 0.8058058058058059\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.92\n",
      "  Accuracy: 0.95\n",
      "  Average Validation loss: 0.17 \n",
      "\n",
      "Dev loss 0.16623296812176705 accuracy 0.9525125628140705 eval_measures {'map': 0.9233668341708543, 'AP@5': 0.9233668341708543, 'P@1': 0.8894472361809045, 'RR': 0.9233668341708543, 'Rprec': 0.8894472361809045, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9233668341708543}\n",
      "Epoch 2/3\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3659.00\n",
      "  Accuracy : 0.92\n",
      "  Average training loss: 0.20\n",
      "relevance results F1 score  0.9115253347335258  precision 0.958586416344561 recall 0.8688688688688688\n",
      " Macro F1 0.9154805731759907 Weighted F1 0.9154805731759907 Accuracy 0.9156656656656657\n",
      "Train loss 0.2044786587357521 accuracy 0.9156656656656657\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.08 \n",
      "\n",
      "Dev loss 0.08040651115775109 accuracy 0.977889447236181 eval_measures {'map': 0.9480737018425461, 'AP@5': 0.9472361809045227, 'P@1': 0.9396984924623115, 'RR': 0.9480737018425461, 'Rprec': 0.9396984924623115, 'R@5': 0.9597989949748744, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9472361809045227}\n",
      "Epoch 3/3\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3778.00\n",
      "  Accuracy : 0.95\n",
      "  Average training loss: 0.12\n",
      "relevance results F1 score  0.9429319371727748  precision 0.9884742041712404 recall 0.9014014014014013\n",
      " Macro F1 0.9453394105806348 Weighted F1 0.9453394105806348 Accuracy 0.9454454454454454\n",
      "Train loss 0.1188442964181304 accuracy 0.9454454454454454\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.10 \n",
      "\n",
      "Dev loss 0.101834931448102 accuracy 0.9761306532663317 eval_measures {'map': 0.9535175879396985, 'AP@5': 0.9535175879396985, 'P@1': 0.9447236180904522, 'RR': 0.9535175879396985, 'Rprec': 0.9447236180904522, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9535175879396985}\n",
      " -------------------Training complete for learning rate 3e-05 and number of epochs 3 ------ \n",
      "Train loss 0.1188442964181304 accuracy 0.9454454454454454 end_of_curriculum 0\n",
      "Dev loss 0.101834931448102 dev 0.9761306532663317 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      "train size  3996\n",
      "dev size  3980\n",
      "Epoch 1/3\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3164.00\n",
      "  Accuracy : 0.79\n",
      "  Average training loss: 0.42\n",
      "relevance results F1 score  0.7718047174986286  precision 0.8537621359223301 recall 0.7042042042042042\n",
      " Macro F1 0.7901821562642706 Weighted F1 0.7901821562642706 Accuracy 0.7917917917917918\n",
      "Train loss 0.4225429766178131 accuracy 0.7917917917917918\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.91\n",
      "  Accuracy: 0.95\n",
      "  Average Validation loss: 0.18 \n",
      "\n",
      "Dev loss 0.1821298820078373 accuracy 0.9535175879396985 eval_measures {'map': 0.9095477386934674, 'AP@5': 0.9087102177554438, 'P@1': 0.8592964824120602, 'RR': 0.9095477386934674, 'Rprec': 0.8592964824120602, 'R@5': 0.9597989949748744, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9087102177554439}\n",
      "Epoch 2/3\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3487.00\n",
      "  Accuracy : 0.87\n",
      "  Average training loss: 0.24\n",
      "relevance results F1 score  0.8595086944521115  precision 0.9581538461538461 recall 0.7792792792792793\n",
      " Macro F1 0.8715030311354172 Weighted F1 0.8715030311354172 Accuracy 0.8726226226226226\n",
      "Train loss 0.2379067135155201 accuracy 0.8726226226226226\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.96\n",
      "  Accuracy: 0.99\n",
      "  Average Validation loss: 0.06 \n",
      "\n",
      "Dev loss 0.05511771862208843 accuracy 0.9856783919597991 eval_measures {'map': 0.9597989949748744, 'AP@5': 0.9597989949748744, 'P@1': 0.9547738693467337, 'RR': 0.9597989949748744, 'Rprec': 0.9547738693467337, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9597989949748744}\n",
      "Epoch 3/3\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3634.00\n",
      "  Accuracy : 0.91\n",
      "  Average training loss: 0.16\n",
      "relevance results F1 score  0.9015233949945594  precision 0.9874851013110846 recall 0.8293293293293293\n",
      " Macro F1 0.9088247188133131 Weighted F1 0.908824718813313 Accuracy 0.9094094094094094\n",
      "Train loss 0.16059023821353913 accuracy 0.9094094094094094\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.08 \n",
      "\n",
      "Dev loss 0.08103510714694857 accuracy 0.9746231155778895 eval_measures {'map': 0.9522613065326633, 'AP@5': 0.9522613065326633, 'P@1': 0.9396984924623115, 'RR': 0.9522613065326633, 'Rprec': 0.9396984924623115, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9522613065326633}\n",
      " -------------------Training complete for learning rate 3e-05 and number of epochs 3 ------ \n",
      "Train loss 0.16059023821353913 accuracy 0.9094094094094094 end_of_curriculum 0\n",
      "Dev loss 0.08103510714694857 dev 0.9746231155778895 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      "train size  3996\n",
      "dev size  3980\n",
      "Epoch 1/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3218.00\n",
      "  Accuracy : 0.81\n",
      "  Average training loss: 0.39\n",
      "relevance results F1 score  0.7861462341946124  precision 0.8719512195121951 recall 0.7157157157157157\n",
      " Macro F1 0.8037299843458132 Weighted F1 0.8037299843458133 Accuracy 0.8053053053053053\n",
      "Train loss 0.39369012624025346 accuracy 0.8053053053053053\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.94\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.08 \n",
      "\n",
      "Dev loss 0.08130424290895462 accuracy 0.9796482412060302 eval_measures {'map': 0.9441505774486468, 'AP@5': 0.9438860971524287, 'P@1': 0.9296482412060302, 'RR': 0.9441505774486468, 'Rprec': 0.9296482412060302, 'R@5': 0.9597989949748744, 'R@10': 0.9597989949748744, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9438860971524289}\n",
      "Epoch 2/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3687.00\n",
      "  Accuracy : 0.92\n",
      "  Average training loss: 0.20\n",
      "relevance results F1 score  0.9191734240125556  precision 0.9627397260273972 recall 0.8793793793793794\n",
      " Macro F1 0.9225274651844979 Weighted F1 0.922527465184498 Accuracy 0.9226726726726727\n",
      "Train loss 0.1958287997841835 accuracy 0.9226726726726727\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.94\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.11 \n",
      "\n",
      "Dev loss 0.1070426826030016 accuracy 0.9706030150753769 eval_measures {'map': 0.9447236180904522, 'AP@5': 0.9447236180904522, 'P@1': 0.9246231155778895, 'RR': 0.9447236180904522, 'Rprec': 0.9246231155778895, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9447236180904522}\n",
      "Epoch 3/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:34.\n",
      "\n",
      "  correct_predictions: 3735.00\n",
      "  Accuracy : 0.93\n",
      "  Average training loss: 0.14\n",
      "relevance results F1 score  0.9314060446780552  precision 0.9806308799114555 recall 0.8868868868868869\n",
      " Macro F1 0.9345351216941745 Weighted F1 0.9345351216941745 Accuracy 0.9346846846846847\n",
      "Train loss 0.13813094257563352 accuracy 0.9346846846846847\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.06 \n",
      "\n",
      "Dev loss 0.06325708223879337 accuracy 0.9836683417085428 eval_measures {'map': 0.9522613065326633, 'AP@5': 0.9522613065326633, 'P@1': 0.9396984924623115, 'RR': 0.9522613065326633, 'Rprec': 0.9396984924623115, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9522613065326633}\n",
      "Epoch 4/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3780.00\n",
      "  Accuracy : 0.95\n",
      "  Average training loss: 0.10\n",
      "relevance results F1 score  0.9434258774227344  precision 0.9895604395604396 recall 0.9014014014014013\n",
      " Macro F1 0.945838477762637 Weighted F1 0.945838477762637 Accuracy 0.9459459459459459\n",
      "Train loss 0.10234690408408642 accuracy 0.9459459459459459\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.11 \n",
      "\n",
      "Dev loss 0.1121246723588556 accuracy 0.9736180904522614 eval_measures {'map': 0.9522613065326633, 'AP@5': 0.9522613065326633, 'P@1': 0.9396984924623115, 'RR': 0.9522613065326633, 'Rprec': 0.9396984924623115, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9522613065326633}\n",
      " -------------------Training complete for learning rate 2e-05 and number of epochs 4 ------ \n",
      "Train loss 0.10234690408408642 accuracy 0.9459459459459459 end_of_curriculum 0\n",
      "Dev loss 0.1121246723588556 dev 0.9736180904522614 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      "train size  3996\n",
      "dev size  3980\n",
      "Epoch 1/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3104.00\n",
      "  Accuracy : 0.78\n",
      "  Average training loss: 0.44\n",
      "relevance results F1 score  0.7596982758620688  precision 0.822637106184364 recall 0.7057057057057057\n",
      " Macro F1 0.7756435304543989 Weighted F1 0.7756435304543989 Accuracy 0.7767767767767768\n",
      "Train loss 0.4399803684949875 accuracy 0.7767767767767768\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.92\n",
      "  Accuracy: 0.91\n",
      "  Average Validation loss: 0.25 \n",
      "\n",
      "Dev loss 0.25416386225819587 accuracy 0.9105527638190956 eval_measures {'map': 0.9237855946398659, 'AP@5': 0.9237855946398659, 'P@1': 0.8894472361809045, 'RR': 0.9237855946398659, 'Rprec': 0.8894472361809045, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9237855946398661}\n",
      "Epoch 2/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3494.00\n",
      "  Accuracy : 0.87\n",
      "  Average training loss: 0.26\n",
      "relevance results F1 score  0.8627665390924002  precision 0.9506024096385542 recall 0.7897897897897898\n",
      " Macro F1 0.8734691024949772 Weighted F1 0.8734691024949772 Accuracy 0.8743743743743744\n",
      "Train loss 0.25675744071602824 accuracy 0.8743743743743744\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.94\n",
      "  Accuracy: 0.96\n",
      "  Average Validation loss: 0.12 \n",
      "\n",
      "Dev loss 0.12036380615085364 accuracy 0.9623115577889448 eval_measures {'map': 0.9405360134003348, 'AP@5': 0.9405360134003348, 'P@1': 0.9195979899497487, 'RR': 0.940536013400335, 'Rprec': 0.9195979899497487, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9405360134003351}\n",
      "Epoch 3/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3593.00\n",
      "  Accuracy : 0.90\n",
      "  Average training loss: 0.19\n",
      "relevance results F1 score  0.890160806759335  precision 0.9772591262716936 recall 0.8173173173173173\n",
      " Macro F1 0.8984692537150827 Weighted F1 0.8984692537150827 Accuracy 0.8991491491491491\n",
      "Train loss 0.19111777970194815 accuracy 0.8991491491491491\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.09 \n",
      "\n",
      "Dev loss 0.08778369146585464 accuracy 0.9766331658291458 eval_measures {'map': 0.9514237855946398, 'AP@5': 0.9514237855946398, 'P@1': 0.9396984924623115, 'RR': 0.9514237855946398, 'Rprec': 0.9396984924623115, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9514237855946399}\n",
      "Epoch 4/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3622.00\n",
      "  Accuracy : 0.91\n",
      "  Average training loss: 0.15\n",
      "relevance results F1 score  0.8978142076502732  precision 0.9885679903730445 recall 0.8223223223223223\n",
      " Macro F1 0.9057399754779529 Weighted F1 0.9057399754779529 Accuracy 0.9064064064064065\n",
      "Train loss 0.15217582097649573 accuracy 0.9064064064064065\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.11 \n",
      "\n",
      "Dev loss 0.11391607924550771 accuracy 0.9695979899497489 eval_measures {'map': 0.9489112227805694, 'AP@5': 0.9489112227805694, 'P@1': 0.9346733668341709, 'RR': 0.9489112227805694, 'Rprec': 0.9346733668341709, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9489112227805696}\n",
      " -------------------Training complete for learning rate 2e-05 and number of epochs 4 ------ \n",
      "Train loss 0.15217582097649573 accuracy 0.9064064064064065 end_of_curriculum 0\n",
      "Dev loss 0.11391607924550771 dev 0.9695979899497489 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      "train size  3996\n",
      "dev size  3980\n",
      "Epoch 1/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3299.00\n",
      "  Accuracy : 0.83\n",
      "  Average training loss: 0.39\n",
      "relevance results F1 score  0.8126847621607095  precision 0.877539175856065 recall 0.7567567567567568\n",
      " Macro F1 0.8247455653463347 Weighted F1 0.8247455653463346 Accuracy 0.8255755755755756\n",
      "Train loss 0.38632824769616125 accuracy 0.8255755755755756\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.90\n",
      "  Accuracy: 0.78\n",
      "  Average Validation loss: 0.55 \n",
      "\n",
      "Dev loss 0.5520660779923201 accuracy 0.7841708542713569 eval_measures {'map': 0.9044029672170376, 'AP@5': 0.9036850921273031, 'P@1': 0.8492462311557789, 'RR': 0.9044029672170376, 'Rprec': 0.8492462311557789, 'R@5': 0.9597989949748744, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9036850921273032}\n",
      "Epoch 2/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3654.00\n",
      "  Accuracy : 0.91\n",
      "  Average training loss: 0.21\n",
      "relevance results F1 score  0.91  precision 0.9594894561598224 recall 0.8653653653653653\n",
      " Macro F1 0.9142080152671755 Weighted F1 0.9142080152671757 Accuracy 0.9144144144144144\n",
      "Train loss 0.20921773453056813 accuracy 0.9144144144144144\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.96\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.06 \n",
      "\n",
      "Dev loss 0.055692710906267165 accuracy 0.9839195979899499 eval_measures {'map': 0.957286432160804, 'AP@5': 0.957286432160804, 'P@1': 0.949748743718593, 'RR': 0.957286432160804, 'Rprec': 0.949748743718593, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.957286432160804}\n",
      "Epoch 3/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3735.00\n",
      "  Accuracy : 0.93\n",
      "  Average training loss: 0.14\n",
      "relevance results F1 score  0.931152730150356  precision 0.9843837144450641 recall 0.8833833833833834\n",
      " Macro F1 0.9345123327019336 Weighted F1 0.9345123327019336 Accuracy 0.9346846846846847\n",
      "Train loss 0.1427873569279909 accuracy 0.9346846846846847\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.96\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.09 \n",
      "\n",
      "Dev loss 0.09132587751746178 accuracy 0.9753768844221107 eval_measures {'map': 0.9597989949748744, 'AP@5': 0.9597989949748744, 'P@1': 0.9547738693467337, 'RR': 0.9597989949748744, 'Rprec': 0.9547738693467337, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9597989949748744}\n",
      "Epoch 4/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3791.00\n",
      "  Accuracy : 0.95\n",
      "  Average training loss: 0.10\n",
      "relevance results F1 score  0.9464052287581699  precision 0.9906951286261632 recall 0.9059059059059059\n",
      " Macro F1 0.9486045822216576 Weighted F1 0.9486045822216574 Accuracy 0.9486986986986987\n",
      "Train loss 0.09687276980280876 accuracy 0.9486986986986987\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.96\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.12 \n",
      "\n",
      "Dev loss 0.12029233208298683 accuracy 0.9693467336683418 eval_measures {'map': 0.9623115577889447, 'AP@5': 0.9623115577889447, 'P@1': 0.9597989949748744, 'RR': 0.9623115577889447, 'Rprec': 0.9597989949748744, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9623115577889447}\n",
      " -------------------Training complete for learning rate 3e-05 and number of epochs 4 ------ \n",
      "Train loss 0.09687276980280876 accuracy 0.9486986986986987 end_of_curriculum 0\n",
      "Dev loss 0.12029233208298683 dev 0.9693467336683418 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      "train size  3996\n",
      "dev size  3980\n",
      "Epoch 1/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3137.00\n",
      "  Accuracy : 0.79\n",
      "  Average training loss: 0.43\n",
      "relevance results F1 score  0.761587565917291  precision 0.854828660436137 recall 0.6866866866866866\n",
      " Macro F1 0.7829355008898372 Weighted F1 0.7829355008898372 Accuracy 0.785035035035035\n",
      "Train loss 0.42555903565883635 accuracy 0.785035035035035\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.93\n",
      "  Accuracy: 0.96\n",
      "  Average Validation loss: 0.15 \n",
      "\n",
      "Dev loss 0.14793046203255653 accuracy 0.9575376884422111 eval_measures {'map': 0.9292294807370183, 'AP@5': 0.9292294807370183, 'P@1': 0.8994974874371859, 'RR': 0.9292294807370184, 'Rprec': 0.8994974874371859, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9292294807370185}\n",
      "Epoch 2/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3483.00\n",
      "  Accuracy : 0.87\n",
      "  Average training loss: 0.28\n",
      "relevance results F1 score  0.8597977589505329  precision 0.9470198675496688 recall 0.7872872872872873\n",
      " Macro F1 0.8707020181782436 Weighted F1 0.8707020181782437 Accuracy 0.8716216216216216\n",
      "Train loss 0.27568285340070725 accuracy 0.8716216216216216\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.08 \n",
      "\n",
      "Dev loss 0.07778358893096447 accuracy 0.9751256281407036 eval_measures {'map': 0.9522613065326633, 'AP@5': 0.9522613065326633, 'P@1': 0.9396984924623115, 'RR': 0.9522613065326633, 'Rprec': 0.9396984924623115, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9522613065326633}\n",
      "Epoch 3/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3597.00\n",
      "  Accuracy : 0.90\n",
      "  Average training loss: 0.20\n",
      "relevance results F1 score  0.8913102696812857  precision 0.9778840406455469 recall 0.8188188188188188\n",
      " Macro F1 0.8994852667545516 Weighted F1 0.8994852667545516 Accuracy 0.9001501501501501\n",
      "Train loss 0.19766082432866097 accuracy 0.9001501501501501\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.08 \n",
      "\n",
      "Dev loss 0.0784701898060739 accuracy 0.9738693467336684 eval_measures {'map': 0.9472361809045227, 'AP@5': 0.9472361809045227, 'P@1': 0.9346733668341709, 'RR': 0.9472361809045227, 'Rprec': 0.9346733668341709, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9472361809045228}\n",
      "Epoch 4/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:04.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3624.00\n",
      "  Accuracy : 0.91\n",
      "  Average training loss: 0.15\n",
      "relevance results F1 score  0.8985823336968374  precision 0.9868263473053892 recall 0.8248248248248248\n",
      " Macro F1 0.9062754406689553 Weighted F1 0.9062754406689552 Accuracy 0.9069069069069069\n",
      "Train loss 0.15214814956486225 accuracy 0.9069069069069069\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.13 \n",
      "\n",
      "Dev loss 0.1292462637722492 accuracy 0.9685929648241207 eval_measures {'map': 0.9484924623115578, 'AP@5': 0.9484924623115578, 'P@1': 0.9346733668341709, 'RR': 0.9484924623115578, 'Rprec': 0.9346733668341709, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9484924623115578}\n",
      " -------------------Training complete for learning rate 3e-05 and number of epochs 4 ------ \n",
      "Train loss 0.15214814956486225 accuracy 0.9069069069069069 end_of_curriculum 0\n",
      "Dev loss 0.1292462637722492 dev 0.9685929648241207 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      "train size  3996\n",
      "dev size  3980\n",
      "Epoch 1/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3243.00\n",
      "  Accuracy : 0.81\n",
      "  Average training loss: 0.41\n",
      "relevance results F1 score  0.7967611336032389  precision 0.8646748681898067 recall 0.7387387387387387\n",
      " Macro F1 0.8105569138974906 Weighted F1 0.8105569138974906 Accuracy 0.8115615615615616\n",
      "Train loss 0.41138957792520525 accuracy 0.8115615615615616\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.96\n",
      "  Accuracy: 0.86\n",
      "  Average Validation loss: 0.40 \n",
      "\n",
      "Dev loss 0.3995478588938713 accuracy 0.85678391959799 eval_measures {'map': 0.9597989949748744, 'AP@5': 0.9597989949748744, 'P@1': 0.9547738693467337, 'RR': 0.9597989949748744, 'Rprec': 0.9547738693467337, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9597989949748744}\n",
      "Epoch 2/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:04.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3649.00\n",
      "  Accuracy : 0.91\n",
      "  Average training loss: 0.22\n",
      "relevance results F1 score  0.909043250327654  precision 0.9543203082003302 recall 0.8678678678678678\n",
      " Macro F1 0.9129846368947343 Weighted F1 0.9129846368947343 Accuracy 0.9131631631631631\n",
      "Train loss 0.22364674955606462 accuracy 0.9131631631631631\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.09 \n",
      "\n",
      "Dev loss 0.08609884117543698 accuracy 0.9763819095477388 eval_measures {'map': 0.9459798994974874, 'AP@5': 0.9459798994974874, 'P@1': 0.9296482412060302, 'RR': 0.9459798994974874, 'Rprec': 0.9296482412060302, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9459798994974874}\n",
      "Epoch 3/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:04.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3714.00\n",
      "  Accuracy : 0.93\n",
      "  Average training loss: 0.16\n",
      "relevance results F1 score  0.9257112750263435  precision 0.9771968854282537 recall 0.8793793793793794\n",
      " Macro F1 0.9292522056733243 Weighted F1 0.9292522056733242 Accuracy 0.9294294294294294\n",
      "Train loss 0.15763441551476717 accuracy 0.9294294294294294\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.07 \n",
      "\n",
      "Dev loss 0.07367161223292351 accuracy 0.977889447236181 eval_measures {'map': 0.9455611390284757, 'AP@5': 0.9455611390284757, 'P@1': 0.9296482412060302, 'RR': 0.9455611390284757, 'Rprec': 0.9296482412060302, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9455611390284758}\n",
      "Epoch 4/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3781.00\n",
      "  Accuracy : 0.95\n",
      "  Average training loss: 0.11\n",
      "relevance results F1 score  0.9437614438922312  precision 0.9884931506849315 recall 0.9029029029029029\n",
      " Macro F1 0.946095161859764 Weighted F1 0.9460951618597638 Accuracy 0.9461961961961962\n",
      "Train loss 0.10802737791836262 accuracy 0.9461961961961962\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.08 \n",
      "\n",
      "Dev loss 0.07761744951456785 accuracy 0.9821608040201006 eval_measures {'map': 0.9539363484087101, 'AP@5': 0.9539363484087101, 'P@1': 0.9447236180904522, 'RR': 0.9539363484087101, 'Rprec': 0.9447236180904522, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9539363484087102}\n",
      "Epoch 5/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3776.00\n",
      "  Accuracy : 0.94\n",
      "  Average training loss: 0.10\n",
      "relevance results F1 score  0.942074776197999  precision 0.9938888888888889 recall 0.8953953953953954\n",
      " Macro F1 0.9448094434161192 Weighted F1 0.9448094434161192 Accuracy 0.944944944944945\n",
      "Train loss 0.09983566716313362 accuracy 0.944944944944945\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.12 \n",
      "\n",
      "Dev loss 0.11713692681491375 accuracy 0.9728643216080403 eval_measures {'map': 0.9539363484087101, 'AP@5': 0.9539363484087101, 'P@1': 0.9447236180904522, 'RR': 0.9539363484087101, 'Rprec': 0.9447236180904522, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9539363484087102}\n",
      " -------------------Training complete for learning rate 2e-05 and number of epochs 5 ------ \n",
      "Train loss 0.09983566716313362 accuracy 0.944944944944945 end_of_curriculum 0\n",
      "Dev loss 0.11713692681491375 dev 0.9728643216080403 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      "train size  3996\n",
      "dev size  3980\n",
      "Epoch 1/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:04.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3083.00\n",
      "  Accuracy : 0.77\n",
      "  Average training loss: 0.45\n",
      "relevance results F1 score  0.7425993797575416  precision 0.8502259522272434 recall 0.6591591591591591\n",
      " Macro F1 0.7686000273365886 Weighted F1 0.7686000273365885 Accuracy 0.7715215215215215\n",
      "Train loss 0.4509281007647514 accuracy 0.7715215215215215\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.94\n",
      "  Accuracy: 0.90\n",
      "  Average Validation loss: 0.29 \n",
      "\n",
      "Dev loss 0.2909392239153385 accuracy 0.8962311557788946 eval_measures {'map': 0.9413735343383584, 'AP@5': 0.9413735343383584, 'P@1': 0.9195979899497487, 'RR': 0.9413735343383584, 'Rprec': 0.9195979899497487, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9413735343383585}\n",
      "Epoch 2/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:04.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3505.00\n",
      "  Accuracy : 0.88\n",
      "  Average training loss: 0.26\n",
      "relevance results F1 score  0.8663945578231294  precision 0.9493142516398331 recall 0.7967967967967968\n",
      " Macro F1 0.8763290834054261 Weighted F1 0.8763290834054261 Accuracy 0.8771271271271271\n",
      "Train loss 0.2586338512599468 accuracy 0.8771271271271271\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.99\n",
      "  Average Validation loss: 0.05 \n",
      "\n",
      "Dev loss 0.05210606854408979 accuracy 0.9866834170854273 eval_measures {'map': 0.9522613065326633, 'AP@5': 0.9522613065326633, 'P@1': 0.9396984924623115, 'RR': 0.9522613065326633, 'Rprec': 0.9396984924623115, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9522613065326633}\n",
      "Epoch 3/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3594.00\n",
      "  Accuracy : 0.90\n",
      "  Average training loss: 0.20\n",
      "relevance results F1 score  0.8904034896401307  precision 0.9778443113772455 recall 0.8173173173173173\n",
      " Macro F1 0.8987170084648388 Weighted F1 0.8987170084648387 Accuracy 0.8993993993993994\n",
      "Train loss 0.19815338400006294 accuracy 0.8993993993993994\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.96\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.09 \n",
      "\n",
      "Dev loss 0.08501102794706822 accuracy 0.9771356783919599 eval_measures {'map': 0.957286432160804, 'AP@5': 0.957286432160804, 'P@1': 0.949748743718593, 'RR': 0.957286432160804, 'Rprec': 0.949748743718593, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.957286432160804}\n",
      "Epoch 4/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:04.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3640.00\n",
      "  Accuracy : 0.91\n",
      "  Average training loss: 0.16\n",
      "relevance results F1 score  0.9033659066232357  precision 0.9869513641755635 recall 0.8328328328328328\n",
      " Macro F1 0.9103644760599929 Weighted F1 0.910364476059993 Accuracy 0.9109109109109109\n",
      "Train loss 0.16126716631650925 accuracy 0.9109109109109109\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.08 \n",
      "\n",
      "Dev loss 0.08207590577378869 accuracy 0.9783919597989951 eval_measures {'map': 0.949748743718593, 'AP@5': 0.949748743718593, 'P@1': 0.9346733668341709, 'RR': 0.949748743718593, 'Rprec': 0.9346733668341709, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.949748743718593}\n",
      "Epoch 5/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3657.00\n",
      "  Accuracy : 0.92\n",
      "  Average training loss: 0.14\n",
      "relevance results F1 score  0.9079054604726977  precision 0.9928698752228164 recall 0.8363363363363363\n",
      " Macro F1 0.9146347065759453 Weighted F1 0.9146347065759451 Accuracy 0.9151651651651652\n",
      "Train loss 0.14335160768032074 accuracy 0.9151651651651652\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.11 \n",
      "\n",
      "Dev loss 0.1083541137650609 accuracy 0.971105527638191 eval_measures {'map': 0.949748743718593, 'AP@5': 0.949748743718593, 'P@1': 0.9346733668341709, 'RR': 0.949748743718593, 'Rprec': 0.9346733668341709, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.949748743718593}\n",
      " -------------------Training complete for learning rate 2e-05 and number of epochs 5 ------ \n",
      "Train loss 0.14335160768032074 accuracy 0.9151651651651652 end_of_curriculum 0\n",
      "Dev loss 0.1083541137650609 dev 0.971105527638191 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      "train size  3996\n",
      "dev size  3980\n",
      "Epoch 1/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3221.00\n",
      "  Accuracy : 0.81\n",
      "  Average training loss: 0.42\n",
      "relevance results F1 score  0.7936085219707056  precision 0.8480364257256687 recall 0.7457457457457457\n",
      " Macro F1 0.8053480419624592 Weighted F1 0.8053480419624592 Accuracy 0.806056056056056\n",
      "Train loss 0.42331455820798874 accuracy 0.806056056056056\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.90\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.09 \n",
      "\n",
      "Dev loss 0.0869538927078247 accuracy 0.9809045226130654 eval_measures {'map': 0.9032244556113903, 'AP@5': 0.9009212730318258, 'P@1': 0.864321608040201, 'RR': 0.9032244556113903, 'Rprec': 0.864321608040201, 'R@5': 0.949748743718593, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9009212730318259}\n",
      "Epoch 2/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3548.00\n",
      "  Accuracy : 0.89\n",
      "  Average training loss: 0.30\n",
      "relevance results F1 score  0.8811671087533156  precision 0.9373589164785553 recall 0.8313313313313313\n",
      " Macro F1 0.8875281304069752 Weighted F1 0.8875281304069752 Accuracy 0.8878878878878879\n",
      "Train loss 0.30440291914343837 accuracy 0.8878878878878879\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.88\n",
      "  Accuracy: 0.95\n",
      "  Average Validation loss: 0.23 \n",
      "\n",
      "Dev loss 0.2303021020591259 accuracy 0.949246231155779 eval_measures {'map': 0.8789782244556115, 'AP@5': 0.878140703517588, 'P@1': 0.8090452261306532, 'RR': 0.8789782244556115, 'Rprec': 0.8090452261306532, 'R@5': 0.9597989949748744, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.878140703517588}\n",
      "Epoch 3/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3684.00\n",
      "  Accuracy : 0.92\n",
      "  Average training loss: 0.21\n",
      "relevance results F1 score  0.9178082191780822  precision 0.9688542825361512 recall 0.8718718718718719\n",
      " Macro F1 0.9217258445747417 Weighted F1 0.9217258445747417 Accuracy 0.9219219219219219\n",
      "Train loss 0.20714933213591574 accuracy 0.9219219219219219\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.96\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.11 \n",
      "\n",
      "Dev loss 0.11486660021543503 accuracy 0.9660804020100503 eval_measures {'map': 0.9589614740368508, 'AP@5': 0.9589614740368508, 'P@1': 0.9547738693467337, 'RR': 0.9589614740368508, 'Rprec': 0.9547738693467337, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.958961474036851}\n",
      "Epoch 4/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3731.00\n",
      "  Accuracy : 0.93\n",
      "  Average training loss: 0.15\n",
      "relevance results F1 score  0.9301344582124967  precision 0.9827298050139276 recall 0.8828828828828829\n",
      " Macro F1 0.9335120969319212 Weighted F1 0.9335120969319212 Accuracy 0.9336836836836837\n",
      "Train loss 0.151908584356308 accuracy 0.9336836836836837\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.09 \n",
      "\n",
      "Dev loss 0.09047997531294823 accuracy 0.9756281407035177 eval_measures {'map': 0.9535175879396985, 'AP@5': 0.9535175879396985, 'P@1': 0.9447236180904522, 'RR': 0.9535175879396985, 'Rprec': 0.9447236180904522, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9535175879396985}\n",
      "Epoch 5/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3778.00\n",
      "  Accuracy : 0.95\n",
      "  Average training loss: 0.11\n",
      "relevance results F1 score  0.9430512016718913  precision 0.9863387978142076 recall 0.9034034034034034\n",
      " Macro F1 0.9453488477139476 Weighted F1 0.9453488477139476 Accuracy 0.9454454454454454\n",
      "Train loss 0.10589160999283194 accuracy 0.9454454454454454\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.13 \n",
      "\n",
      "Dev loss 0.12881780446879565 accuracy 0.9670854271356785 eval_measures {'map': 0.9514237855946398, 'AP@5': 0.9514237855946398, 'P@1': 0.9396984924623115, 'RR': 0.9514237855946398, 'Rprec': 0.9396984924623115, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9514237855946399}\n",
      " -------------------Training complete for learning rate 3e-05 and number of epochs 5 ------ \n",
      "Train loss 0.10589160999283194 accuracy 0.9454454454454454 end_of_curriculum 0\n",
      "Dev loss 0.12881780446879565 dev 0.9670854271356785 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      "train size  3996\n",
      "dev size  3980\n",
      "Epoch 1/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3067.00\n",
      "  Accuracy : 0.77\n",
      "  Average training loss: 0.48\n",
      "relevance results F1 score  0.741585535465925  precision 0.8346900438321854 recall 0.6671671671671672\n",
      " Macro F1 0.7651525584993941 Weighted F1 0.7651525584993941 Accuracy 0.7675175175175175\n",
      "Train loss 0.48366999757289886 accuracy 0.7675175175175175\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.87\n",
      "  Accuracy: 0.86\n",
      "  Average Validation loss: 0.44 \n",
      "\n",
      "Dev loss 0.4387114816904068 accuracy 0.8582914572864323 eval_measures {'map': 0.8665013378581217, 'AP@5': 0.8615577889447236, 'P@1': 0.8241206030150754, 'RR': 0.8665013378581219, 'Rprec': 0.8241206030150754, 'R@5': 0.9195979899497487, 'R@10': 0.9447236180904522, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.8615577889447239}\n",
      "Epoch 2/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:04.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:36.\n",
      "\n",
      "  correct_predictions: 3308.00\n",
      "  Accuracy : 0.83\n",
      "  Average training loss: 0.41\n",
      "relevance results F1 score  0.813953488372093  precision 0.8852941176470588 recall 0.7532532532532532\n",
      " Macro F1 0.8268649603015565 Weighted F1 0.8268649603015565 Accuracy 0.8278278278278278\n",
      "Train loss 0.4124797354340553 accuracy 0.8278278278278278\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.88\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.16 \n",
      "\n",
      "Dev loss 0.15667296683788298 accuracy 0.9763819095477388 eval_measures {'map': 0.8777219430485763, 'AP@5': 0.8760469011725294, 'P@1': 0.8140703517587939, 'RR': 0.8777219430485763, 'Rprec': 0.8140703517587939, 'R@5': 0.9547738693467337, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.8760469011725294}\n",
      "Epoch 3/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3512.00\n",
      "  Accuracy : 0.88\n",
      "  Average training loss: 0.26\n",
      "relevance results F1 score  0.8676872607982504  precision 0.9560240963855422 recall 0.7942942942942943\n",
      " Macro F1 0.8780060669473486 Weighted F1 0.8780060669473485 Accuracy 0.8788788788788788\n",
      "Train loss 0.26471560180187226 accuracy 0.8788788788788788\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.88\n",
      "  Accuracy: 0.95\n",
      "  Average Validation loss: 0.15 \n",
      "\n",
      "Dev loss 0.15470314337313176 accuracy 0.9530150753768846 eval_measures {'map': 0.8798157453936347, 'AP@5': 0.8798157453936347, 'P@1': 0.8040201005025126, 'RR': 0.8798157453936347, 'Rprec': 0.8040201005025126, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.8798157453936349}\n",
      "Epoch 4/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:04.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3569.00\n",
      "  Accuracy : 0.89\n",
      "  Average training loss: 0.20\n",
      "relevance results F1 score  0.8826600714482001  precision 0.9786715417428398 recall 0.8038038038038038\n",
      " Macro F1 0.8922834012191609 Weighted F1 0.8922834012191609 Accuracy 0.8931431431431431\n",
      "Train loss 0.2000860719382763 accuracy 0.8931431431431431\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.88\n",
      "  Accuracy: 0.96\n",
      "  Average Validation loss: 0.14 \n",
      "\n",
      "Dev loss 0.13752522234618664 accuracy 0.9610552763819097 eval_measures {'map': 0.8752093802345057, 'AP@5': 0.8752093802345057, 'P@1': 0.7989949748743719, 'RR': 0.875209380234506, 'Rprec': 0.7989949748743719, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.875209380234506}\n",
      "Epoch 5/5\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3633.00\n",
      "  Accuracy : 0.91\n",
      "  Average training loss: 0.16\n",
      "relevance results F1 score  0.9010089991818926  precision 0.9898142600359496 recall 0.8268268268268268\n",
      " Macro F1 0.9085391816718712 Weighted F1 0.9085391816718712 Accuracy 0.9091591591591591\n",
      "Train loss 0.15770176845788955 accuracy 0.9091591591591591\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.89\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.13 \n",
      "\n",
      "Dev loss 0.1303146854713559 accuracy 0.9655778894472363 eval_measures {'map': 0.8932160804020101, 'AP@5': 0.8932160804020101, 'P@1': 0.8291457286432161, 'RR': 0.8932160804020101, 'Rprec': 0.8291457286432161, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.8932160804020101}\n",
      " -------------------Training complete for learning rate 3e-05 and number of epochs 5 ------ \n",
      "Train loss 0.15770176845788955 accuracy 0.9091591591591591 end_of_curriculum 0\n",
      "Dev loss 0.1303146854713559 dev 0.9655778894472363 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      " ------------------- Overall Training complete! ---------------------------------------\n",
      "Best learning rate 3e-05 and number of epochs 4 end_of_curriculum 0 ------ \n",
      "Best Dev loss 0.12029233208298683 and dev accuracy  0.9693467336683418 best map {'map': 0.9623115577889447, 'AP@5': 0.9623115577889447, 'P@1': 0.9597989949748744, 'RR': 0.9623115577889447, 'Rprec': 0.9597989949748744, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9623115577889447}\n",
      "\n",
      " ------------------------------------------------------------ \n",
      "Running Testing ...\n",
      "  Batch   100  of    127.    Elapsed: 0:00:54.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2021-mono_paraphrase-mpnet-base-v2-seed-61168821_rerankded_data-depth_20.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2021/en-clef2021-mono_paraphrase-mpnet-base-v2-seed-61168821_rerankded_data-depth_20.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.9295929592959297, 'AP@5': 0.9290429042904291, 'P@1': 0.900990099009901, 'RR': 0.9295929592959296, 'Rprec': 0.900990099009901, 'R@5': 0.9653465346534653, 'R@10': 0.9702970297029703, 'R@20': 0.9702970297029703, 'R@50': 0.9702970297029703, 'RR@5': 0.929042904290429}\n",
      "device:  cuda:0\n",
      "train size  3996\n",
      "dev size  3980\n",
      "Epoch 1/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3281.00\n",
      "  Accuracy : 0.82\n",
      "  Average training loss: 0.41\n",
      "relevance results F1 score  0.8065999459020827  precision 0.8775750441436139 recall 0.7462462462462462\n",
      " Macro F1 0.8200636516471997 Weighted F1 0.8200636516471996 Accuracy 0.821071071071071\n",
      "Train loss 0.4050292650461197 accuracy 0.821071071071071\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.93\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.09 \n",
      "\n",
      "Dev loss 0.0879036589562893 accuracy 0.9741206030150755 eval_measures {'map': 0.9258793969849246, 'AP@5': 0.9258793969849246, 'P@1': 0.8944723618090452, 'RR': 0.9258793969849246, 'Rprec': 0.8944723618090452, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9258793969849246}\n",
      "Epoch 2/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3649.00\n",
      "  Accuracy : 0.91\n",
      "  Average training loss: 0.22\n",
      "relevance results F1 score  0.9098467134320603  precision 0.9459751485683414 recall 0.8763763763763763\n",
      " Macro F1 0.913045490435557 Weighted F1 0.913045490435557 Accuracy 0.9131631631631631\n",
      "Train loss 0.22364233976602554 accuracy 0.9131631631631631\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.93\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.05 \n",
      "\n",
      "Dev loss 0.054839307479560374 accuracy 0.9836683417085428 eval_measures {'map': 0.9342546063651589, 'AP@5': 0.9334170854271356, 'P@1': 0.914572864321608, 'RR': 0.9342546063651591, 'Rprec': 0.914572864321608, 'R@5': 0.9597989949748744, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9334170854271356}\n",
      "Epoch 3/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3730.00\n",
      "  Accuracy : 0.93\n",
      "  Average training loss: 0.14\n",
      "relevance results F1 score  0.9303300157150339  precision 0.9758241758241758 recall 0.8888888888888888\n",
      " Macro F1 0.9333010883558399 Weighted F1 0.9333010883558399 Accuracy 0.9334334334334334\n",
      "Train loss 0.14485833448171614 accuracy 0.9334334334334334\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.12 \n",
      "\n",
      "Dev loss 0.11918306629359722 accuracy 0.9688442211055277 eval_measures {'map': 0.9530988274706869, 'AP@5': 0.9530988274706869, 'P@1': 0.9447236180904522, 'RR': 0.9530988274706867, 'Rprec': 0.9447236180904522, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9530988274706869}\n",
      "Epoch 4/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3769.00\n",
      "  Accuracy : 0.94\n",
      "  Average training loss: 0.12\n",
      "relevance results F1 score  0.9406846093545859  precision 0.9841443411700382 recall 0.9009009009009009\n",
      " Macro F1 0.9430914043171488 Weighted F1 0.9430914043171488 Accuracy 0.9431931931931932\n",
      "Train loss 0.1154146138727665 accuracy 0.9431931931931932\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.93\n",
      "  Accuracy: 0.96\n",
      "  Average Validation loss: 0.16 \n",
      "\n",
      "Dev loss 0.15942105695977807 accuracy 0.9618090452261308 eval_measures {'map': 0.9313232830820769, 'AP@5': 0.9313232830820769, 'P@1': 0.9045226130653267, 'RR': 0.9313232830820769, 'Rprec': 0.9045226130653267, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9313232830820771}\n",
      " -------------------Training complete for learning rate 3e-05 and number of epochs 4 ------ \n",
      "Train loss 0.1154146138727665 accuracy 0.9431931931931932 end_of_curriculum 0\n",
      "Dev loss 0.15942105695977807 dev 0.9618090452261308 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      " ------------------- Overall Training complete! ---------------------------------------\n",
      "Best learning rate 3e-05 and number of epochs 4 end_of_curriculum 0 ------ \n",
      "Best Dev loss 0.15942105695977807 and dev accuracy  0.9618090452261308 best map {'map': 0.9313232830820769, 'AP@5': 0.9313232830820769, 'P@1': 0.9045226130653267, 'RR': 0.9313232830820769, 'Rprec': 0.9045226130653267, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9313232830820771}\n",
      "\n",
      " ------------------------------------------------------------ \n",
      "Running Testing ...\n",
      "  Batch   100  of    127.    Elapsed: 0:00:54.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2021-mono_paraphrase-mpnet-base-v2-seed-129995678_rerankded_data-depth_20.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2021/en-clef2021-mono_paraphrase-mpnet-base-v2-seed-129995678_rerankded_data-depth_20.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.8962458745874587, 'AP@5': 0.8956270627062706, 'P@1': 0.8366336633663366, 'RR': 0.8962458745874589, 'Rprec': 0.8366336633663366, 'R@5': 0.9653465346534653, 'R@10': 0.9702970297029703, 'R@20': 0.9702970297029703, 'R@50': 0.9702970297029703, 'RR@5': 0.8956270627062707}\n",
      "device:  cuda:0\n",
      "train size  3996\n",
      "dev size  3980\n",
      "Epoch 1/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3173.00\n",
      "  Accuracy : 0.79\n",
      "  Average training loss: 0.44\n",
      "relevance results F1 score  0.7815237589593842  precision 0.8321085358959864 recall 0.7367367367367368\n",
      " Macro F1 0.79336542977555 Weighted F1 0.79336542977555 Accuracy 0.794044044044044\n",
      "Train loss 0.4375585668087006 accuracy 0.794044044044044\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.94\n",
      "  Accuracy: 0.95\n",
      "  Average Validation loss: 0.18 \n",
      "\n",
      "Dev loss 0.17665582808852195 accuracy 0.9484924623115579 eval_measures {'map': 0.9396984924623115, 'AP@5': 0.9396984924623115, 'P@1': 0.914572864321608, 'RR': 0.9396984924623115, 'Rprec': 0.914572864321608, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9396984924623115}\n",
      "Epoch 2/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3617.00\n",
      "  Accuracy : 0.91\n",
      "  Average training loss: 0.22\n",
      "relevance results F1 score  0.8994961548660833  precision 0.9565707839819515 recall 0.8488488488488488\n",
      " Macro F1 0.9048535026877207 Weighted F1 0.9048535026877207 Accuracy 0.9051551551551551\n",
      "Train loss 0.2246821728646755 accuracy 0.9051551551551551\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.94\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.06 \n",
      "\n",
      "Dev loss 0.05643951075524092 accuracy 0.9849246231155779 eval_measures {'map': 0.9384422110552764, 'AP@5': 0.9376046901172528, 'P@1': 0.9195979899497487, 'RR': 0.9384422110552764, 'Rprec': 0.9195979899497487, 'R@5': 0.9597989949748744, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.937604690117253}\n",
      "Epoch 3/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3730.00\n",
      "  Accuracy : 0.93\n",
      "  Average training loss: 0.14\n",
      "relevance results F1 score  0.9301837270341207  precision 0.977924944812362 recall 0.8868868868868869\n",
      " Macro F1 0.933288898428586 Weighted F1 0.933288898428586 Accuracy 0.9334334334334334\n",
      "Train loss 0.14270376518368721 accuracy 0.9334334334334334\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.11 \n",
      "\n",
      "Dev loss 0.1117960081845522 accuracy 0.9721105527638192 eval_measures {'map': 0.9469849246231156, 'AP@5': 0.9469849246231156, 'P@1': 0.9346733668341709, 'RR': 0.9469849246231156, 'Rprec': 0.9346733668341709, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9469849246231156}\n",
      "Epoch 4/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3794.00\n",
      "  Accuracy : 0.95\n",
      "  Average training loss: 0.11\n",
      "relevance results F1 score  0.9472584856396866  precision 0.990174672489083 recall 0.9079079079079079\n",
      " Macro F1 0.9493620635790937 Weighted F1 0.9493620635790936 Accuracy 0.9494494494494494\n",
      "Train loss 0.11249804573506117 accuracy 0.9494494494494494\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.96\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.13 \n",
      "\n",
      "Dev loss 0.1330497931484133 accuracy 0.9655778894472363 eval_measures {'map': 0.9556113902847571, 'AP@5': 0.9556113902847571, 'P@1': 0.949748743718593, 'RR': 0.9556113902847571, 'Rprec': 0.949748743718593, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9556113902847572}\n",
      " -------------------Training complete for learning rate 3e-05 and number of epochs 4 ------ \n",
      "Train loss 0.11249804573506117 accuracy 0.9494494494494494 end_of_curriculum 0\n",
      "Dev loss 0.1330497931484133 dev 0.9655778894472363 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      " ------------------- Overall Training complete! ---------------------------------------\n",
      "Best learning rate 3e-05 and number of epochs 4 end_of_curriculum 0 ------ \n",
      "Best Dev loss 0.1330497931484133 and dev accuracy  0.9655778894472363 best map {'map': 0.9556113902847571, 'AP@5': 0.9556113902847571, 'P@1': 0.949748743718593, 'RR': 0.9556113902847571, 'Rprec': 0.949748743718593, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9556113902847572}\n",
      "\n",
      " ------------------------------------------------------------ \n",
      "Running Testing ...\n",
      "  Batch   100  of    127.    Elapsed: 0:00:54.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2021-mono_paraphrase-mpnet-base-v2-seed-22612812_rerankded_data-depth_20.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2021/en-clef2021-mono_paraphrase-mpnet-base-v2-seed-22612812_rerankded_data-depth_20.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.9360561056105611, 'AP@5': 0.9360561056105611, 'P@1': 0.905940594059406, 'RR': 0.936056105610561, 'Rprec': 0.905940594059406, 'R@5': 0.9702970297029703, 'R@10': 0.9702970297029703, 'R@20': 0.9702970297029703, 'R@50': 0.9702970297029703, 'RR@5': 0.936056105610561}\n",
      "device:  cuda:0\n",
      "train size  3996\n",
      "dev size  3980\n",
      "Epoch 1/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3284.00\n",
      "  Accuracy : 0.82\n",
      "  Average training loss: 0.40\n",
      "relevance results F1 score  0.812137203166227  precision 0.8588169642857143 recall 0.7702702702702703\n",
      " Macro F1 0.8213470404217618 Weighted F1 0.8213470404217618 Accuracy 0.8218218218218218\n",
      "Train loss 0.39849982035160064 accuracy 0.8218218218218218\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.92\n",
      "  Accuracy: 0.89\n",
      "  Average Validation loss: 0.28 \n",
      "\n",
      "Dev loss 0.284469092220068 accuracy 0.8894472361809046 eval_measures {'map': 0.9242043551088777, 'AP@5': 0.9242043551088777, 'P@1': 0.8894472361809045, 'RR': 0.9242043551088777, 'Rprec': 0.8894472361809045, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9242043551088778}\n",
      "Epoch 2/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3689.00\n",
      "  Accuracy : 0.92\n",
      "  Average training loss: 0.20\n",
      "relevance results F1 score  0.9198224079394097  precision 0.9617695248498088 recall 0.8813813813813813\n",
      " Macro F1 0.9230387562156814 Weighted F1 0.9230387562156814 Accuracy 0.9231731731731732\n",
      "Train loss 0.19548344798386097 accuracy 0.9231731731731732\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.96\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.06 \n",
      "\n",
      "Dev loss 0.0625554752573371 accuracy 0.9831658291457287 eval_measures {'map': 0.955611390284757, 'AP@5': 0.955611390284757, 'P@1': 0.949748743718593, 'RR': 0.9556113902847571, 'Rprec': 0.949748743718593, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9556113902847572}\n",
      "Epoch 3/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3707.00\n",
      "  Accuracy : 0.93\n",
      "  Average training loss: 0.14\n",
      "relevance results F1 score  0.9243653493849777  precision 0.9687328579264948 recall 0.8838838838838838\n",
      " Macro F1 0.9275387044215706 Weighted F1 0.9275387044215706 Accuracy 0.9276776776776777\n",
      "Train loss 0.1445358137488365 accuracy 0.9276776776776777\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.07 \n",
      "\n",
      "Dev loss 0.07155996949039399 accuracy 0.9796482412060302 eval_measures {'map': 0.9505862646566166, 'AP@5': 0.9505862646566166, 'P@1': 0.9396984924623115, 'RR': 0.9505862646566163, 'Rprec': 0.9396984924623115, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9505862646566166}\n",
      "Epoch 4/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3769.00\n",
      "  Accuracy : 0.94\n",
      "  Average training loss: 0.10\n",
      "relevance results F1 score  0.940215959968396  precision 0.9922178988326849 recall 0.8933933933933934\n",
      " Macro F1 0.9430519609138763 Weighted F1 0.9430519609138762 Accuracy 0.9431931931931932\n",
      "Train loss 0.10414985400438309 accuracy 0.9431931931931932\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.11 \n",
      "\n",
      "Dev loss 0.11288365078158677 accuracy 0.971105527638191 eval_measures {'map': 0.9530988274706869, 'AP@5': 0.9530988274706869, 'P@1': 0.9447236180904522, 'RR': 0.9530988274706867, 'Rprec': 0.9447236180904522, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9530988274706869}\n",
      " -------------------Training complete for learning rate 3e-05 and number of epochs 4 ------ \n",
      "Train loss 0.10414985400438309 accuracy 0.9431931931931932 end_of_curriculum 0\n",
      "Dev loss 0.11288365078158677 dev 0.971105527638191 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      " ------------------- Overall Training complete! ---------------------------------------\n",
      "Best learning rate 3e-05 and number of epochs 4 end_of_curriculum 0 ------ \n",
      "Best Dev loss 0.11288365078158677 and dev accuracy  0.971105527638191 best map {'map': 0.9530988274706869, 'AP@5': 0.9530988274706869, 'P@1': 0.9447236180904522, 'RR': 0.9530988274706867, 'Rprec': 0.9447236180904522, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9530988274706869}\n",
      "\n",
      " ------------------------------------------------------------ \n",
      "Running Testing ...\n",
      "  Batch   100  of    127.    Elapsed: 0:00:54.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2021-mono_paraphrase-mpnet-base-v2-seed-146764631_rerankded_data-depth_20.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2021/en-clef2021-mono_paraphrase-mpnet-base-v2-seed-146764631_rerankded_data-depth_20.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.9157237152286658, 'AP@5': 0.9150165016501649, 'P@1': 0.8762376237623762, 'RR': 0.9157237152286658, 'Rprec': 0.8762376237623762, 'R@5': 0.9653465346534653, 'R@10': 0.9702970297029703, 'R@20': 0.9702970297029703, 'R@50': 0.9702970297029703, 'RR@5': 0.9150165016501649}\n",
      "device:  cuda:0\n",
      "train size  3996\n",
      "dev size  3980\n",
      "Epoch 1/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3231.00\n",
      "  Accuracy : 0.81\n",
      "  Average training loss: 0.46\n",
      "relevance results F1 score  0.7969206264932308  precision 0.8485019785189373 recall 0.7512512512512513\n",
      " Macro F1 0.8079277688679172 Weighted F1 0.8079277688679172 Accuracy 0.8085585585585585\n",
      "Train loss 0.45602015966176984 accuracy 0.8085585585585585\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.87\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.12 \n",
      "\n",
      "Dev loss 0.12196610155701637 accuracy 0.9718592964824122 eval_measures {'map': 0.8729061976549413, 'AP@5': 0.8722780569514237, 'P@1': 0.8040201005025126, 'RR': 0.8729061976549413, 'Rprec': 0.8040201005025126, 'R@5': 0.9597989949748744, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.8722780569514238}\n",
      "Epoch 2/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3629.00\n",
      "  Accuracy : 0.91\n",
      "  Average training loss: 0.24\n",
      "relevance results F1 score  0.903598634095088  precision 0.9508015478164732 recall 0.8608608608608609\n",
      " Macro F1 0.9079522441682131 Weighted F1 0.9079522441682131 Accuracy 0.9081581581581581\n",
      "Train loss 0.24148776116967202 accuracy 0.9081581581581581\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.05 \n",
      "\n",
      "Dev loss 0.05003192505985499 accuracy 0.9839195979899499 eval_measures {'map': 0.9459798994974874, 'AP@5': 0.9459798994974874, 'P@1': 0.9296482412060302, 'RR': 0.9459798994974874, 'Rprec': 0.9296482412060302, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9459798994974874}\n",
      "Epoch 3/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:35.\n",
      "\n",
      "  correct_predictions: 3741.00\n",
      "  Accuracy : 0.94\n",
      "  Average training loss: 0.15\n",
      "relevance results F1 score  0.9336110387919813  precision 0.9728703201302225 recall 0.8973973973973974\n",
      " Macro F1 0.9360900291526757 Weighted F1 0.9360900291526757 Accuracy 0.9361861861861862\n",
      "Train loss 0.14593249326944352 accuracy 0.9361861861861862\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.96\n",
      "  Accuracy: 0.98\n",
      "  Average Validation loss: 0.09 \n",
      "\n",
      "Dev loss 0.08918716300278902 accuracy 0.9753768844221107 eval_measures {'map': 0.955611390284757, 'AP@5': 0.955611390284757, 'P@1': 0.949748743718593, 'RR': 0.9556113902847571, 'Rprec': 0.949748743718593, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.9556113902847572}\n",
      "Epoch 4/4\n",
      "----------\n",
      "  Batch    40  of    125.    Elapsed: 0:00:32.\n",
      "  Batch    80  of    125.    Elapsed: 0:01:03.\n",
      "  Batch   120  of    125.    Elapsed: 0:01:34.\n",
      "\n",
      "  correct_predictions: 3777.00\n",
      "  Accuracy : 0.95\n",
      "  Average training loss: 0.11\n",
      "relevance results F1 score  0.9424441524310119  precision 0.9922523519645822 recall 0.8973973973973974\n",
      " Macro F1 0.9450696998123533 Weighted F1 0.9450696998123532 Accuracy 0.9451951951951952\n",
      "Train loss 0.11197487410902977 accuracy 0.9451951951951952\n",
      "Running Evaluation...\n",
      "  Batch   100  of    125.    Elapsed: 0:00:28.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  \n",
      "Trec run is saved into  ./data/runs/dev_resuable_trec_run.tsv\n",
      " \n",
      " Done evaluation  -------------------\n",
      "  Map measure : 0.95\n",
      "  Accuracy: 0.97\n",
      "  Average Validation loss: 0.12 \n",
      "\n",
      "Dev loss 0.1214621655549854 accuracy 0.9688442211055277 eval_measures {'map': 0.949748743718593, 'AP@5': 0.949748743718593, 'P@1': 0.9346733668341709, 'RR': 0.949748743718593, 'Rprec': 0.9346733668341709, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.949748743718593}\n",
      " -------------------Training complete for learning rate 3e-05 and number of epochs 4 ------ \n",
      "Train loss 0.11197487410902977 accuracy 0.9451951951951952 end_of_curriculum 0\n",
      "Dev loss 0.1214621655549854 dev 0.9688442211055277 \n",
      "\n",
      " ------------------------------------------------------------ \n",
      " ------------------- Overall Training complete! ---------------------------------------\n",
      "Best learning rate 3e-05 and number of epochs 4 end_of_curriculum 0 ------ \n",
      "Best Dev loss 0.1214621655549854 and dev accuracy  0.9688442211055277 best map {'map': 0.949748743718593, 'AP@5': 0.949748743718593, 'P@1': 0.9346733668341709, 'RR': 0.949748743718593, 'Rprec': 0.9346733668341709, 'R@5': 0.964824120603015, 'R@10': 0.964824120603015, 'R@20': 0.964824120603015, 'R@50': 0.964824120603015, 'RR@5': 0.949748743718593}\n",
      "\n",
      " ------------------------------------------------------------ \n",
      "Running Testing ...\n",
      "  Batch   100  of    127.    Elapsed: 0:00:54.\n",
      "Done with re-ranking tweets \n",
      "Output is saved into  ./data/runs/en-clef2021-mono_paraphrase-mpnet-base-v2-seed-21228945_rerankded_data-depth_20.tsv\n",
      "Trec run is saved into  ./data/runs/trec_eval/EN2021/en-clef2021-mono_paraphrase-mpnet-base-v2-seed-21228945_rerankded_data-depth_20.tsv\n",
      " \n",
      "\\Done evaluation \n",
      "{'map': 0.9165453310036885, 'AP@5': 0.9162541254125411, 'P@1': 0.8712871287128713, 'RR': 0.9165453310036886, 'Rprec': 0.8712871287128713, 'R@5': 0.9653465346534653, 'R@10': 0.9653465346534653, 'R@20': 0.9702970297029703, 'R@50': 0.9702970297029703, 'RR@5': 0.9162541254125411}\n"
     ]
    }
   ],
   "source": [
    "bert_models = [\n",
    "            \"sentence-transformers/paraphrase-mpnet-base-v2\",\n",
    "            \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "            \"sentence-transformers/stsb-mpnet-base-v2\",\n",
    "            \"sentence-transformers/msmarco-roberta-base-v2\",\n",
    "            \"sentence-transformers/paraphrase-MiniLM-L12-v2\",\n",
    "            \"bert-base-cased\",\n",
    "            ]\n",
    "models_names = [\n",
    "            \"paraphrase-mpnet-base-v2\",\n",
    "            \"paraphrase-multilingual-mpnet-base-v2\",\n",
    "            \"stsb-mpnet-base-v2\",\n",
    "            \"msmarco-roberta-base-v2\",\n",
    "            \"paraphrase-MiniLM-L12-v2\",\n",
    "            \"BERT\",\n",
    "            ]\n",
    "            \n",
    "num_rand_seeds  = 5\n",
    "seeds = random_seeds[:num_rand_seeds]\n",
    "for i in range(len(bert_models)):\n",
    "    l = 0\n",
    "    seed = seeds[l]\n",
    "    bert_model = bert_models[i]\n",
    "    model_name = models_names[i]\n",
    "    print(\"---------- training for model : --------\", model_name)\n",
    "    hp = { # hyper parameters\n",
    "\n",
    "    \"model_name\": bert_model,\n",
    "    \"name\": model_name +\"-seed-\"+ str(seed),\n",
    "    \"model_save_path\": \"./data/saved_models/EN-clef2021-\"+model_name+\"_mono_trained_model-tuning-seed-\"+ str(seed)+\".bin\",\n",
    "    \"model_training_log\": \"./data/bert_evaluation/en-clef2021-mono_\"+model_name+\"_training_log-tuning-seed-\"+ str(seed)+\".xlsx\",\n",
    "    \"batch_size\": 32,\n",
    "    \"test_depth\": 30,\n",
    "    \"num_of_epochs\": [3, 4, 5], \n",
    "    \"learning_rate\" : [2e-5, 3e-5],\n",
    "    \"dropout\": [0.3, 0.4,], \n",
    "    \"seeds\": [seed],\n",
    "    \"max_len\": 256,\n",
    "    \"curricula_type\": 0,\n",
    "    \"end_of_curriculum\": [0],\n",
    "    \"num_of_layers\": ONE_LAYER,\n",
    "    \"is_output_probability\": False, # if false, put loss_function = mono_loss, otherwise put loss_function=CrossEntropy\n",
    "    \"loss_function\": \"mono_loss\"}\n",
    "   \n",
    "\n",
    "\n",
    "    _, _, best_learning_rate, best_num_of_epochs, best_end_of_curriculum, best_dropout = mono_bert_trainer.train_mono_bert(\n",
    "                        mono_bert_train_set, mono_bert_dev_set_depth_20, hp[\"model_name\"], apply_cleaning=False,  seeds=hp[\"seeds\"],\n",
    "                        trained_model_save_path=hp[\"model_save_path\"], shuffle=True, freeze_bert=False, max_len=hp[\"max_len\"], \n",
    "                        batch_size=hp[\"batch_size\"],epochs=hp[\"num_of_epochs\"], learning_rates = hp[\"learning_rate\"],\n",
    "                        is_output_probability=hp[\"is_output_probability\"],  end_of_curriculums=hp[\"end_of_curriculum\"],\n",
    "                        curricula_type=hp[\"curricula_type\"], dropout=hp[\"dropout\"], hp=hp, results_path=hp[\"model_training_log\"],\n",
    "                        classifier_layers=hp[\"num_of_layers\"], what_to_eval=cf.VCLAIM_AND_TITLE)\n",
    "\n",
    "\n",
    "    # store the best hyperparameters\n",
    "    hp[\"num_of_epochs\"]= best_num_of_epochs\n",
    "    hp[\"learning_rate\"]= best_learning_rate\n",
    "    hp[\"end_of_curriculum\"]= best_end_of_curriculum\n",
    "    hp[\"dropout\"]= best_dropout\n",
    "\n",
    "\n",
    "    # measure the performance on the test set\n",
    "    depths = [20,]\n",
    "    for k in range(len(depths)):\n",
    "        depth = depths[k]\n",
    "        hp[\"test_depth\"] = depth\n",
    "        mono_bert_test_set_path = \"./data/CLEF_2021/English/test_sets/mono_bert_test_set_top_\" + str(depth) + \".tsv\"\n",
    "        run_name = \"en-clef2021-mono_\"+model_name+\"-seed-\"+str(seed)+\"_rerankded_data-depth_\" +str(depth)+\".tsv\"\n",
    "        hp[\"reranked_data_path\"] = \"./data/runs/\" + run_name\n",
    "        hp[\"trec_run_path\"] = \"./data/runs/trec_eval/EN2021/\"+ run_name\n",
    "\n",
    "        mono_bert_tester.test_mono_bert(hp[\"model_name\"], hp[\"model_save_path\"], mono_bert_test_set_path, \n",
    "                        hp[\"reranked_data_path\"],  evaluation_save_path, max_len=hp[\"max_len\"], batch_size=hp[\"batch_size\"],\n",
    "                        dropout=hp[\"dropout\"], apply_cleaning=False, is_output_probability=hp[\"is_output_probability\"], \n",
    "                        hyper_parameters=hp, classifier_layers=hp[\"num_of_layers\"], trec_run_path=hp[\"trec_run_path\"],\n",
    "                        what_to_test=cf.VCLAIM_AND_TITLE)\n",
    "\n",
    "\n",
    "    # then we get the best hyper parameters values\n",
    "    # and we can run the training with different random seeds\n",
    "\n",
    "    l = 1\n",
    "    while l < num_rand_seeds:\n",
    "        seed = seeds[l]\n",
    "        l = l + 1\n",
    "        hp[\"name\"] = model_name +\"-seed-\"+ str(seed)\n",
    "        hp[\"model_save_path\"]= \"./data/saved_models/EN-clef2021-\"+model_name+\"_mono_trained_model-tuning-seed-\"+ str(seed)+\".bin\"\n",
    "        hp[\"model_training_log\"]= \"./data/bert_evaluation/en-clef2021-mono_\"+model_name+\"_training_log-tuning-seed-\"+ str(seed)+\".xlsx\"\n",
    "        hp[\"learning_rate\"]= [best_learning_rate]\n",
    "        hp[\"num_of_epochs\"]= [best_num_of_epochs]\n",
    "        hp[\"end_of_curriculum\"]= [best_end_of_curriculum]\n",
    "        hp[\"dropout\"]= [best_dropout]\n",
    "        hp[\"seeds\"] = [seed]\n",
    "\n",
    "\n",
    "        _, _, best_learning_rate, best_num_of_epochs, best_end_of_curriculum, best_dropout = mono_bert_trainer.train_mono_bert(\n",
    "                        mono_bert_train_set, mono_bert_dev_set_depth_20, hp[\"model_name\"], apply_cleaning=False,  seeds=hp[\"seeds\"],\n",
    "                        trained_model_save_path=hp[\"model_save_path\"], shuffle=True, freeze_bert=False, max_len=hp[\"max_len\"], \n",
    "                        batch_size=hp[\"batch_size\"],epochs=hp[\"num_of_epochs\"], learning_rates = hp[\"learning_rate\"],\n",
    "                        is_output_probability=hp[\"is_output_probability\"],  end_of_curriculums=hp[\"end_of_curriculum\"],\n",
    "                        curricula_type=hp[\"curricula_type\"], dropout=hp[\"dropout\"], hp=hp, results_path=hp[\"model_training_log\"],\n",
    "                        classifier_layers=hp[\"num_of_layers\"], what_to_eval=cf.VCLAIM_AND_TITLE)\n",
    "\n",
    "        # store the best hyperparameters\n",
    "        hp[\"num_of_epochs\"]= best_num_of_epochs\n",
    "        hp[\"learning_rate\"]= best_learning_rate\n",
    "        hp[\"end_of_curriculum\"]= best_end_of_curriculum\n",
    "        hp[\"dropout\"]= best_dropout\n",
    "\n",
    "\n",
    "         # measure the performance on the test set\n",
    "        depths = [20,]\n",
    "        for k in range(len(depths)):\n",
    "            depth = depths[k]\n",
    "            hp[\"test_depth\"] = depth\n",
    "            mono_bert_test_set_path = \"./data/CLEF_2021/English/test_sets/mono_bert_test_set_top_\" + str(depth) + \".tsv\"\n",
    "            run_name = \"en-clef2021-mono_\"+model_name+\"-seed-\"+str(seed)+\"_rerankded_data-depth_\" +str(depth)+\".tsv\"\n",
    "            hp[\"reranked_data_path\"] = \"./data/runs/\" + run_name\n",
    "            hp[\"trec_run_path\"] = \"./data/runs/trec_eval/EN2021/\"+ run_name\n",
    "\n",
    "            mono_bert_tester.test_mono_bert(hp[\"model_name\"], hp[\"model_save_path\"], mono_bert_test_set_path, \n",
    "                            hp[\"reranked_data_path\"],  evaluation_save_path, max_len=hp[\"max_len\"], batch_size=hp[\"batch_size\"],\n",
    "                            dropout=hp[\"dropout\"], apply_cleaning=False, is_output_probability=hp[\"is_output_probability\"], \n",
    "                            hyper_parameters=hp, classifier_layers=hp[\"num_of_layers\"], trec_run_path=hp[\"trec_run_path\"],\n",
    "                            what_to_test=cf.VCLAIM_AND_TITLE)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('tweetEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32ea63ce396b580d232aa3acb554bdf20f8441e7d8ed4d44b8e0f0e042cd90e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
